{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GroupKFold\n",
    "import datetime\n",
    "import pkg_resources\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import gc\n",
    "import re\n",
    "import operator \n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from scipy.stats import spearmanr\n",
    "import os\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "from apex import amp\n",
    "import random\n",
    "import shutil\n",
    "import transformers\n",
    "from spacy.lang.en import English\n",
    "from spacy_readability import Readability\n",
    "from math import floor, ceil\n",
    "#from comment_parser import comment_parser\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from xml.sax.saxutils import unescape\n",
    "import torch.nn.init as init\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_10_fold.csv', 'train_new_features.csv', 'train_5_fold.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED                = 2020\n",
    "DATA_DIR            =\"./input/\"\n",
    "WORK_DIR            = \"./working/\"\n",
    "nlp                 = English()\n",
    "device              = torch.device('cuda')\n",
    "seed_everything(SEED)\n",
    "epochs              = 4\n",
    "lr                  = 3e-5\n",
    "batch_size          = 8\n",
    "accumulation_steps  = 1\n",
    "NUM_FOLD            = 10\n",
    "warmup              = 0.04\n",
    "config = transformers.BertConfig.from_pretrained('bert-base-uncased',output_hidden_states=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 6079 train records\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR,\"train_new_features.csv\"))\n",
    "print('loaded %d train records' % len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def _get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def _trim_input(title, question, answer, max_sequence_length, \n",
    "                t_max_len=30, q_max_len=239, a_max_len=239):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n",
    "        \n",
    "        t = t[:t_new_len]\n",
    "        q_len_head = round(q_new_len/3)\n",
    "        q_len_tail = -1* (q_new_len -q_len_head)\n",
    "        a_len_head = round(a_new_len/3)\n",
    "        a_len_tail = -1* (a_new_len -a_len_head) \n",
    "        \n",
    "        q = q[:q_len_head]+q[q_len_tail:]\n",
    "        a = a[:a_len_head]+a[a_len_tail:]\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "def convert_lines(title, question, answer, max_sequence_length, tokenizer, t_max_len_seq=30, q_max_len_seq=239, a_max_len_seq=239):\n",
    "    \n",
    "    all_tokens   = []\n",
    "    all_masks    = []\n",
    "    all_segments = []\n",
    "    \n",
    "    longer = 0\n",
    "    \n",
    "    for t, q, a in tqdm(zip(title, question, answer)):\n",
    "        \n",
    "        tokens_t, tokens_q, tokens_a  = _trim_input(t, q, a, max_sequence_length=max_sequence_length)\n",
    "        #print(tokens_t)\n",
    "        #print(tokens_q)\n",
    "        #print(tokens_a)\n",
    "        \n",
    "        stoken = [\"[CLS]\"] + tokens_t + [\"[SEP]\"] + tokens_q + [\"[SEP]\"] + tokens_a + [\"[SEP]\"] \n",
    "        ##############\n",
    "        #token_ids\n",
    "        ##############\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(stoken)\n",
    "        input_ids = token_ids + [0] * (max_sequence_length-len(token_ids))\n",
    "        \n",
    "        #############\n",
    "        #input_masks\n",
    "        #############\n",
    "        attention_masks = _get_masks(stoken, max_sequence_length)\n",
    "        #print(attention_masks)\n",
    "        \n",
    "        ##############\n",
    "        #input_segments\n",
    "        ###############\n",
    "        input_segments = _get_segments(stoken, max_sequence_length)\n",
    "        \n",
    "        all_tokens.append(input_ids)\n",
    "        all_masks.append(attention_masks)\n",
    "        all_segments.append(input_segments)\n",
    "        #break\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6079it [00:38, 157.21it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokens, train_masks, train_segments = convert_lines(train_df[\"question_title\"],\n",
    "                                                          train_df[\"question_body\"], \n",
    "                                                          train_df[\"answer\"],\n",
    "                                                          max_sequence_length=512, \n",
    "                                                          tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_external_features(df):\n",
    "    \n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    #If the question is longer, it may be more clear, which may help users give a more \n",
    "    df['question_body']      = df['question_body'].progress_apply(lambda x:str(x))\n",
    "    df['question_num_words'] = df.question_body.str.count('\\S+')\n",
    "    df['question_title_num_words'] = df.question_title.str.count('\\S+')\n",
    "    \n",
    "    #The assumption here is that longer answer could bring more useful detail\n",
    "    df['answer']            = df['answer'].progress_apply(lambda x:str(x))\n",
    "    df['answer_num_words']  = df.answer.str.count('\\S+')\n",
    "    \n",
    "    df[\"question_title_num_unique_words\"] = df[\"question_title\"].progress_apply(lambda x: len(set(str(x).split())))\n",
    "    df[\"question_body_num_unique_words\"]  = df[\"question_body\"].progress_apply(lambda x: len(set(str(x).split())))\n",
    "    df[\"answer_num_unique_words\"]         = df[\"answer\"].progress_apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    df[\"question_title_num_chars\"] = df[\"question_title\"].apply(lambda x: len(str(x)))\n",
    "    df[\"question_body_num_chars\"]  = df[\"question_body\"].apply(lambda x: len(str(x)))\n",
    "    df[\"answer_num_chars\"]         = df[\"answer\"].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    df['qt_words'] = df['question_title'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords])\n",
    "    df['q_words'] = df['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords])\n",
    "    df['a_words'] = df['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords])\n",
    "    df['qa_word_overlap'] = df.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\n",
    "    df['qt_word_overlap'] = df.apply(lambda s: len(np.intersect1d(s['qt_words'], s['a_words'])), axis = 1)\n",
    "    \n",
    "    df['qa_word_overlap_norm'] = df.apply(lambda s: s['qa_word_overlap']/(len(s['a_words']) + len(s['q_words'])  - s['qa_word_overlap']) , axis = 1)\n",
    "    df['qta_word_overlap_norm'] = df.apply(lambda s: s['qt_word_overlap']/(len(s['a_words']) + len(s['qt_words']) - s['qt_word_overlap']), axis = 1)\n",
    "    df.drop(['q_words', 'a_words', 'qt_words'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6079/6079 [00:00<00:00, 705257.49it/s]\n",
      "100%|██████████| 6079/6079 [00:00<00:00, 719162.13it/s]\n",
      "100%|██████████| 6079/6079 [00:00<00:00, 351879.30it/s]\n",
      "100%|██████████| 6079/6079 [00:00<00:00, 52205.52it/s]\n",
      "100%|██████████| 6079/6079 [00:00<00:00, 47679.84it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = add_external_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handmade_cols = [\"question_body_num_unique_words\", 'question_num_words',\n",
    "                 \"question_title_num_unique_words\", \"question_title_num_words\", \n",
    "                 \"answer_num_unique_words\", \"answer_num_words\",\n",
    "                ]\n",
    "\n",
    "num_words_scaler = MinMaxScaler()\n",
    "num_words_scaler.fit(train_df[handmade_cols].values)\n",
    "with open('scaler.pickle', mode='wb') as f:\n",
    "    pickle.dump(num_words_scaler, f)\n",
    "\n",
    "train_df[handmade_cols]=  num_words_scaler.transform(train_df[handmade_cols].values)\n",
    "train_handmade_features= train_df[handmade_cols+['l2_qt_a', 'l2_qb_a', 'cos_qt_a', 'cos_qb_a',\n",
    "                                                'qa_word_overlap_norm',\n",
    "                                                'qta_word_overlap_norm']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(x, dict_reverse):\n",
    "    try:\n",
    "        return dict_reverse[x]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "unique_categories = train_df['category'].unique().tolist()\n",
    "category_dict = {i + 1: e for i, e in enumerate(unique_categories)}\n",
    "category_dict_reverse = {v: k for k, v in category_dict.items()}\n",
    "with open('category.pickle', mode='wb') as f:\n",
    "    pickle.dump(category_dict_reverse, f)\n",
    "train_df['category'] = train_df['category'].apply(lambda x: label_encoder(x, category_dict_reverse))\n",
    "\n",
    "unique_hosts = train_df['host'].unique().tolist()\n",
    "host_dict = {i + 1: e for i, e in enumerate(unique_hosts)}\n",
    "host_dict_reverse = {v: k for k, v in host_dict.items()}\n",
    "with open('host.pickle', mode='wb') as f:\n",
    "    pickle.dump(host_dict_reverse, f)\n",
    "train_df['host'] = train_df['host'].apply(lambda x: label_encoder(x, host_dict_reverse))\n",
    "\n",
    "n_cat    = len(category_dict_reverse) + 1\n",
    "cat_emb  = 128\n",
    "n_host   = len(host_dict_reverse) + 1\n",
    "host_emb = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, token_ids, masks, segments, hosts, categories, handmade_features):\n",
    "                \n",
    "        self.df         = df\n",
    "        self.token_ids  = token_ids\n",
    "        self.masks      = masks\n",
    "        self.segments   = segments\n",
    "        self.hosts      = hosts\n",
    "        self.categories = categories\n",
    "        self.handmades  = handmade_features\n",
    "\n",
    "        self.question_cols = ['question_asker_intent_understanding',\n",
    "                              'question_body_critical', 'question_conversational',\n",
    "                              'question_expect_short_answer', 'question_fact_seeking',\n",
    "                              'question_has_commonly_accepted_answer',\n",
    "                              'question_interestingness_others', 'question_interestingness_self',\n",
    "                              'question_multi_intent', 'question_not_really_a_question',\n",
    "                              'question_opinion_seeking', 'question_type_choice',\n",
    "                              'question_type_compare', 'question_type_consequence',\n",
    "                              'question_type_definition', 'question_type_entity',\n",
    "                              'question_type_instructions', 'question_type_procedure',\n",
    "                              'question_type_reason_explanation', 'question_type_spelling',\n",
    "                              'question_well_written']\n",
    "        self.answer_cols   = ['answer_helpful', 'answer_level_of_information',\n",
    "                              'answer_plausible', 'answer_relevance',\n",
    "                              'answer_satisfaction', 'answer_type_instructions',\n",
    "                              'answer_type_procedure', 'answer_type_reason_explanation',\n",
    "                              'answer_well_written']\n",
    "\n",
    "        self.label = self.df[self.question_cols + self.answer_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        token_id = self.token_ids[idx]\n",
    "        mask     = self.masks[idx]\n",
    "        segment  = self.segments[idx]\n",
    "        host     = self.hosts[idx]\n",
    "        category = self.categories[idx]\n",
    "        handmade = self.handmades[idx]\n",
    "        \n",
    "        labels = self.label[idx]\n",
    "\n",
    "        return [token_id, mask, segment, host, category, handmade], labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def children(m):\n",
    "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "def set_trainable_attr(m, b):\n",
    "    m.trainable = b\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = b\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, torch.nn.Module):\n",
    "        f(m)\n",
    "    if len(c) > 0:\n",
    "        for l in c:\n",
    "            apply_leaf(l, f)\n",
    "\n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestModel6(nn.Module):\n",
    "\n",
    "    def __init__(self, n_cat, cat_emb, n_host, host_emb, num_labels):\n",
    "        super().__init__()\n",
    "        \n",
    "        BERT_DIMS = 768\n",
    "        self.bert_model = transformers.BertModel.from_pretrained('bert-base-uncased', config=config)    \n",
    "        set_trainable(self.bert_model.embeddings.word_embeddings, False)\n",
    "        \n",
    "        self.category_embedding = nn.Embedding(n_cat, cat_emb)\n",
    "        self.host_embedding     = nn.Embedding(n_host, host_emb)\n",
    "        \n",
    "        self.fc1  = nn.Linear(BERT_DIMS*4, BERT_DIMS*4)\n",
    "        self.fc2  = nn.Linear(BERT_DIMS*4 + int(cat_emb) + int(host_emb) + 12, 21)\n",
    "        self.fc3  = nn.Linear(BERT_DIMS*4 + int(cat_emb) + int(host_emb) + 12, 9)\n",
    "        self.tanh = nn.SELU()\n",
    "        \n",
    "        self._init_weights(self.category_embedding)\n",
    "        self._init_weights(self.host_embedding)\n",
    "        self._init_weights(self.fc1)\n",
    "        self._init_weights(self.fc2)\n",
    "        self._init_weights(self.fc3)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            print(\"initailize weight\")\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            print(\"initailize bias\")\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "        \n",
    "    def forward(self, token_ids, masks, segments, hosts, categories, handmades):\n",
    "        \n",
    "        category_embed      = self.category_embedding(categories)\n",
    "        host_embed          = self.host_embedding(hosts)\n",
    "        \n",
    "        external_features   = torch.cat((category_embed, host_embed, handmades), 1)\n",
    "        _,_, hidden_layers = self.bert_model(input_ids=token_ids, \n",
    "                                             token_type_ids=segments, \n",
    "                                             attention_mask=masks)\n",
    "        \n",
    "        hidden_input  = [hidden_layers[-1][:, 0, :],\n",
    "                         hidden_layers[-2][:, 0, :],\n",
    "                         hidden_layers[-3][:, 0, :], \n",
    "                         hidden_layers[-4][:, 0, :]]\n",
    "        \n",
    "        cls           = torch.cat(hidden_input, dim = -1)\n",
    "        cls           = self.fc1(cls)\n",
    "        cls           = self.tanh(cls)\n",
    "        cls           = torch.cat((cls, external_features), 1)\n",
    "        q_results     = self.fc2(cls)\n",
    "        a_results     = self.fc3(cls)\n",
    "        \n",
    "        results       = torch.cat((q_results, a_results), 1)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, train_loader, optimizer, scheduler):\n",
    "    max_grad_norm = 1.0\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    question_weight = 0.7\n",
    "    answer_weight = 0.3\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "        token_ids, masks, segments, hosts, categories, handmades = inputs\n",
    "        token_ids   = token_ids.long().cuda()\n",
    "        masks       = masks.long().cuda()\n",
    "        segments    = segments.long().cuda()\n",
    "        hosts       = hosts.long().cuda()\n",
    "        categories  = categories.long().cuda()\n",
    "        handmades   = handmades.float().cuda()\n",
    "        \n",
    "        labels = labels.float().cuda()\n",
    "        output_train = model(token_ids, masks, segments, hosts, categories, handmades)\n",
    "        loss = question_weight*criterion(output_train[:,0:21], labels[:,0:21]) + \\\n",
    "               answer_weight*criterion(output_train[:,21:30], labels[:,21:30])\n",
    "        \n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        \n",
    "    return avg_loss\n",
    "\n",
    "def val_model(model, criterion, val_loader):\n",
    "    avg_val_loss = 0.\n",
    "    model.eval()\n",
    "    preds    = []\n",
    "    original = []\n",
    "    question_weight = 0.7\n",
    "    answer_weight = 0.3\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(val_loader):\n",
    "            token_ids, masks, segments, hosts, categories, handmades = inputs\n",
    "            token_ids   = token_ids.long().cuda()\n",
    "            masks       = masks.long().cuda()\n",
    "            segments    = segments.long().cuda()\n",
    "            hosts       = hosts.long().cuda()\n",
    "            categories  = categories.long().cuda()\n",
    "            handmades   = handmades.float().cuda()\n",
    "            labels      = labels.float().cuda()\n",
    "            \n",
    "            output_val = model(token_ids, masks, segments, hosts, categories, handmades)\n",
    "            loss = question_weight*criterion(output_val[:,0:21], labels[:,0:21]) + \\\n",
    "                   answer_weight*criterion(output_val[:,21:30], labels[:,21:30])\n",
    "            avg_val_loss += loss.item() / len(val_loader)\n",
    "            preds.append(torch.sigmoid(output_val).cpu().numpy())\n",
    "            original.append(labels.cpu().numpy())\n",
    "        \n",
    "        trues = np.concatenate(original)\n",
    "        preds = np.concatenate(preds)\n",
    "        \n",
    "        rhos = []\n",
    "        for col_trues, col_pred in zip(trues.T, preds.T):\n",
    "            rhos.append(np.nan_to_num(spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation))\n",
    "        \n",
    "    return avg_val_loss, np.mean(rhos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.validation import _num_samples, check_array\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "from sklearn.model_selection._split import _BaseKFold, _RepeatedSplits, \\\n",
    "    BaseShuffleSplit, _validate_shuffle_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\n",
    "        'question_asker_intent_understanding',\n",
    "        'question_body_critical',\n",
    "        'question_conversational',\n",
    "        'question_expect_short_answer',\n",
    "        'question_fact_seeking',\n",
    "        'question_has_commonly_accepted_answer',\n",
    "        'question_interestingness_others',\n",
    "        'question_interestingness_self',\n",
    "        'question_multi_intent',\n",
    "        'question_not_really_a_question',\n",
    "        'question_opinion_seeking',\n",
    "        'question_type_choice',\n",
    "        'question_type_compare',\n",
    "        'question_type_consequence',\n",
    "        'question_type_definition',\n",
    "        'question_type_entity',\n",
    "        'question_type_instructions',\n",
    "        'question_type_procedure',\n",
    "        'question_type_reason_explanation',\n",
    "        'question_type_spelling',\n",
    "        'question_well_written',\n",
    "        'answer_helpful',\n",
    "        'answer_level_of_information',\n",
    "        'answer_plausible',\n",
    "        'answer_relevance',\n",
    "        'answer_satisfaction',\n",
    "        'answer_type_instructions',\n",
    "        'answer_type_procedure',\n",
    "        'answer_type_reason_explanation',\n",
    "        'answer_well_written'    \n",
    "    ] + ['host', 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/ratthachat/ml-stratifiers\n",
    "def IterativeStratification(labels, r, random_state):\n",
    "    \"\"\"This function implements the Iterative Stratification algorithm described\n",
    "    in the following paper:\n",
    "    Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of\n",
    "    Multi-Label Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M.\n",
    "    (eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n",
    "    2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin,\n",
    "    Heidelberg.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = labels.shape[0]\n",
    "    test_folds = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    # Calculate the desired number of examples at each subset\n",
    "    c_folds = r * n_samples\n",
    "\n",
    "    # Calculate the desired number of examples of each label at each subset\n",
    "    c_folds_labels = np.outer(r, labels.sum(axis=0))\n",
    "\n",
    "    labels_not_processed_mask = np.ones(n_samples, dtype=bool)\n",
    "\n",
    "    while np.any(labels_not_processed_mask):\n",
    "        # Find the label with the fewest (but at least one) remaining examples,\n",
    "        # breaking ties randomly\n",
    "        num_labels = labels[labels_not_processed_mask].sum(axis=0)\n",
    "\n",
    "        # Handle case where only all-zero labels are left by distributing\n",
    "        # across all folds as evenly as possible (not in original algorithm but\n",
    "        # mentioned in the text). (By handling this case separately, some\n",
    "        # code redundancy is introduced; however, this approach allows for\n",
    "        # decreased execution time when there are a relatively large number\n",
    "        # of all-zero labels.)\n",
    "        if num_labels.sum() == 0:\n",
    "            sample_idxs = np.where(labels_not_processed_mask)[0]\n",
    "\n",
    "            for sample_idx in sample_idxs:\n",
    "                fold_idx = np.where(c_folds == c_folds.max())[0]\n",
    "\n",
    "                if fold_idx.shape[0] > 1:\n",
    "                    fold_idx = fold_idx[random_state.choice(fold_idx.shape[0])]\n",
    "\n",
    "                test_folds[sample_idx] = fold_idx\n",
    "                c_folds[fold_idx] -= 1\n",
    "\n",
    "            break\n",
    "\n",
    "        label_idx = np.where(num_labels == num_labels[np.nonzero(num_labels)].min())[0]\n",
    "        if label_idx.shape[0] > 1:\n",
    "            label_idx = label_idx[random_state.choice(label_idx.shape[0])]\n",
    "\n",
    "        sample_idxs = np.where(np.logical_and(labels[:, label_idx].flatten(), labels_not_processed_mask))[0]\n",
    "\n",
    "        for sample_idx in sample_idxs:\n",
    "            # Find the subset(s) with the largest number of desired examples\n",
    "            # for this label, breaking ties by considering the largest number\n",
    "            # of desired examples, breaking further ties randomly\n",
    "            label_folds = c_folds_labels[:, label_idx]\n",
    "            fold_idx = np.where(label_folds == label_folds.max())[0]\n",
    "\n",
    "            if fold_idx.shape[0] > 1:\n",
    "                temp_fold_idx = np.where(c_folds[fold_idx] ==\n",
    "                                         c_folds[fold_idx].max())[0]\n",
    "                fold_idx = fold_idx[temp_fold_idx]\n",
    "\n",
    "                if temp_fold_idx.shape[0] > 1:\n",
    "                    fold_idx = fold_idx[random_state.choice(temp_fold_idx.shape[0])]\n",
    "\n",
    "            test_folds[sample_idx] = fold_idx\n",
    "            labels_not_processed_mask[sample_idx] = False\n",
    "\n",
    "            # Update desired number of examples\n",
    "            c_folds_labels[fold_idx, labels[sample_idx]] -= 1\n",
    "            c_folds[fold_idx] -= 1\n",
    "\n",
    "    return test_folds\n",
    "\n",
    "\n",
    "class MultilabelStratifiedKFold(_BaseKFold):\n",
    "    \"\"\"Multilabel stratified K-Folds cross-validator\n",
    "    Provides train/test indices to split multilabel data into train/test sets.\n",
    "    This cross-validation object is a variation of KFold that returns\n",
    "    stratified folds for multilabel data. The folds are made by preserving\n",
    "    the percentage of samples for each label.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=3\n",
    "        Number of folds. Must be at least 2.\n",
    "    shuffle : boolean, optional\n",
    "        Whether to shuffle each stratification of the data before splitting\n",
    "        into batches.\n",
    "    random_state : int, RandomState instance or None, optional, default=None\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`. Unlike StratifiedKFold that only uses random_state\n",
    "        when ``shuffle`` == True, this multilabel implementation\n",
    "        always uses the random_state since the iterative stratification\n",
    "        algorithm breaks ties randomly.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n",
    "    >>> mskf.get_n_splits(X, y)\n",
    "    2\n",
    "    >>> print(mskf)  # doctest: +NORMALIZE_WHITESPACE\n",
    "    MultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n",
    "    >>> for train_index, test_index in mskf.split(X, y):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    Notes\n",
    "    -----\n",
    "    Train and test sizes may be slightly different in each fold.\n",
    "    See also\n",
    "    --------\n",
    "    RepeatedMultilabelStratifiedKFold: Repeats Multilabel Stratified K-Fold\n",
    "    n times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=3, shuffle=False, random_state=None):\n",
    "        super(MultilabelStratifiedKFold, self).__init__(n_splits, shuffle, random_state)\n",
    "\n",
    "    def _make_test_folds(self, X, y):\n",
    "        y = np.asarray(y, dtype=bool)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "\n",
    "        if type_of_target_y != 'multilabel-indicator':\n",
    "            raise ValueError(\n",
    "                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(type_of_target_y))\n",
    "\n",
    "        num_samples = y.shape[0]\n",
    "\n",
    "        rng = check_random_state(self.random_state)\n",
    "        indices = np.arange(num_samples)\n",
    "\n",
    "        if self.shuffle:\n",
    "            rng.shuffle(indices)\n",
    "            y = y[indices]\n",
    "\n",
    "        r = np.asarray([1 / self.n_splits] * self.n_splits)\n",
    "\n",
    "        test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n",
    "\n",
    "        return test_folds[np.argsort(indices)]\n",
    "\n",
    "    def _iter_test_masks(self, X=None, y=None, groups=None):\n",
    "        test_folds = self._make_test_folds(X, y)\n",
    "        for i in range(self.n_splits):\n",
    "            yield test_folds == i\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "            Note that providing ``y`` is sufficient to generate the splits and\n",
    "            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
    "            ``X`` instead of actual training data.\n",
    "        y : array-like, shape (n_samples, n_labels)\n",
    "            The target variable for supervised learning problems.\n",
    "            Multilabel stratification is done based on the y labels.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        Notes\n",
    "        -----\n",
    "        Randomized CV splitters may return different results for each call of\n",
    "        split. You can make the results identical by setting ``random_state``\n",
    "        to an integer.\n",
    "        \"\"\"\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        return super(MultilabelStratifiedKFold, self).split(X, y, groups)\n",
    "\n",
    "\n",
    "class RepeatedMultilabelStratifiedKFold(_RepeatedSplits):\n",
    "    \"\"\"Repeated Multilabel Stratified K-Fold cross validator.\n",
    "    Repeats Mulilabel Stratified K-Fold n times with different randomization\n",
    "    in each repetition.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of folds. Must be at least 2.\n",
    "    n_repeats : int, default=10\n",
    "        Number of times cross-validator needs to be repeated.\n",
    "    random_state : None, int or RandomState, default=None\n",
    "        Random state to be used to generate random state for each\n",
    "        repetition as well as randomly breaking ties within the iterative\n",
    "        stratification algorithm.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> rmskf = RepeatedMultilabelStratifiedKFold(n_splits=2, n_repeats=2,\n",
    "    ...     random_state=0)\n",
    "    >>> for train_index, test_index in rmskf.split(X, y):\n",
    "    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...     X_train, X_test = X[train_index], X[test_index]\n",
    "    ...     y_train, y_test = y[train_index], y[test_index]\n",
    "    ...\n",
    "    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    TRAIN: [0 1 4 5] TEST: [2 3 6 7]\n",
    "    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n",
    "    See also\n",
    "    --------\n",
    "    RepeatedStratifiedKFold: Repeats (Non-multilabel) Stratified K-Fold\n",
    "    n times.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n",
    "        super(RepeatedMultilabelStratifiedKFold, self).__init__(\n",
    "            MultilabelStratifiedKFold, n_repeats, random_state,\n",
    "            n_splits=n_splits)\n",
    "\n",
    "\n",
    "class MultilabelStratifiedShuffleSplit(BaseShuffleSplit):\n",
    "    \"\"\"Multilabel Stratified ShuffleSplit cross-validator\n",
    "    Provides train/test indices to split data into train/test sets.\n",
    "    This cross-validation object is a merge of MultilabelStratifiedKFold and\n",
    "    ShuffleSplit, which returns stratified randomized folds for multilabel\n",
    "    data. The folds are made by preserving the percentage of each label.\n",
    "    Note: like the ShuffleSplit strategy, multilabel stratified random splits\n",
    "    do not guarantee that all folds will be different, although this is\n",
    "    still very likely for sizeable datasets.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default 10\n",
    "        Number of re-shuffling & splitting iterations.\n",
    "    test_size : float, int, None, optional\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "        of the dataset to include in the test split. If int, represents the\n",
    "        absolute number of test samples. If None, the value is set to the\n",
    "        complement of the train size. By default, the value is set to 0.1.\n",
    "        The default will change in version 0.21. It will remain 0.1 only\n",
    "        if ``train_size`` is unspecified, otherwise it will complement\n",
    "        the specified ``train_size``.\n",
    "    train_size : float, int, or None, default is None\n",
    "        If float, should be between 0.0 and 1.0 and represent the\n",
    "        proportion of the dataset to include in the train split. If\n",
    "        int, represents the absolute number of train samples. If None,\n",
    "        the value is automatically set to the complement of the test size.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`. Unlike StratifiedShuffleSplit that only uses\n",
    "        random_state when ``shuffle`` == True, this multilabel implementation\n",
    "        always uses the random_state since the iterative stratification\n",
    "        algorithm breaks ties randomly.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> msss = MultilabelStratifiedShuffleSplit(n_splits=3, test_size=0.5,\n",
    "    ...    random_state=0)\n",
    "    >>> msss.get_n_splits(X, y)\n",
    "    3\n",
    "    >>> print(mss)       # doctest: +ELLIPSIS\n",
    "    MultilabelStratifiedShuffleSplit(n_splits=3, random_state=0, test_size=0.5,\n",
    "                                     train_size=None)\n",
    "    >>> for train_index, test_index in msss.split(X, y):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n",
    "    TRAIN: [1 2 5 6] TEST: [0 3 4 7]\n",
    "    Notes\n",
    "    -----\n",
    "    Train and test sizes may be slightly different from desired due to the\n",
    "    preference of stratification over perfectly sized folds.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=10, test_size=\"default\", train_size=None,\n",
    "                 random_state=None):\n",
    "        super(MultilabelStratifiedShuffleSplit, self).__init__(\n",
    "            n_splits, test_size, train_size, random_state)\n",
    "\n",
    "    def _iter_indices(self, X, y, groups=None):\n",
    "        n_samples = _num_samples(X)\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        y = np.asarray(y, dtype=bool)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "\n",
    "        if type_of_target_y != 'multilabel-indicator':\n",
    "            raise ValueError(\n",
    "                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(\n",
    "                    type_of_target_y))\n",
    "\n",
    "        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,\n",
    "                                                  self.train_size)\n",
    "\n",
    "        n_samples = y.shape[0]\n",
    "        rng = check_random_state(self.random_state)\n",
    "        y_orig = y.copy()\n",
    "\n",
    "        r = np.array([n_train, n_test]) / (n_train + n_test)\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            indices = np.arange(n_samples)\n",
    "            rng.shuffle(indices)\n",
    "            y = y_orig[indices]\n",
    "\n",
    "            test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n",
    "\n",
    "            test_idx = test_folds[np.argsort(indices)] == 1\n",
    "            test = np.where(test_idx)[0]\n",
    "            train = np.where(~test_idx)[0]\n",
    "\n",
    "            yield train, test\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "            Note that providing ``y`` is sufficient to generate the splits and\n",
    "            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
    "            ``X`` instead of actual training data.\n",
    "        y : array-like, shape (n_samples, n_labels)\n",
    "            The target variable for supervised learning problems.\n",
    "            Multilabel stratification is done based on the y labels.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        Notes\n",
    "        -----\n",
    "        Randomized CV splitters may return different results for each call of\n",
    "        split. You can make the results identical by setting ``random_state``\n",
    "        to an integer.\n",
    "        \"\"\"\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        return super(MultilabelStratifiedShuffleSplit, self).split(X, y, groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[targets].values\n",
    "X_train = np.array(range(len(train_df))).reshape(-1, 1)\n",
    "kf = MultilabelStratifiedKFold(n_splits = NUM_FOLD, shuffle=True, random_state = SEED).split(X_train,y_train)\n",
    "if os.path.exists('./input/train_{}_fold.csv'.format(NUM_FOLD)) == False:\n",
    "    train_df['{}_fold'.format(NUM_FOLD)] = 0\n",
    "    for fold, (train_index, val_index) in enumerate(kf):\n",
    "        train_df.loc[val_index, '{}_fold'.format(NUM_FOLD)] = fold\n",
    "    train_df.to_csv('./input/train_{}_fold.csv'.format(NUM_FOLD), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_kfold():\n",
    "    \n",
    "    fold_df = pd.read_csv('./input/train_{}_fold.csv'.format(NUM_FOLD))\n",
    "    \n",
    "    for fold in range(NUM_FOLD):\n",
    "        print(\"fold:\", fold)\n",
    "        if fold < 7: \n",
    "            continue\n",
    "        train_index = fold_df[fold_df['{}_fold'.format(NUM_FOLD)] != fold].index\n",
    "        val_index = fold_df[fold_df['{}_fold'.format(NUM_FOLD)] == fold].index\n",
    "\n",
    "        train_df_ = train_df.iloc[train_index]\n",
    "        val_df    = train_df.iloc[val_index]\n",
    "    \n",
    "        train_set    = QuestDataset(train_df_,\n",
    "                                    train_tokens[train_index], \n",
    "                                    train_masks[train_index], \n",
    "                                    train_segments[train_index],\n",
    "                                    train_df['host'].values[train_index],\n",
    "                                    train_df['category'].values[train_index],\n",
    "                                    train_handmade_features[train_index],\n",
    "                                    )\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "        val_set      = QuestDataset(val_df,\n",
    "                                    train_tokens[val_index],\n",
    "                                    train_masks[val_index],\n",
    "                                    train_segments[val_index],\n",
    "                                    train_df['host'].values[val_index],\n",
    "                                    train_df['category'].values[val_index],\n",
    "                                    train_handmade_features[val_index]\n",
    "                                   )\n",
    "        val_loader   = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "\n",
    "        model = QuestModel6(n_cat, cat_emb, n_host, host_emb, num_labels=30)\n",
    "        model.to(device)\n",
    "        \n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        num_training_steps = int(epochs*len(train_df_)/batch_size/accumulation_steps)\n",
    "        num_warmup_steps   = int(warmup * num_training_steps)\n",
    "    \n",
    "        best_avg_loss    = 100.0\n",
    "        best_score       = 0.0\n",
    "        best_param_loss  = None\n",
    "        best_param_score = None\n",
    "        i = 0\n",
    "        optimizer        = transformers.AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False) \n",
    "        criterion        = nn.BCEWithLogitsLoss()\n",
    "        scheduler        = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                                        num_warmup_steps=num_warmup_steps, \n",
    "                                                                        num_training_steps=num_training_steps)  \n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "        \n",
    "            if i == 5: break\n",
    "            start_time   = time.time()\n",
    "            avg_loss     = train_model(model, criterion, train_loader, optimizer, scheduler)\n",
    "            avg_val_loss, score = val_model(model, criterion, val_loader)\n",
    "            elapsed_time = time.time() - start_time \n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(epoch + 1,\n",
    "                                                                                                         epochs, \n",
    "                                                                                                         avg_loss, \n",
    "                                                                                                         avg_val_loss,\n",
    "                                                                                                         score, \n",
    "                                                                                                         elapsed_time))\n",
    "    \n",
    "            if best_avg_loss > avg_val_loss:\n",
    "                i = 0\n",
    "                best_avg_loss = avg_val_loss \n",
    "                best_param_loss = model.state_dict()\n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                best_param_score = model.state_dict()\n",
    "            else:\n",
    "                i += 1\n",
    "                \n",
    "        torch.save(best_param_score, 'weight_score_best_{}.pt'.format(fold))\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        del train_df_\n",
    "        del val_df\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "fold: 6\n",
      "fold: 7\n",
      "initailize weight\n",
      "initailize weight\n",
      "initailize weight\n",
      "initailize bias\n",
      "initailize weight\n",
      "initailize bias\n",
      "initailize weight\n",
      "initailize bias\n",
      "Epoch 1/4 \t loss=0.3882 \t val_loss=0.3664 \t score=0.392814 \t time=128.36s\n",
      "Epoch 2/4 \t loss=0.3555 \t val_loss=0.3618 \t score=0.402260 \t time=126.64s\n",
      "Epoch 3/4 \t loss=0.3375 \t val_loss=0.3641 \t score=0.402916 \t time=129.00s\n",
      "Epoch 4/4 \t loss=0.3250 \t val_loss=0.3664 \t score=0.400165 \t time=129.72s\n",
      "fold: 8\n",
      "initailize weight\n",
      "initailize weight\n",
      "initailize weight\n",
      "initailize bias\n",
      "initailize weight\n",
      "initailize bias\n",
      "initailize weight\n",
      "initailize bias\n",
      "Epoch 1/4 \t loss=0.3894 \t val_loss=0.3702 \t score=0.404522 \t time=138.13s\n",
      "Epoch 2/4 \t loss=0.3544 \t val_loss=0.3618 \t score=0.420205 \t time=136.16s\n",
      "Epoch 3/4 \t loss=0.3368 \t val_loss=0.3628 \t score=0.418234 \t time=135.98s\n",
      "Epoch 4/4 \t loss=0.3241 \t val_loss=0.3663 \t score=0.414469 \t time=136.45s\n",
      "fold: 9\n",
      "initailize weight\n",
      "initailize weight\n",
      "initailize weight\n",
      "initailize bias\n",
      "initailize weight\n",
      "initailize bias\n",
      "initailize weight\n",
      "initailize bias\n",
      "Epoch 1/4 \t loss=0.3890 \t val_loss=0.3687 \t score=0.391259 \t time=144.98s\n",
      "Epoch 2/4 \t loss=0.3553 \t val_loss=0.3643 \t score=0.411721 \t time=143.79s\n",
      "Epoch 3/4 \t loss=0.3370 \t val_loss=0.3659 \t score=0.409114 \t time=144.12s\n",
      "Epoch 4/4 \t loss=0.3245 \t val_loss=0.3691 \t score=0.406598 \t time=143.93s\n"
     ]
    }
   ],
   "source": [
    "train_model_kfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
