{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n",
    "!pip install ../input/transformers-master/transformers-master/ > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改定履歴\n",
    "1. Blend Ver2 First Submit (0.424)\n",
    "2. Change Xlnet to two branch Model\n",
    "3. BugFix output_base (0.426)\n",
    "4. Update Tricks (0.435)\n",
    "5. Add Large Back (0.440)\n",
    "6. Update Trick with rescalling -- submission trick2 (0.447)\n",
    "7. Update XlNet, Change Weight -> 0.3 (submission Overtime)\n",
    "8. More Efficiency (0.449)\n",
    "9. Back to Origin Weight\n",
    "10. Change Bert-Large to Roberta-Large (0.452)\n",
    "11. Change Bert-Finetune-1 to 0.396 New Sturcture(0.451)\n",
    "12. Trick Test (add limit 0.453)\n",
    "13. New Trick Test \n",
    "14. Change Activation of Bert-Base 2 (0.455) from (tanh => SELU)\n",
    "15. round up\n",
    "16. round down (0.443)\n",
    "17. np.around -> np.floor （0.459)\n",
    "18. Change Bert-Finetune-1 to 0.396 New Sturcture\n",
    "19. Change Bert-Finetune-1 to 0.398 Model （0.458)\n",
    "20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import gc\n",
    "import re\n",
    "import operator \n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from scipy.stats import spearmanr\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import transformers\n",
    "from math import floor, ceil\n",
    "from glob import glob\n",
    "#from xml.sax.saxutils import unescape\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras.backend as K\n",
    "from numba import cuda\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED                = 2020\n",
    "DATA_DIR            = \"../input/google-quest-challenge/\"\n",
    "WORK_DIR            = \"../working/\"\n",
    "BERT_VOCAB_PATH     = \"../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\"\n",
    "BERT_MODEL_PATH     = \"../input/pretrained-bert-models-for-pytorch/bert-base-uncased/pytorch_model.bin\"\n",
    "BERT_CONFIG_PATH    = \"../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json\"\n",
    "XLNET_VOCAB_PATH    = \"../input/xlnetlargecased/xlnet_cased_L-24_H-1024_A-16/spiece.model\"\n",
    "XLNET_MODEL_PATH    = \"../input/xlnetlargecased/xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt.index\"\n",
    "XLNET_CONFIG_PATH   = \"../input/xlnetlargecased/xlnet_cased_L-24_H-1024_A-16/xlnet_config.json\"\n",
    "input_columns       = ['question_title', 'question_body', 'answer']\n",
    "seed_everything(SEED)\n",
    "batch_size          = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df  = pd.read_csv(os.path.join(DATA_DIR,\"test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = transformers.BertConfig.from_json_file(BERT_CONFIG_PATH)\n",
    "bert_config.output_hidden_states = True\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(BERT_VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def _get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def _trim_input(title, question, answer, max_sequence_length, \n",
    "                t_max_len=30, q_max_len=239, a_max_len=239):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n",
    "        \n",
    "        t = t[:t_new_len]\n",
    "        q_len_head = round(q_new_len/3)\n",
    "        q_len_tail = -1* (q_new_len -q_len_head)\n",
    "        a_len_head = round(a_new_len/3)\n",
    "        a_len_tail = -1* (a_new_len -a_len_head) \n",
    "        \n",
    "        q = q[:q_len_head]+q[q_len_tail:]\n",
    "        a = a[:a_len_head]+a[a_len_tail:]\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "def convert_lines(title, question, answer, max_sequence_length, tokenizer, t_max_len_seq=30, q_max_len_seq=239, a_max_len_seq=239):\n",
    "    \n",
    "    all_tokens   = []\n",
    "    all_masks    = []\n",
    "    all_segments = []\n",
    "    \n",
    "    longer = 0\n",
    "    \n",
    "    for t, q, a in tqdm(zip(title, question, answer)):\n",
    "        \n",
    "        tokens_t, tokens_q, tokens_a  = _trim_input(t, q, a, max_sequence_length=max_sequence_length)\n",
    "        #print(tokens_t)\n",
    "        #print(tokens_q)\n",
    "        #print(tokens_a)\n",
    "        \n",
    "        stoken = [\"[CLS]\"] + tokens_t + [\"[SEP]\"] + tokens_q + [\"[SEP]\"] + tokens_a + [\"[SEP]\"] \n",
    "        ##############\n",
    "        #token_ids\n",
    "        ##############\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(stoken)\n",
    "        input_ids = token_ids + [0] * (max_sequence_length-len(token_ids))\n",
    "        \n",
    "        #############\n",
    "        #input_masks\n",
    "        #############\n",
    "        attention_masks = _get_masks(stoken, max_sequence_length)\n",
    "        #print(attention_masks)\n",
    "        \n",
    "        ##############\n",
    "        #input_segments\n",
    "        ###############\n",
    "        input_segments = _get_segments(stoken, max_sequence_length)\n",
    "        \n",
    "        all_tokens.append(input_ids)\n",
    "        all_masks.append(attention_masks)\n",
    "        all_segments.append(input_segments)\n",
    "        #break\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "476it [00:05, 80.62it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tokens, test_masks, test_segments = convert_lines(test_df[\"question_title\"],\n",
    "                                                       test_df[\"question_body\"], \n",
    "                                                       test_df[\"answer\"],\n",
    "                                                       max_sequence_length=512, \n",
    "                                                       tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"../input/universalsentenceencoderlarge4/\"\n",
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_title\n",
      "question_body\n",
      "answer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "302230"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_test = {}\n",
    "\n",
    "for text in input_columns:\n",
    "    print(text)\n",
    "    test_text = test_df[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    \n",
    "    curr_test_emb = []\n",
    "    batch_size_ = 4\n",
    "    ind = 0\n",
    "    while ind*batch_size_ < len(test_text):\n",
    "        curr_test_emb.append(embed(test_text[ind*batch_size_: (ind + 1)*batch_size_])[\"outputs\"].numpy())\n",
    "        ind += 1\n",
    "        \n",
    "    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n",
    "    \n",
    "del embed\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.select_device(0) #clear GPU memory \n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weakproxy at 0x7ff69030a458 to Device at 0x7ff72a3f9a90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.select_device(0) #restart cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n",
    "cos_dist = lambda x, y: (x*y).sum(axis=1)\n",
    "\n",
    "dist_features_test = np.array([\n",
    "    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n",
    "    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n",
    "]).T\n",
    "\n",
    "idxs = range(len(test_df))\n",
    "columns = ['l2_qt_a', 'l2_qb_a', 'l2_qb_qt',\n",
    "           'cos_qt_a', 'cos_qb_a', 'cos_qb_qt',\n",
    "          ]\n",
    "dist_features_df = pd.DataFrame(index=idxs, columns=columns)\n",
    "dist_features_df[columns] = dist_features_test\n",
    "for col in columns: test_df[col] = dist_features_df[col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_external_features(df):\n",
    "    \n",
    "    #If the question is longer, it may be more clear, which may help users give a more \n",
    "    df['question_body']      = df['question_body'].progress_apply(lambda x:str(x))\n",
    "    df['question_num_words'] = df.question_body.str.count('\\S+')\n",
    "    df['question_title_num_words'] = df.question_title.str.count('\\S+')\n",
    "    \n",
    "    #The assumption here is that longer answer could bring more useful detail\n",
    "    df['answer']            = df['answer'].progress_apply(lambda x:str(x))\n",
    "    df['answer_num_words']  = df.answer.str.count('\\S+')\n",
    "    \n",
    "    df[\"question_title_num_unique_words\"] = df[\"question_title\"].progress_apply(lambda x: len(set(str(x).split())))\n",
    "    df[\"question_body_num_unique_words\"]  = df[\"question_body\"].progress_apply(lambda x: len(set(str(x).split())))\n",
    "    df[\"answer_num_unique_words\"]         = df[\"answer\"].progress_apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    df[\"question_title_num_chars\"] = df[\"question_title\"].apply(lambda x: len(str(x)))\n",
    "    df[\"question_body_num_chars\"]  = df[\"question_body\"].apply(lambda x: len(str(x)))\n",
    "    df[\"answer_num_chars\"]         = df[\"answer\"].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    df['qt_words'] = df['question_title'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords])\n",
    "    df['q_words'] = df['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords])\n",
    "    df['a_words'] = df['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords])\n",
    "    df['qa_word_overlap'] = df.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\n",
    "    df['qt_word_overlap'] = df.apply(lambda s: len(np.intersect1d(s['qt_words'], s['a_words'])), axis = 1)\n",
    "    \n",
    "    df['qa_word_overlap_norm'] = df.apply(lambda s: s['qa_word_overlap']/(len(s['a_words']) + len(s['q_words'])  - s['qa_word_overlap']) , axis = 1)\n",
    "    df['qta_word_overlap_norm'] = df.apply(lambda s: s['qt_word_overlap']/(len(s['a_words']) + len(s['qt_words']) - s['qt_word_overlap']), axis = 1)\n",
    "    df.drop(['q_words', 'a_words', 'qt_words'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [00:00<00:00, 213004.24it/s]\n",
      "100%|██████████| 476/476 [00:00<00:00, 205717.54it/s]\n",
      "100%|██████████| 476/476 [00:00<00:00, 152905.62it/s]\n",
      "100%|██████████| 476/476 [00:00<00:00, 38240.32it/s]\n",
      "100%|██████████| 476/476 [00:00<00:00, 30374.55it/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = add_external_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "handmade_cols = [\"question_body_num_unique_words\", 'question_num_words',\n",
    "                 \"question_title_num_unique_words\", \"question_title_num_words\", \n",
    "                 \"answer_num_unique_words\", \"answer_num_words\"]\n",
    "\n",
    "with open('../input/quest-bert-3/scaler.pickle', mode='rb') as f:\n",
    "    num_words_scaler = pickle.load(f)\n",
    "\n",
    "test_df[handmade_cols]=  num_words_scaler.transform(test_df[handmade_cols].values)\n",
    "#test_handmade_features= test_df[handmade_cols + ['l2_qt_a', 'l2_qb_a', 'cos_qt_a', 'cos_qb_a']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(x, dict_reverse):\n",
    "    try:\n",
    "        return dict_reverse[x]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "with open('../input/quest-bert-2/category.pickle', mode='rb') as f:\n",
    "    category_dict_reverse = pickle.load(f)\n",
    "test_df['category'] = test_df['category'].apply(lambda x: label_encoder(x, category_dict_reverse))\n",
    "\n",
    "with open('../input/quest-bert-2/host.pickle', mode='rb') as f:\n",
    "    host_dict_reverse = pickle.load(f)\n",
    "test_df['host'] = test_df['host'].apply(lambda x: label_encoder(x, host_dict_reverse))\n",
    "\n",
    "n_cat    = len(category_dict_reverse) + 1\n",
    "cat_emb  = 128\n",
    "n_host   = len(host_dict_reverse) + 1\n",
    "host_emb = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestDataset_test(Dataset):\n",
    "\n",
    "    def __init__(self, token_ids, masks, segments, hosts, categories, handmade_features):\n",
    "                \n",
    "        self.token_ids  = token_ids\n",
    "        self.masks      = masks\n",
    "        self.segments   = segments\n",
    "        self.hosts      = hosts\n",
    "        self.categories = categories\n",
    "        self.handmades  = handmade_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.token_ids.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        token_id = self.token_ids[idx]\n",
    "        mask     = self.masks[idx]\n",
    "        segment  = self.segments[idx]\n",
    "        host     = self.hosts[idx]\n",
    "        category = self.categories[idx]\n",
    "        handmade = self.handmades[idx]\n",
    "\n",
    "        return [token_id, mask, segment, host, category, handmade]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model1 -- Bert-Base Finetune(LB:0.398)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def children(m):\n",
    "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "def set_trainable_attr(m, b):\n",
    "    m.trainable = b\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = b\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, torch.nn.Module):\n",
    "        f(m)\n",
    "    if len(c) > 0:\n",
    "        for l in c:\n",
    "            apply_leaf(l, f)\n",
    "\n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_cat, cat_emb, n_host, host_emb, num_labels):\n",
    "        super().__init__()\n",
    "        BERT_DIMS = 768\n",
    "        model_path = os.path.join('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/')\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(model_path, config=bert_config)\n",
    "        set_trainable(self.bert_model.embeddings.word_embeddings, False)\n",
    "        \n",
    "        self.category_embedding = nn.Embedding(n_cat, cat_emb)\n",
    "        self.host_embedding     = nn.Embedding(n_host, host_emb)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.fc1  = nn.Linear(BERT_DIMS*4, BERT_DIMS*4)\n",
    "        self.fc2  = nn.Linear(BERT_DIMS*5 + int(cat_emb) + int(host_emb) + 12, 21)\n",
    "        self.fc3  = nn.Linear(BERT_DIMS*5 + int(cat_emb) + int(host_emb) + 12, 9)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        self._init_weights(self.category_embedding)\n",
    "        self._init_weights(self.host_embedding)\n",
    "        self._init_weights(self.fc1)\n",
    "        self._init_weights(self.fc2)\n",
    "        self._init_weights(self.fc3)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            print(\"initailize weight\")\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            print(\"initailize bias\")\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "        \n",
    "    def forward(self, token_ids, masks, segments, hosts, categories, handmades):\n",
    "        \n",
    "        category_embed      = self.category_embedding(categories)\n",
    "        host_embed          = self.host_embedding(hosts)\n",
    "        \n",
    "        external_features   = torch.cat((category_embed, host_embed, handmades), 1)\n",
    "        _, pooled_output, hidden_states = self.bert_model(input_ids=token_ids, \n",
    "                                                          token_type_ids=segments, \n",
    "                                                          attention_mask=masks)\n",
    "        \n",
    "        meanpooled_output = torch.mean(torch.cat((hidden_states[-1], \n",
    "                                                  hidden_states[-2],\n",
    "                                                  hidden_states[-3], \n",
    "                                                  hidden_states[-4],), 2) , 1)\n",
    "        \n",
    "        meanpooled_output = self.fc1(meanpooled_output)\n",
    "        meanpooled_output = self.activation(meanpooled_output)\n",
    "        \n",
    "        pooled_output     = torch.cat((meanpooled_output,\n",
    "                                       pooled_output, \n",
    "                                       external_features), 1)\n",
    "    \n",
    "        q_results           = self.fc2(pooled_output)\n",
    "        a_results           = self.fc3(pooled_output)\n",
    "        \n",
    "        results             = torch.cat((q_results, a_results), 1)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_result(model, test_loader, batch_size=batch_size):\n",
    "    \n",
    "    output = np.zeros((len(test_set), 30))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, inputs in enumerate(test_loader):\n",
    "            start_index = idx * batch_size\n",
    "            end_index   = min(start_index + batch_size, len(test_set))\n",
    "            token_ids, masks, segments, hosts, categories, handmades = inputs\n",
    "            token_ids   = token_ids.long().cuda()\n",
    "            masks       = masks.long().cuda()\n",
    "            segments    = segments.long().cuda()\n",
    "            hosts       = hosts.long().cuda()\n",
    "            categories  = categories.long().cuda()\n",
    "            handmades   = handmades.float().cuda()\n",
    "            \n",
    "            predictions = model(token_ids, masks, segments, hosts, categories, handmades)\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            output[start_index:end_index, :] = predictions.detach().cpu().numpy()\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "pretrain_weighted =  glob('../input/quest-bert-5/*.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_handmade_features= test_df[handmade_cols + ['l2_qt_a', 'l2_qb_a', 'cos_qt_a', 'cos_qb_a',\n",
    "                                                 'qa_word_overlap_norm',\n",
    "                                                 'qta_word_overlap_norm']].values\n",
    "test_set    = QuestDataset_test(test_tokens, test_masks, test_segments, \n",
    "                                test_df['host'].values,\n",
    "                                test_df['category'].values,\n",
    "                                test_handmade_features)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initailize weight\n",
      "initailize weight\n",
      "initailize weight\n",
      "initailize bias\n",
      "initailize weight\n",
      "initailize bias\n",
      "initailize weight\n",
      "initailize bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuestModel(\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (category_embedding): Embedding(6, 128)\n",
       "  (host_embedding): Embedding(64, 128)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "  (fc2): Linear(in_features=4108, out_features=21, bias=True)\n",
       "  (fc3): Linear(in_features=4108, out_features=9, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = QuestModel(n_cat, cat_emb, n_host, host_emb, num_labels=30)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:30,  9.06s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, weight in tqdm(enumerate(pretrain_weighted)):\n",
    "    model.load_state_dict(torch.load(weight))\n",
    "    results.append(predict_result(model, test_loader))\n",
    "    \n",
    "output_base = np.zeros((len(test_set),30))\n",
    "for result in results:\n",
    "    output_base += result\n",
    "output_base /= len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.45578533e-01, 6.71771759e-01, 2.00605123e-01, 4.14248723e-01,\n",
       "       6.44091034e-01, 4.84917250e-01, 6.85563320e-01, 6.66393864e-01,\n",
       "       6.76841265e-01, 1.26994071e-03, 7.70822793e-01, 6.89096743e-01,\n",
       "       4.94433356e-03, 2.47739451e-01, 1.95695126e-03, 2.10819520e-03,\n",
       "       9.48266493e-02, 6.05030615e-02, 7.64178061e-01, 6.41298998e-04,\n",
       "       9.33368802e-01, 9.21338159e-01, 5.54943672e-01, 9.68319619e-01,\n",
       "       9.65477335e-01, 8.22495657e-01, 3.56030266e-02, 1.78340932e-02,\n",
       "       8.96994001e-01, 9.29834127e-01])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_base[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del results\n",
    "del result\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model2: Bert-base Li's Method (LB:0.400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() ## Clear_Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_cat, cat_emb, n_host, host_emb, num_labels):\n",
    "        super().__init__()\n",
    "        BERT_DIMS = 768\n",
    "        model_path = os.path.join('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/')\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(model_path, config=bert_config)\n",
    "        set_trainable(self.bert_model.embeddings.word_embeddings, False)\n",
    "        \n",
    "        self.category_embedding = nn.Embedding(n_cat, cat_emb)\n",
    "        self.host_embedding     = nn.Embedding(n_host, host_emb)\n",
    "        \n",
    "        #self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc1  = nn.Linear(BERT_DIMS*4, BERT_DIMS*4)\n",
    "        self.fc2  = nn.Linear(BERT_DIMS*4 + int(cat_emb) + int(host_emb) + 12, 21)\n",
    "        self.fc3  = nn.Linear(BERT_DIMS*4 + int(cat_emb) + int(host_emb) + 12, 9)\n",
    "        self.tanh = nn.SELU()\n",
    "        \n",
    "        self._init_weights(self.category_embedding)\n",
    "        self._init_weights(self.host_embedding)\n",
    "        self._init_weights(self.fc1)\n",
    "        self._init_weights(self.fc2)\n",
    "        self._init_weights(self.fc3)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            print(\"initailize weight\")\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            print(\"initailize bias\")\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "        \n",
    "    def forward(self, token_ids, masks, segments, hosts, categories, handmades):\n",
    "        \n",
    "        category_embed      = self.category_embedding(categories)\n",
    "        host_embed          = self.host_embedding(hosts)\n",
    "        \n",
    "        external_features   = torch.cat((category_embed, host_embed, handmades), 1)\n",
    "        _, _, hidden_layers = self.bert_model(input_ids=token_ids, token_type_ids=segments, attention_mask=masks)\n",
    "        hidden_input  = [hidden_layers[-1][:, 0, :],\n",
    "                         hidden_layers[-2][:, 0, :],\n",
    "                         hidden_layers[-3][:, 0, :], \n",
    "                         hidden_layers[-4][:, 0, :]]\n",
    "        \n",
    "        cls           = torch.cat(hidden_input, dim = -1)\n",
    "        cls           = self.fc1(cls)\n",
    "        cls           = self.tanh(cls)\n",
    "        cls           = torch.cat((cls, external_features), 1)\n",
    "       # cls           = self.dropout1(cls)\n",
    "        q_results     = self.fc2(cls)\n",
    "        a_results     = self.fc3(cls)\n",
    "        \n",
    "        results       = torch.cat((q_results, a_results), 1)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_handmade_features= test_df[handmade_cols + ['l2_qt_a', 'l2_qb_a', 'cos_qt_a', 'cos_qb_a',\n",
    "                                                 'qa_word_overlap_norm',\n",
    "                                                 'qta_word_overlap_norm']].values\n",
    "test_set    = QuestDataset_test(test_tokens, test_masks, test_segments, \n",
    "                                test_df['host'].values,\n",
    "                                test_df['category'].values,\n",
    "                                test_handmade_features)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "pretrain_weighted = glob('../input/quest-bert-3/*.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initailize weight\n",
      "initailize weight\n",
      "initailize weight\n",
      "initailize bias\n",
      "initailize weight\n",
      "initailize bias\n",
      "initailize weight\n",
      "initailize bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuestModel(\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (category_embedding): Embedding(6, 128)\n",
       "  (host_embedding): Embedding(64, 128)\n",
       "  (fc1): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "  (fc2): Linear(in_features=3340, out_features=21, bias=True)\n",
       "  (fc3): Linear(in_features=3340, out_features=9, bias=True)\n",
       "  (tanh): SELU()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = QuestModel(n_cat, cat_emb, n_host, host_emb, num_labels=30)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:29,  8.99s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, weight in tqdm(enumerate(pretrain_weighted)):\n",
    "    model.load_state_dict(torch.load(weight))\n",
    "    results.append(predict_result(model, test_loader))\n",
    "    \n",
    "output_base2 = np.zeros((len(test_set),30))\n",
    "for result in results:\n",
    "    output_base2 += result\n",
    "output_base2 /= len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.53080994e-01, 6.55766457e-01, 1.39395885e-01, 4.74947986e-01,\n",
       "       5.87175775e-01, 5.77031636e-01, 7.00860041e-01, 7.08363593e-01,\n",
       "       5.21874544e-01, 1.11731946e-03, 8.26921171e-01, 6.31747839e-01,\n",
       "       2.39375678e-03, 2.92256103e-01, 2.34182907e-03, 1.28975821e-03,\n",
       "       7.69624813e-02, 6.44144397e-02, 7.56942523e-01, 4.25323534e-04,\n",
       "       9.22005981e-01, 9.26921600e-01, 5.80926031e-01, 9.75345409e-01,\n",
       "       9.73542809e-01, 8.21196079e-01, 1.42902730e-02, 1.88925107e-02,\n",
       "       9.37293965e-01, 9.29328126e-01])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_base2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del results\n",
    "del result\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model3 -- RoBerta-Large Feature_based （LB：0.403)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() ## Clear_Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_VOCAB_PATH     = \"../input/roberta-large/roberta-large-vocab.json\"\n",
    "BERT_MERGES_PATH    = \"../input/roberta-large/roberta-large-merges.txt\"\n",
    "BERT_MODEL_PATH     = \"../input/roberta-large/roberta-large-pytorch_model.bin\"\n",
    "BERT_CONFIG_PATH    = \"../input/roberta-large/roberta-large-config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = transformers.RobertaConfig.from_json_file(BERT_CONFIG_PATH)\n",
    "bert_config.output_hidden_states = True\n",
    "tokenizer = transformers.RobertaTokenizer(BERT_VOCAB_PATH,BERT_MERGES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trim_input(title, question, answer, max_sequence_length, \n",
    "                t_max_len=30, q_max_len=238, a_max_len=238):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+6) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+6 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+6)))\n",
    "        \n",
    "        t = t[:t_new_len]\n",
    "        q_len_head = round(q_new_len/3)\n",
    "        q_len_tail = -1* (q_new_len -q_len_head)\n",
    "        a_len_head = round(a_new_len/3)\n",
    "        a_len_tail = -1* (a_new_len -a_len_head) \n",
    "        \n",
    "        q = q[:q_len_head]+q[q_len_tail:]\n",
    "        a = a[:a_len_head]+a[a_len_tail:]\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "def convert_lines(title, question, answer, max_sequence_length, tokenizer, t_max_len_seq=30, q_max_len_seq=238, a_max_len_seq=238):\n",
    "    \n",
    "    all_tokens   = []\n",
    "    all_masks    = []\n",
    "    all_segments = []\n",
    "    \n",
    "    longer = 0\n",
    "    \n",
    "    for t, q, a in tqdm(zip(title, question, answer)):\n",
    "        \n",
    "        tokens_t, tokens_q, tokens_a = _trim_input(t, q, a, max_sequence_length=max_sequence_length)\n",
    "        #print(tokens_t)\n",
    "        #print(tokens_q)\n",
    "        #print(tokens_a)\n",
    "        \n",
    "        stoken = [\"<s>\"] + tokens_t + [\"</s>\"] + [\"</s>\"] + tokens_q + [\"</s>\"] + [\"</s>\"] + tokens_a + [\"</s>\"]\n",
    "        ##############\n",
    "        #token_ids\n",
    "        ##############\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(stoken)\n",
    "        input_ids = token_ids + [0] * (max_sequence_length-len(token_ids))\n",
    "        \n",
    "        #############\n",
    "        #input_masks\n",
    "        #############\n",
    "        attention_masks = _get_masks(stoken, max_sequence_length)\n",
    "        #print(attention_masks)\n",
    "        \n",
    "        ##############\n",
    "        #input_segments\n",
    "        ###############\n",
    "        input_segments = _get_segments(stoken, max_sequence_length)\n",
    "        #print(len(input_ids))\n",
    "        \n",
    "        all_tokens.append(input_ids)\n",
    "        all_masks.append(attention_masks)\n",
    "        all_segments.append(input_segments)\n",
    "        #break\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_handmade_features= test_df[handmade_cols + ['qa_word_overlap_norm','qta_word_overlap_norm']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "476it [00:02, 165.12it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tokens, test_masks, test_segments = convert_lines(test_df[\"question_title\"],\n",
    "                                                       test_df[\"question_body\"], \n",
    "                                                       test_df[\"answer\"],\n",
    "                                                       max_sequence_length=512, \n",
    "                                                       tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set    = QuestDataset_test(test_tokens, test_masks, test_segments, \n",
    "                                test_df['host'].values,\n",
    "                                test_df['category'].values,\n",
    "                                test_handmade_features)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim),\n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "\n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        \n",
    "        return torch.sum(weighted_input, 1)\n",
    "    \n",
    "class QuestModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_cat, cat_emb, n_host, host_emb, num_labels):\n",
    "        super().__init__()\n",
    "        BERT_DIMS = 1024\n",
    "        LSTM_UNITS = 512\n",
    "        model_path = os.path.join(BERT_MODEL_PATH)\n",
    "        self.bert_model = transformers.RobertaModel.from_pretrained(model_path,config=bert_config)\n",
    "        set_trainable(self.bert_model, False)\n",
    "        \n",
    "        self.category_embedding = nn.Embedding(n_cat, cat_emb)\n",
    "        self.host_embedding     = nn.Embedding(n_host, host_emb)\n",
    "        self.embedding_dropout  = SpatialDropout(0.5)\n",
    "        self.lstm1              = nn.LSTM(BERT_DIMS*2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2              = nn.GRU(LSTM_UNITS*2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.atten1             = Attention(LSTM_UNITS*2, 512)\n",
    "        self.atten2             = Attention(LSTM_UNITS*2, 512)\n",
    "        self.dropout1           = nn.Dropout(0.2)\n",
    "        self.fc1                = nn.Linear(LSTM_UNITS*6 + int(cat_emb) + int(host_emb) + 8, 21)\n",
    "        self.fc2                = nn.Linear(LSTM_UNITS*6 + int(cat_emb) + int(host_emb) + 8, 9)\n",
    "        self._init_weights(self.category_embedding)\n",
    "        self._init_weights(self.host_embedding)\n",
    "        self._init_weights(self.fc1)\n",
    "        self._init_weights(self.fc2)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            print(\"initailize weight\")\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            print(\"initailize bias\")\n",
    "            module.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, token_ids, masks, segment, hosts, categories, handmades):\n",
    "        \n",
    "        category_embed = self.category_embedding(categories)\n",
    "        host_embed     = self.host_embedding(hosts)\n",
    "        external_features = torch.cat((category_embed, host_embed, handmades), 1)\n",
    "        #print(external_features.shape)\n",
    "        _, seq_output, hidden_states = self.bert_model(input_ids=token_ids,\n",
    "                                                    attention_mask=masks,\n",
    "                                                    token_type_ids=segment)\n",
    "        \n",
    "        last_four_layer = torch.cat((hidden_states[-5], hidden_states[-6]), 2)\n",
    "        lstm_input      = self.embedding_dropout(last_four_layer)\n",
    "        lstm1_output, _ = self.lstm1(lstm_input)\n",
    "        lstm2_output, _ = self.lstm2(lstm1_output)\n",
    "        \n",
    "        meanpooled_output   = torch.mean(lstm2_output, 1)\n",
    "        maxpooled_output, _ = torch.max(lstm2_output, 1)\n",
    "        attention_output_q  = self.atten1(lstm2_output)\n",
    "        attention_output_a  = self.atten2(lstm2_output)\n",
    "        \n",
    "        pooled_output_q = torch.cat((meanpooled_output, maxpooled_output, attention_output_q, external_features), 1)\n",
    "        pooled_output_q = self.dropout1(pooled_output_q)\n",
    "        \n",
    "        pooled_output_a = torch.cat((meanpooled_output, maxpooled_output, attention_output_a, external_features), 1)\n",
    "        pooled_output_a = self.dropout1(pooled_output_a)\n",
    "        \n",
    "        q_results     = self.fc1(pooled_output_q)\n",
    "        a_results     = self.fc2(pooled_output_a)\n",
    "        results       = torch.cat((q_results, a_results), 1)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "pretrain_weighted = glob('../input/large-roberta-b32-steplr3-drop-tb-rmf-h56-1/*.pt') + glob('../input/large-roberta-b32-steplr3-drop-tb-rmf-h56-2/*.pt') \n",
    "cat_emb = 256\n",
    "host_emb = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/large-roberta-b32-steplr3-drop-tb-rmf-h56-1/C_1.pt',\n",
       " '../input/large-roberta-b32-steplr3-drop-tb-rmf-h56-1/C_2.pt',\n",
       " '../input/large-roberta-b32-steplr3-drop-tb-rmf-h56-1/C_0.pt',\n",
       " '../input/large-roberta-b32-steplr3-drop-tb-rmf-h56-2/C_3.pt',\n",
       " '../input/large-roberta-b32-steplr3-drop-tb-rmf-h56-2/C_4.pt']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initailize weight\n",
      "initailize weight\n",
      "initailize weight\n",
      "initailize bias\n",
      "initailize weight\n",
      "initailize bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuestModel(\n",
       "  (bert_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (category_embedding): Embedding(6, 256)\n",
       "  (host_embedding): Embedding(64, 256)\n",
       "  (embedding_dropout): SpatialDropout(p=0.5, inplace=False)\n",
       "  (lstm1): LSTM(2048, 512, batch_first=True, bidirectional=True)\n",
       "  (lstm2): GRU(1024, 512, batch_first=True, bidirectional=True)\n",
       "  (atten1): Attention()\n",
       "  (atten2): Attention()\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=3592, out_features=21, bias=True)\n",
       "  (fc2): Linear(in_features=3592, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = QuestModel(n_cat, cat_emb, n_host, host_emb, num_labels=30)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [02:27, 29.52s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, weight in tqdm(enumerate(pretrain_weighted)):\n",
    "    model.load_state_dict(torch.load(weight))\n",
    "    results.append(predict_result(model, test_loader))\n",
    "    \n",
    "output_large = np.zeros((len(test_set),30))\n",
    "for result in results:\n",
    "    output_large += result\n",
    "output_large /= len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.46754766e-01, 8.00233185e-01, 1.00257943e-01, 7.24978971e-01,\n",
       "       7.70410395e-01, 8.02530169e-01, 6.78361404e-01, 5.70091641e-01,\n",
       "       5.60915333e-01, 2.85077139e-04, 5.07255709e-01, 7.94069052e-01,\n",
       "       7.81184603e-03, 4.85957157e-02, 2.53684779e-03, 7.96837751e-03,\n",
       "       2.11237656e-01, 7.43329749e-02, 5.58339846e-01, 2.37485243e-04,\n",
       "       9.45630944e-01, 9.25696194e-01, 5.91083348e-01, 9.62860894e-01,\n",
       "       9.65110230e-01, 8.37498164e-01, 7.71636002e-02, 4.70619030e-02,\n",
       "       7.97724640e-01, 9.17057133e-01])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_large[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del results\n",
    "del result\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model4: DeepAnxiety's XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() ## Clear_Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED= 1414\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_handmade_features= test_df[handmade_cols + ['qa_word_overlap_norm', 'qta_word_overlap_norm']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlent_config = transformers.XLNetConfig.from_json_file(XLNET_CONFIG_PATH)\n",
    "xlent_config.output_hidden_states = True\n",
    "tokenizer = transformers.XLNetTokenizer(XLNET_VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def _get_segments2(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def _trim_input(title, question, answer, max_sequence_length, \n",
    "                t_max_len=30, q_max_len=239, a_max_len=239):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n",
    "        \n",
    "        t = t[:t_new_len]\n",
    "        q_len_head = round(q_new_len/3)\n",
    "        q_len_tail = -1* (q_new_len -q_len_head)\n",
    "        a_len_head = round(a_new_len/3)\n",
    "        a_len_tail = -1* (a_new_len -a_len_head) \n",
    "        \n",
    "        q = q[:q_len_head]+q[q_len_tail:]\n",
    "        a = a[:a_len_head]+a[a_len_tail:]\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "\n",
    "def convert_xlnet(title, question, answer, max_sequence_length, tokenizer, \n",
    "                  t_max_len_seq=30, q_max_len_seq=239, a_max_len_seq=239):\n",
    "    \n",
    "    all_tokens   = []\n",
    "    all_masks    = []\n",
    "    all_segments = []\n",
    "    \n",
    "    longer = 0\n",
    "    \n",
    "    for t, q, a in tqdm(zip(title, question, answer)):\n",
    "        \n",
    "        tokens_t, tokens_q, tokens_a  = _trim_input(t, q, a, max_sequence_length=max_sequence_length)\n",
    "        #print(tokens_t)\n",
    "        #print(tokens_q)\n",
    "        #print(tokens_a)\n",
    "        \n",
    "        stoken = tokens_t + [\"[SEP]\"] + tokens_q + [\"[SEP]\"] + tokens_a + [\"[SEP]\"] + [\"[CLS]\"]\n",
    "        ##############\n",
    "        #token_ids\n",
    "        ##############\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(stoken)\n",
    "        input_ids = token_ids + [0] * (max_sequence_length-len(token_ids))\n",
    "        \n",
    "        #############\n",
    "        #input_masks\n",
    "        #############\n",
    "        attention_masks = _get_masks(stoken, max_sequence_length)\n",
    "        \n",
    "        ##############\n",
    "        #input_segments\n",
    "        ###############\n",
    "        input_segments = _get_segments2(stoken, max_sequence_length)\n",
    "        \n",
    "        all_tokens.append(input_ids)\n",
    "        all_masks.append(attention_masks)\n",
    "        all_segments.append(input_segments)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "476it [00:01, 410.03it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tokens, test_masks, test_segments = convert_xlnet(test_df[\"question_title\"],\n",
    "                                                       test_df[\"question_body\"], \n",
    "                                                       test_df[\"answer\"],\n",
    "                                                       max_sequence_length=512, \n",
    "                                                       tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_cat, cat_emb, n_host, host_emb, num_labels):\n",
    "        super().__init__()\n",
    "        BERT_DIMS = 1024\n",
    "        LSTM_UNITS = 512\n",
    "        model_path = os.path.join(XLNET_MODEL_PATH)\n",
    "        self.bert_model = transformers.XLNetModel.from_pretrained(model_path, from_tf=True, config=xlent_config)\n",
    "        set_trainable(self.bert_model, False)\n",
    "        \n",
    "        self.category_embedding = nn.Embedding(n_cat, cat_emb)\n",
    "        self.host_embedding     = nn.Embedding(n_host, host_emb)\n",
    "        self.embedding_dropout  = SpatialDropout(0.5)\n",
    "        self.lstm1              = nn.LSTM(BERT_DIMS*2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2              = nn.GRU(LSTM_UNITS*2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.atten1             = Attention(LSTM_UNITS*2, 512)\n",
    "        self.atten2             = Attention(LSTM_UNITS*2, 512)\n",
    "        self.dropout1           = nn.Dropout(0.2)\n",
    "        self.fc1                = nn.Linear(LSTM_UNITS*6 + int(cat_emb) + int(host_emb) + 8, 21)\n",
    "        self.fc2                = nn.Linear(LSTM_UNITS*6 + int(cat_emb) + int(host_emb) + 8, 9)\n",
    "        self._init_weights(self.category_embedding)\n",
    "        self._init_weights(self.host_embedding)\n",
    "        self._init_weights(self.fc1)\n",
    "        self._init_weights(self.fc2)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            print(\"initailize weight\")\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            print(\"initailize bias\")\n",
    "            module.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, token_ids, masks, segment, hosts, categories, handmades):\n",
    "        \n",
    "        category_embed = self.category_embedding(categories)\n",
    "        host_embed     = self.host_embedding(hosts)\n",
    "        external_features = torch.cat((category_embed, host_embed, handmades), 1)\n",
    "                                      \n",
    "        seq_output, hidden_states = self.bert_model(input_ids=token_ids,\n",
    "                                                    attention_mask=masks,\n",
    "                                                    token_type_ids=segment)\n",
    "        \n",
    "        last_four_layer = torch.cat((hidden_states[-5], hidden_states[-6]), 2)\n",
    "        lstm_input      = self.embedding_dropout(last_four_layer)\n",
    "        lstm1_output, _ = self.lstm1(lstm_input)\n",
    "        lstm2_output, _ = self.lstm2(lstm1_output)\n",
    "        \n",
    "        meanpooled_output   = torch.mean(lstm2_output, 1)\n",
    "        maxpooled_output, _ = torch.max(lstm2_output, 1)\n",
    "        attention_output_q  = self.atten1(lstm2_output)\n",
    "        attention_output_a  = self.atten2(lstm2_output)\n",
    "        \n",
    "        pooled_output_q = torch.cat((meanpooled_output, maxpooled_output, attention_output_q, external_features), 1)\n",
    "        pooled_output_q = self.dropout1(pooled_output_q)\n",
    "        \n",
    "        pooled_output_a = torch.cat((meanpooled_output, maxpooled_output, attention_output_a, external_features), 1)\n",
    "        pooled_output_a = self.dropout1(pooled_output_a)\n",
    "        \n",
    "        q_results     = self.fc1(pooled_output_q)\n",
    "        a_results     = self.fc2(pooled_output_a)\n",
    "        results       = torch.cat((q_results, a_results), 1)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_weighted_1 = glob('../input/large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-1/*.pt')\n",
    "pretrain_weighted_2 = glob('../input/large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-2/*.pt')\n",
    "pretrain_weighted = pretrain_weighted_1 + pretrain_weighted_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-1/C_1.pt',\n",
       " '../input/large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-1/C_2.pt',\n",
       " '../input/large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-1/C_0.pt',\n",
       " '../input/large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-2/C_3.pt',\n",
       " '../input/large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-2/C_4.pt']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set    = QuestDataset_test(test_tokens, test_masks, test_segments,\n",
    "                                test_df['host'].values,\n",
    "                                test_df['category'].values,\n",
    "                                test_handmade_features)\n",
    "test_loader = DataLoader(test_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initailize weight\n",
      "initailize weight\n",
      "initailize weight\n",
      "initailize bias\n",
      "initailize weight\n",
      "initailize bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuestModel(\n",
       "  (bert_model): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 1024)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (12): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (13): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (14): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (15): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (16): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (17): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (18): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (19): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (20): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (21): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (22): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (23): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (category_embedding): Embedding(6, 256)\n",
       "  (host_embedding): Embedding(64, 256)\n",
       "  (embedding_dropout): SpatialDropout(p=0.5, inplace=False)\n",
       "  (lstm1): LSTM(2048, 512, batch_first=True, bidirectional=True)\n",
       "  (lstm2): GRU(1024, 512, batch_first=True, bidirectional=True)\n",
       "  (atten1): Attention()\n",
       "  (atten2): Attention()\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=3592, out_features=21, bias=True)\n",
       "  (fc2): Linear(in_features=3592, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = QuestModel(n_cat, cat_emb, n_host, host_emb, num_labels=30)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [06:14, 74.98s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, weight in tqdm(enumerate(pretrain_weighted)):\n",
    "    model.load_state_dict(torch.load(weight))\n",
    "    results.append(predict_result(model, test_loader, batch_size=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_xlent = np.zeros((len(test_set),30))\n",
    "for result in results:\n",
    "    output_xlent += result\n",
    "output_xlent /= len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.56954038e-01, 7.53987896e-01, 2.35867222e-01, 7.33301818e-01,\n",
       "       6.63828313e-01, 6.98445797e-01, 7.09063935e-01, 6.02101219e-01,\n",
       "       4.84228837e-01, 3.81849470e-04, 6.31998843e-01, 7.74403799e-01,\n",
       "       2.44912617e-03, 6.06968559e-02, 7.66801025e-04, 3.93287488e-03,\n",
       "       3.79134666e-02, 2.21464265e-02, 7.71949387e-01, 1.33851613e-04,\n",
       "       9.52072585e-01, 9.35963130e-01, 6.14697981e-01, 9.70498788e-01,\n",
       "       9.80776143e-01, 8.62422705e-01, 1.71736382e-02, 1.02984903e-02,\n",
       "       9.34757733e-01, 9.32515740e-01])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_xlent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del results\n",
    "del result\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part4: Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df  = pd.read_csv(os.path.join(DATA_DIR,\"test.csv\"))\n",
    "train_df  = pd.read_csv(os.path.join(DATA_DIR,\"train.csv\"))\n",
    "output_categories = list(train_df.columns[11:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "culture_category_list = list(test_df[(test_df['host'] == 'english.stackexchange.com') | (test_df['host'] == 'ell.stackexchange.com')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_raters_dict = {\n",
    " 'question_asker_intent_understanding': 18,\n",
    " 'question_body_critical': 18,\n",
    " 'question_conversational': 6,\n",
    " 'question_expect_short_answer': 6,\n",
    " 'question_fact_seeking': 6,\n",
    " 'question_has_commonly_accepted_answer': 6,\n",
    " 'question_interestingness_others': 18,\n",
    " 'question_interestingness_self': 18,\n",
    " 'question_multi_intent': 6,\n",
    " 'question_not_really_a_question': 6,\n",
    " 'question_opinion_seeking': 6,\n",
    " 'question_type_choice': 6,\n",
    " 'question_type_compare': 6,\n",
    " 'question_type_consequence': 6,\n",
    " 'question_type_definition': 6,\n",
    " 'question_type_entity': 6,\n",
    " 'question_type_instructions': 6,\n",
    " 'question_type_procedure': 6,\n",
    " 'question_type_reason_explanation': 6,\n",
    " 'question_type_spelling': 3,\n",
    " 'question_well_written': 18,\n",
    " 'answer_helpful': 18,\n",
    " 'answer_level_of_information': 18,\n",
    " 'answer_plausible': 18,\n",
    " 'answer_relevance': 18,\n",
    " 'answer_satisfaction': 30,\n",
    " 'answer_type_instructions': 6,\n",
    " 'answer_type_procedure': 6,\n",
    " 'answer_type_reason_explanation': 6,\n",
    " 'answer_well_written': 18\n",
    " }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submssion_trick1(output):\n",
    "    #min_rater_value = list(num_raters_dict.values())\n",
    "    predictions = np.zeros_like(output)\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            col = output_categories[j]\n",
    "            num_raters = num_raters_dict[col]\n",
    "            if j == 19:\n",
    "                if i not in culture_category_list:\n",
    "                    predictions[i][j] = 0.0\n",
    "                else:\n",
    "                    predictions[i][j] = np.around(output[i][j] * num_raters) / num_raters\n",
    "            else:\n",
    "                predictions[i][j] = np.around(output[i][j] * num_raters) / num_raters\n",
    "            if num_raters == 18:\n",
    "                predictions[i][j] = max(predictions[i][j], 1/3)\n",
    "            if num_raters == 30:\n",
    "                predictions[i][j] = max(predictions[i][j], 1/5)\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submssion_trick2(output):\n",
    "    #min_rater_value = list(num_raters_dict.values())\n",
    "    predictions = np.zeros_like(output)\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            col = output_categories[j]\n",
    "            num_raters = num_raters_dict[col]\n",
    "            if j == 19:\n",
    "                if i not in culture_category_list:\n",
    "                    predictions[i][j] = 0.0\n",
    "                else:\n",
    "                    predictions[i][j] = np.floor(output[i][j] * num_raters) / num_raters\n",
    "            else:\n",
    "                predictions[i][j] = np.floor(output[i][j] * num_raters) / num_raters\n",
    "            if num_raters == 18:\n",
    "                predictions[i][j] = max(predictions[i][j], 1/3)\n",
    "            if num_raters == 30:\n",
    "                predictions[i][j] = max(predictions[i][j], 1/5)\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 0.7*(0.55*(output_base*0.4 + output_base2*0.6) + 0.45*output_xlent) + 0.3*output_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "for i in range(30):\n",
    "    output[:, i] = scaler.fit_transform(output[:, i].reshape(-1, 1)).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = submssion_trick2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions1 = submssion_trick1(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n",
    "#best_submission_right_now =  pd.read_csv(\"../input/quest-test/submission.csv\")\n",
    "#best_submission_right_now =  pd.read_csv(\"../input/quest-best-single-model-2/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_score = best_submission_right_now.iloc[: ,1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(30):\n",
    "   #print(submission.columns[i+1], len(np.unique(best_score[:, i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(30):\n",
    "    #print(submission.columns[i+1])\n",
    "    #print(np.unique(predictions1[:, i]))\n",
    "    #print(Counter(predictions1[:, i]))\n",
    "    #print(np.unique(predictions[:, i]))\n",
    "    #print(Counter(predictions[:, i]))\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submission Checker\n",
    "for i in range(30): \n",
    "    if len(np.unique(predictions[:, i])) == 1:\n",
    "        print(submission.columns[i+1])\n",
    "        print(np.unique(predictions[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submission Checker2\n",
    "##Please Comment Out When you would like to submit.!!!!!\n",
    "#total_score = 0\n",
    "#for i in range(30):\n",
    "    #score = spearmanr(predictions[:, i], best_score[:, i]).correlation\n",
    "    #total_score += score\n",
    "    #print(submission.columns[i+1], score)\n",
    "    \n",
    "#print(total_score / 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.loc[:, 'question_asker_intent_understanding':] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>245</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>257</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>267</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>284</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>292</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.833333                0.666667   \n",
       "1     46                             0.500000                0.333333   \n",
       "2     70                             0.666667                0.611111   \n",
       "3    132                             0.611111                0.333333   \n",
       "4    200                             0.722222                0.333333   \n",
       "5    245                             0.777778                0.888889   \n",
       "6    257                             0.611111                0.333333   \n",
       "7    267                             0.888889                0.777778   \n",
       "8    284                             0.555556                0.333333   \n",
       "9    292                             0.833333                0.500000   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.166667                      0.500000   \n",
       "1                 0.000000                      0.666667   \n",
       "2                 0.000000                      0.666667   \n",
       "3                 0.000000                      0.666667   \n",
       "4                 0.000000                      0.833333   \n",
       "5                 0.000000                      0.500000   \n",
       "6                 0.000000                      0.666667   \n",
       "7                 0.333333                      0.666667   \n",
       "8                 0.000000                      0.666667   \n",
       "9                 0.000000                      0.833333   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.500000                               0.500000   \n",
       "1               0.666667                               0.833333   \n",
       "2               0.833333                               0.833333   \n",
       "3               0.500000                               0.833333   \n",
       "4               0.500000                               0.833333   \n",
       "5               0.833333                               0.833333   \n",
       "6               0.500000                               0.833333   \n",
       "7               0.500000                               0.666667   \n",
       "8               0.666667                               0.833333   \n",
       "9               0.833333                               0.833333   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.777778                       0.611111   \n",
       "1                         0.333333                       0.333333   \n",
       "2                         0.388889                       0.333333   \n",
       "3                         0.333333                       0.333333   \n",
       "4                         0.611111                       0.611111   \n",
       "5                         0.500000                       0.333333   \n",
       "6                         0.333333                       0.333333   \n",
       "7                         0.722222                       0.722222   \n",
       "8                         0.333333                       0.333333   \n",
       "9                         0.666667                       0.500000   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.500000  ...               0.888889        0.666667   \n",
       "1               0.166667  ...               0.333333        0.833333   \n",
       "2               0.000000  ...               0.777778        0.555556   \n",
       "3               0.000000  ...               0.333333        0.833333   \n",
       "4               0.000000  ...               0.333333        0.555556   \n",
       "5               0.166667  ...               0.888889        0.777778   \n",
       "6               0.000000  ...               0.444444        0.888889   \n",
       "7               0.000000  ...               0.833333        0.722222   \n",
       "8               0.333333  ...               0.500000        0.888889   \n",
       "9               0.000000  ...               0.666667        0.833333   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.333333          0.722222          0.722222   \n",
       "1                     0.388889          0.833333          0.833333   \n",
       "2                     0.333333          0.611111          0.555556   \n",
       "3                     0.555556          0.833333          0.888889   \n",
       "4                     0.388889          0.722222          0.611111   \n",
       "5                     0.388889          0.722222          0.722222   \n",
       "6                     0.611111          0.888889          0.944444   \n",
       "7                     0.611111          0.833333          0.833333   \n",
       "8                     0.555556          0.888889          0.944444   \n",
       "9                     0.444444          0.888889          0.888889   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.533333                  0.000000               0.000000   \n",
       "1             0.700000                  0.833333               0.333333   \n",
       "2             0.466667                  0.000000               0.166667   \n",
       "3             0.766667                  0.833333               0.333333   \n",
       "4             0.500000                  0.166667               0.333333   \n",
       "5             0.733333                  0.000000               0.500000   \n",
       "6             0.833333                  0.833333               0.333333   \n",
       "7             0.700000                  0.000000               0.000000   \n",
       "8             0.800000                  0.833333               0.333333   \n",
       "9             0.766667                  0.333333               0.166667   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.833333             0.722222  \n",
       "1                        0.000000             0.333333  \n",
       "2                        0.833333             0.444444  \n",
       "3                        0.666667             0.555556  \n",
       "4                        0.500000             0.555556  \n",
       "5                        0.833333             0.555556  \n",
       "6                        0.500000             0.777778  \n",
       "7                        0.833333             0.777778  \n",
       "8                        0.333333             0.722222  \n",
       "9                        0.833333             0.777778  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submission = pd.read_csv(\"../input/quest-best-single-model-2/submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
