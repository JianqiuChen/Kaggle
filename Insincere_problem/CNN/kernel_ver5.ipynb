{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\npath = '../input'\nmax_features = 50000\nmax_len = 100\n\n\n#train Data \ncsv_path_train = os.path.join(path, 'train.csv')\ncsv_data_train = pd.read_csv(csv_path_train)\ntrain_data, val_data = train_test_split(csv_data_train, test_size=0.1, random_state=2018)\n\ntrain_X = train_data[\"question_text\"].fillna(\"_na_\").values\nval_X = val_data[\"question_text\"].fillna(\"_na_\").values\ntrain_y = train_data['target'].values\nval_y = val_data['target'].values\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\nword_index = tokenizer.index_word\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntrain_X = pad_sequences(train_X, maxlen=100)\nval_X = pad_sequences(val_X, maxlen=100)        \n\n#test_Data\ncsv_path_test = os.path.join(path, 'test.csv')\ncsv_data_test = pd.read_csv(csv_path_test)\n\ntest_X = csv_data_test[\"question_text\"].fillna(\"_na_\").values\ntest_X = tokenizer.texts_to_sequences(test_X)\ntest_X = pad_sequences(test_X, maxlen=100)\n\ntest_qid = csv_data_test['qid']\n\ndel csv_data_train\ndel csv_data_test\n\nprint(train_X.shape, val_X.shape, test_X.shape, train_y.shape, val_y.shape, test_qid.shape, len(word_index))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90e0d70a60b2cdc057346517407653d3a179250a"
      },
      "cell_type": "code",
      "source": "from tqdm import tqdm\n\nprint(os.listdir(\"../input/embeddings/glove.840B.300d\"))\nembed_size = 300\ndef create_embed_matrix():\n    embedding_file_path = os.path.join(path, 'embeddings', 'glove.840B.300d', 'glove.840B.300d.txt')\n    \n    embed_dict = {}\n    with open(embedding_file_path) as file:\n        for o in tqdm(file):\n            split_o = o.split(\" \")\n            embed_dict[split_o[0]] = np.array(split_o[1:], dtype=np.float32)\n        \n    all_embeds = np.stack(embed_dict.values())\n    embed_mean,embed_std = all_embeds.mean(), all_embeds.std()\n    \n    embed_matrix = np.random.uniform(embed_mean, embed_std, size=(max_features, embed_size))\n    \n    for key, word in tqdm(word_index.items()):\n        if key >= max_features: continue\n        embed_vector = embed_dict.get(word)\n        if embed_vector is not None: \n            embed_matrix[key] = embed_vector\n    \n    return embed_matrix\n\nembed_matrix = create_embed_matrix()\nprint(embed_matrix.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7cf77fb371b2bda7a6e3275dc5ed9f78937aef8b"
      },
      "cell_type": "code",
      "source": "#\"Convolutional Neural Networks for Sentence Classification\"\n#https://arxiv.org/pdf/1408.5882.pdf\n#\"Character-level Convolutional Networks for Text Classification∗\"\n#https://www.kaggle.com/yekenot/2dcnn-textclassifier\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom keras.layers import Input, Conv1D, Embedding, MaxPool1D, Dense, Dropout, Flatten, Concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import adam\nfrom keras.models import Model\n\nclass create_model(object):\n\n    def __init__(self, num_filter, Output_Unit=None, kernel_size=None, max_features=50000, max_len=100, embed_size=300, pretrain_embeding_matrix=None):\n\n        self.max_features = max_features\n        self.max_len = max_len\n        self.embed_size = embed_size\n        self.kernel_size = kernel_size\n        self.Output_Unit = Output_Unit\n        self.num_filter = num_filter\n        self.pretrain_embed_matrix = pretrain_embeding_matrix\n        self.adam = adam(lr=0.01)\n\n    def get_chara_level_Net(self, check=False):\n\n        inputs = Input(shape=(self.max_len,))\n        x = Embedding(self.max_features, self.embed_size, weights=[self.pretrain_embed_matrix])(inputs)  # shape: batch_size, max_len, emb_size\n        net = Conv1D(filters=self.num_filter, kernel_size=(7),\n                        kernel_initializer='he_normal', activation='tanh')(x)\n        net = MaxPool1D(pool_size=(3))(net)\n        net = Conv1D(filters=self.num_filter, kernel_size=(5),\n                    kernel_initializer='he_normal', activation='tanh')(net)\n        net = MaxPool1D(pool_size=(3))(net)\n\n        net = Flatten()(net)\n        net = Dense(self.Output_Unit, activation='tanh', use_bias=True)(net)\n        net = Dropout(0.1)(net)\n        net = Dense(self.Output_Unit, activation='tanh', use_bias=True)(net)\n        outputs = Dense(1, activation='sigmoid', use_bias=False)(net)\n        model = Model(inputs=inputs, outputs=outputs)\n        model.compile(optimizer=self.adam, loss='binary_crossentropy', metrics=['accuracy'])\n\n        if check:\n            model.summary()\n\n        return model\n\nmodel = create_model(num_filter=48, Output_Unit=96, pretrain_embeding_matrix=embed_matrix).get_chara_level_Net(check=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c5b9c570d3f8bc649bc7ac118d3c91436e1deac"
      },
      "cell_type": "code",
      "source": "model.fit(train_X, train_y, batch_size=1024, epochs=3, validation_data=(val_X, val_y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e5814f3c08c1ba95dcda79cae829de1b91594e2b"
      },
      "cell_type": "code",
      "source": "from sklearn import metrics\n\npred_glove_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nprint(pred_glove_val_y[pred_glove_val_y==1])\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_glove_val_y > thresh).astype(int))))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d3e1e810262c55a6d3d4dfe9a16a7b89c155b20"
      },
      "cell_type": "code",
      "source": "output = model.predict([test_X], batch_size=1024, verbose=1)\noutput = (output > 0.35).astype(int)\noutput = output.flatten()\n\nsubmission = pd.DataFrame({'qid':test_qid,\n                          'prediction':output})\n\nsubmission.head()\nsubmission.to_csv('submission_character_level.csv', index=False)\nprint(os.listdir('./'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65eae75044c16ae65b6f162f23da8409bbdc7f46"
      },
      "cell_type": "code",
      "source": "class create_model(object):\n\n    def __init__(self, num_filter, Ouput_Unit=None, kernel_size=None, max_features=50000, max_len=100, embed_size=300, pretrain_embeding_matrix=None):\n\n        self.max_features = max_features\n        self.max_len = max_len\n        self.embed_size = embed_size\n        self.kernel_size = kernel_size\n        self.Ouput_Unit = Ouput_Unit\n        self.num_filter = num_filter\n        self.pretrain_embed_matrix = pretrain_embeding_matrix\n        self.adam = adam(lr=0.01)\n\n    def get_cnn(self, check=False):\n\n        \"\"\"\n        A sentence of length n (padded where necessary) is represented as\n        x1:n = x1 ⊕ x2 ⊕ . . . ⊕ xn\n        where ⊕ is the concatenation operator.\n\n        A convolution operation involves a filter w ∈ R hk, which is applied to a window\n        of h words to produce a new feature. For example, a feature ci is generated\n        from a window of words xi:i+h−1 by\n        ci = f(w · xi:i+h−1 + b).\n\n        This filter is applied to each possible window of words in the sentence.\n        The model uses multiple filters, (with varying window sizes)\n        loss are passed to a fully connected softmax layer whose output is the probability\n        distribution over labels.\n\n        More discussion:\n        While we had expected performance gains through the use of pre-trained vectors,\n        we were surprised at the magnitude of the gains.\n\n        \"\"\"\n\n        inputs = Input(shape=(self.max_len,))\n        x = Embedding(self.max_features, self.embed_size, weights=[self.pretrain_embed_matrix])(inputs) # shape: batch_size, max_len, emb_size\n\n        #create feature map, to each possible window\n        #c = [c1, c2, ..., cn−h + 1]\n\n        conv_0 = Conv1D(filters=self.num_filter, kernel_size=(self.kernel_size[0]),\n                        kernel_initializer='he_normal', activation='tanh')(x) # (batch_size, max_len-kernel_size + 1 , num_filter)\n        conv_1 = Conv1D(filters=self.num_filter, kernel_size=(self.kernel_size[1]),\n                        kernel_initializer='he_normal', activation='tanh')(x) #  (batch_size, max_len-kernel_size + 1 , num_filter)\n        conv_2 = Conv1D(filters=self.num_filter, kernel_size=(self.kernel_size[2]),\n                        kernel_initializer='he_normal', activation='tanh')(x)\n        conv_3 = Conv1D(filters=self.num_filter, kernel_size=(self.kernel_size[3]),\n                        kernel_initializer='he_normal', activation='tanh')(x)\n\n        maxpool_0 = MaxPool1D(pool_size=(self.max_len - self.kernel_size[0] + 1))(conv_0)\n        maxpool_1 = MaxPool1D(pool_size=(self.max_len - self.kernel_size[1] + 1))(conv_1)\n        maxpool_2 = MaxPool1D(pool_size=(self.max_len - self.kernel_size[2] + 1))(conv_2)\n        maxpool_3 = MaxPool1D(pool_size=(self.max_len - self.kernel_size[3] + 1))(conv_3)\n\n        # Fully connected layer with dropout and softmax output\n\n        z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])\n        z = Flatten()(z)\n        #dropout on the penultimate layer with a constraint on l2-norms of the weight vectors(Hintonetal., 2012).\n        z = Dropout(0.1)(z)\n        outputs = Dense(1, activation='sigmoid', use_bias=True)(z)\n\n        model = Model(inputs=inputs, outputs=outputs)\n        model.compile(optimizer=self.adam, loss='binary_crossentropy', metrics=['accuracy'])\n\n        if check:\n            model.summary()\n\n        return model\n    \nmodel_cnn = create_model(num_filter=42, kernel_size=[1,3,5,7], pretrain_embeding_matrix=embed_matrix).get_cnn(check=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d08f2e4a47e5318a5a7d8306eec2476d7c0d9ad8"
      },
      "cell_type": "code",
      "source": "model_cnn.fit(train_X, train_y, batch_size=1024, epochs=2, validation_data=(val_X, val_y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9a4b547873b059e094c3b899e2fca79ca99a58ed"
      },
      "cell_type": "code",
      "source": "pred_glove_val_y = model_cnn.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_glove_val_y > thresh).astype(int))))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2bc415fa107eb19fd674c6012be34d3af5f7e684"
      },
      "cell_type": "code",
      "source": "output = model_cnn.predict([test_X], batch_size=1024, verbose=1)\noutput = (output > 0.34).astype(int)\noutput = output.flatten()\n\nsubmission = pd.DataFrame({'qid':test_qid,\n                          'prediction':output})\n\nsubmission.head()\nsubmission.to_csv('submission.csv', index=False)\nprint(os.listdir('./'))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}