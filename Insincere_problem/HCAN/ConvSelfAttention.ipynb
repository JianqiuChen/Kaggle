{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06f87cf07f95ae0e45fa8c8fa0cba5ab272e3621",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.utils import Sequence\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.optimizers import Adam\nfrom keras.callbacks import *\nfrom tqdm import tqdm\nfrom sklearn import metrics\nimport pickle\nfrom keras.engine.topology import Layer\nfrom keras import backend as K\nimport gc\nfrom tensorflow.keras.backend import batch_dot\nimport tensorflow as tf\nimport re",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "path = '../input/'\nmax_features = 120000\nmax_len = 70\nmax_word_len = 30\nembed_size = 300\nn_split = 5\nn_epoch = 5\n\ntrain_df = pd.read_csv(os.path.join(path, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(path, 'test.csv'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "158bd87e061af4dce3ac7bea7a501dd5eff6a1f8"
      },
      "cell_type": "code",
      "source": "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, '{puncts}')\n    return x\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\nmispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5ffcee2862855af9511a7c007edf5ddf4972ab0"
      },
      "cell_type": "code",
      "source": "train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n\n# Clean the text\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n    \n# Clean numbers\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n    \n# Clean speelings\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95d295c7fad357042f8bca78875eb3a91d985987"
      },
      "cell_type": "code",
      "source": "##################################\n##Prepare data for train/test\n##################################\n\ntr_X = train_df[\"question_text\"].fillna(\"_na_\").values\ntr_y = train_df['target'].values\nte_X = test_df[\"question_text\"].fillna(\"_na_\").values\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(tr_X))\nword_index = tokenizer.index_word\n        \ntr_X = tokenizer.texts_to_sequences(tr_X)\ntr_X = pad_sequences(tr_X, maxlen=max_len, padding='post')       \nte_X = tokenizer.texts_to_sequences(te_X)\nte_X = pad_sequences(te_X, maxlen=max_len, padding='post')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77bd387eacd712e677635d75a2c3c9abca6f3f3f"
      },
      "cell_type": "code",
      "source": "##################################\n##create input function for train/test\n##################################\n\n#class input_creator(Sequence):\n    \n    #def __init__(self, x, y=None, batch_size=3):\n        #super(input_creator, self).__init__()\n        #self.x = x\n        #self.y = y\n        #self.batch_size = batch_size\n        \n    #def __getitem__(self, index):\n        #start = self.batch_size * index\n        #end = min(start + self.batch_size, len(self.x))\n        #size  = end - start\n        #batch_x = self.x[start:end, :]\n\n        #if self.y is not None:\n            #batch_y = self.y[start:end]\n            #return batch_x, batch_y\n        #else:\n            #return batch_x\n\n    #def __len__(self):\n        #return (self.x.shape[0] + self.batch_size - 1) // self.batch_size\n\ndef input_creator(x, y, batch_size):\n    while True: \n        for i in range((x.shape[0]//batch_size)+1):\n            start = batch_size * i\n            end = min(start+batch_size, x.shape[0])\n            \n            batch_x = x[start:end, :]\n            batch_y = y[start:end]\n            \n            yield batch_x, batch_y",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90e0d70a60b2cdc057346517407653d3a179250a"
      },
      "cell_type": "code",
      "source": "## FUNCTIONS TAKEN FROM https://www.kaggle.com/gmhost/gru-capsule\n\ndef load_glove(word_index):\n    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n    \n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = -0.005838499,0.48782197\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    # Why random embedding for OOV? what if use mean?\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    #embedding_matrix = np.random.normal(emb_mean, 0, (nb_words, embed_size)) # std 0\n    for i, word in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n    \ndef load_fasttext(word_index):    \n    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    #embedding_matrix = np.random.normal(emb_mean, 0, (nb_words, embed_size))\n    for i, word in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix\n\ndef load_para(word_index):\n    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = -0.0053247833,0.49346462\n    embed_size = all_embs.shape[1]\n\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for i, word in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix\n\nglove_embeddings = load_glove(word_index)\nparagram_embeddings = load_para(word_index)\nfasttext_embeddings = load_fasttext(word_index)\n\nembedding_matrix = np.mean([glove_embeddings, paragram_embeddings, fasttext_embeddings], axis=0)\n\n# vocab = build_vocab(df['question_text'])\n# add_lower(embedding_matrix, vocab)\ndel glove_embeddings, paragram_embeddings, fasttext_embeddings\ngc.collect()\n\nnp.shape(embedding_matrix)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "33152b7589f3d55821fceed5908b2d4d487be63e"
      },
      "cell_type": "code",
      "source": "splits = list(StratifiedKFold(n_splits=n_split, shuffle=True, random_state=2018).split(tr_X, tr_y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c751851e2fe9dacee2e0436f2c459d087582114"
      },
      "cell_type": "code",
      "source": "class PositionEmbedding(Layer):\n\n    def __init__(self, input_dim, output_dim,\n                 mode='Add', **kwargs):\n\n        self.input_dim   = input_dim #max_position_embeddings\n        self.output_dim  = output_dim\n        self.mode = mode\n        super(PositionEmbedding, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n\n        self.embeddings = self.add_weight(name='position_matrix',\n                                 shape=(self.input_dim, self.output_dim),\n                                 initializer='glorot_uniform',\n                                         )\n        super(PositionEmbedding, self).build(input_shape)\n\n    def call(self, x):\n\n        input_shape = K.shape(x)\n\n        if self.mode == 'Add':\n            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], input_shape[2]\n        else:\n            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], self.output_dim #concat\n\n        #assert seq_len == self.input_dim\n        position_embed = K.tile(K.expand_dims(self.embeddings[:seq_len, :self.output_dim], axis=0), K.stack([batch_size, 1, 1]),)\n        \n        if self.mode == 'Add':\n            return x + position_embed\n        \n        return K.concatenate([x, position_embed], axis=-1)\n    def compute_output_shape(self, input_shape):\n        if self.mode == 'Concat':\n            return (input_shape[0], input_shape[1], input_shape[2]+self.output_dim)\n        return input_shape\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "64fa720d6e3534e76792516005c94dbe553d1080"
      },
      "cell_type": "code",
      "source": "class ConvSelfAttention(Layer):\n\n    def __init__(self, kernel_size, n_head, output_dim, use_bias=True, V_activation='elu', **kwargs):\n        \n        self.kernel_size = kernel_size\n        self.output_dim = output_dim\n        self.n_head = n_head\n        self.use_bias = use_bias\n        self.V_activation = V_activation\n        self.per_head_dim = int(self.output_dim / self.n_head)\n        super(ConvSelfAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n\n        self.WQ = self.add_weight(name='WQ',\n                                  shape=(self.kernel_size, input_shape[-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        self.WK = self.add_weight(name='WK',\n                                  shape=(self.kernel_size, input_shape[-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        self.WV = self.add_weight(name='WV',\n                                  shape=(self.kernel_size, input_shape[-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n\n        if self.use_bias:\n            self.BQ = self.add_weight(name='BQ',\n                                      shape=(self.output_dim,),\n                                      initializer='zeros',\n                                      trainable=True)\n            self.BK = self.add_weight(name='BK',\n                                      shape=(self.output_dim,),\n                                      initializer='zeros',\n                                      trainable=True)\n            self.BV = self.add_weight(name='BV',\n                                      shape=(self.output_dim,),\n                                      initializer='zeros',\n                                      trainable=True)\n\n        super(ConvSelfAttention, self).build(input_shape)\n\n    def call(self, x):\n\n        Q_seq = x #Query Embeddings\n        K_seq = x #Key Embeddings\n        V_seq = x #Value Embeddings\n\n        Q_seq = self.feature_extraction(Q_seq, self.WQ, self.BQ, activation='elu')\n        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.n_head, self.per_head_dim)) #(batch_size, max_len-kernel_size + 1, n_head, per_head_dim)\n        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n        K_seq = self.feature_extraction(K_seq, self.WK, self.BK, activation='elu') #(batch_size, max_len-kernel_size + 1, n_head, per_head_dim)\n        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.n_head, self.per_head_dim))\n        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n        V_seq = self.feature_extraction(V_seq, self.WV, self.BV, activation=self.V_activation) #(batch_size, max_len-kernel_size + 1, n_head, per_head_dim)\n        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.n_head, self.per_head_dim))\n        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))#(batch_size, n_head, max_len-kernel_size + 1, per_head_dim)\n\n        A = batch_dot(Q_seq, K_seq, axes=[3,3]) / self.per_head_dim**0.5 #(batch_size, n_head, max_len-kernel_size + 1, max_len-kernel_size+1)\n        A = K.softmax(A) #(batch_size n_head, max_len-kernel_size + 1, max_len-kernel_size+1)\n        A = batch_dot(A, V_seq, axes=[3,2]) \n        A = K.permute_dimensions(A, (0,2,1,3))\n        A = K.reshape(A, (-1, K.shape(A)[1], self.output_dim))\n\n        return A\n\n    def feature_extraction(self, inp, kernel, bias, activation='elu'):\n\n        x = K.conv1d(inp, kernel=kernel, padding='same', strides=1)\n        if self.use_bias:\n            x = K.bias_add(x, bias)\n        if activation == 'elu':\n            x = K.elu(x)\n        elif activation == 'tanh':\n            x = K.tanh(x)\n        else:\n            x = K.relu(x)\n\n        return x\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[1], self.output_dim)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8763c4efb3ee72e8686bd22d32fd3c6406e6c19c"
      },
      "cell_type": "code",
      "source": "class LayerNormalization(Layer):\n\n    def __init__(self, eps=1e-6, **kwargs):\n\n        self.eps = eps\n        super(LayerNormalization, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.gamma  = self.add_weight(name='gamma',\n                                     shape=(input_shape[-1:]),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        self.beta = self.add_weight(name='bata',\n                                  shape=(input_shape[-1:]),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n\n        super(LayerNormalization,self).build(input_shape)\n\n    def call(self, x):\n        \n        means = K.mean(x, axis=-1, keepdims=True)\n        std = K.std(x, axis=-1, keepdims=True)\n        x_norm = (x - means) / ((std + self.eps)**0.5)\n        output = self.gamma * x_norm + self.beta\n\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f3dc9dae9246b984e4ce1966b7b6e527e99fb043"
      },
      "cell_type": "code",
      "source": "class ConvTargetAttention(Layer):\n\n    def __init__(self, kernel_size, n_head, output_dim, use_bias=True, **kwargs):\n\n        self.kernel_size = kernel_size\n        self.output_dim = output_dim\n        self.n_head = n_head\n        self.use_bias = use_bias\n        self.per_head_dim = int(self.output_dim / self.n_head)\n        super(ConvTargetAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n\n        self.WK = self.add_weight(name='WK',\n                                  shape=(self.kernel_size, input_shape[1][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        self.WV = self.add_weight(name='WV',\n                                  shape=(self.kernel_size, input_shape[2][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n\n        if self.use_bias:\n            self.BK = self.add_weight(name='BK',\n                                      shape=(self.output_dim,),\n                                      initializer='zeros',\n                                      trainable=True)\n            self.BV = self.add_weight(name='BV',\n                                      shape=(self.output_dim,),\n                                      initializer='zeros',\n                                      trainable=True)\n\n        super(ConvTargetAttention, self).build(input_shape)\n\n    def call(self, inputs):\n\n        assert len(inputs) == 3\n        T_seq, K_seq, V_seq = inputs # T:(batch_size, embed_size)\n        T_seq = K.reshape(T_seq, (-1, self.n_head, self.per_head_dim))\n\n        K_seq = self.feature_extraction(K_seq, self.WK, self.BK, activation='elu')\n        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.n_head, self.per_head_dim))\n        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))  #(batch_size, n_head, seq_len-kernel_size+1, self.per_head_dim)\n        V_seq = self.feature_extraction(V_seq, self.WV, self.BV, activation='elu')\n        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.n_head, self.per_head_dim))\n        V_seq = K.permute_dimensions(V_seq, (0,2,1,3)) #(batch_size, n_head, seq_len-kernel_size+1, self.per_head_dim)\n\n        A = batch_dot(T_seq, K_seq, axes=[2, 3]) / self.per_head_dim**0.5\n        A = K.softmax(A)\n        A = batch_dot(A, V_seq, axes=[1, 2])\n        A = K.reshape(A, (-1, self.output_dim))\n\n        return A\n\n    def feature_extraction(self, inp, kernel, bias, activation='elu'):\n\n            x = K.conv1d(inp, kernel=kernel)\n            if self.use_bias:\n                x = K.bias_add(x, bias)\n            if activation == 'elu':\n                x = K.elu(x)\n            elif activation == 'tanh':\n                x = K.tanh(x)\n\n            return x\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0][0], input_shape[0][1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "25ef7d21c267374e721b85dc2376d716dfe18edb"
      },
      "cell_type": "code",
      "source": "#The code from \n#https://www.kaggle.com/braquino/5-fold-lstm-attention-fully-commented-0-694\n\ndef matthews_correlation(y_true, y_pred):\n    '''Calculates the Matthews correlation coefficient measure for quality\n    of binary classification problems.\n    '''\n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_pred_neg = 1 - y_pred_pos\n\n    y_pos = K.round(K.clip(y_true, 0, 1))\n    y_neg = 1 - y_pos\n\n    tp = K.sum(y_pos * y_pred_pos)\n    tn = K.sum(y_neg * y_pred_neg)\n\n    fp = K.sum(y_neg * y_pred_pos)\n    fn = K.sum(y_pos * y_pred_neg)\n\n    numerator = (tp * tn - fp * fn)\n    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n    return numerator / (denominator + K.epsilon())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28aa778d0671072ff7aae68ba7f10867e3cc377c",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "class main(object):\n\n    def __init__(self, max_features, max_len, pretrain_embed_matrix=None, embed_size=300):\n        \n        self.max_features = max_features\n        self.max_len = max_len\n        self.output_dim = 60\n        self.pretrain_embed_matrix = pretrain_embed_matrix\n        self.embed_size = embed_size\n\n    def build(self):\n\n        Inp = Input(shape=(self.max_len,))\n        \n        x  = Embedding(self.max_features, self.embed_size, weights=[self.pretrain_embed_matrix])(Inp)\n        x  = PositionEmbedding(self.max_len, self.embed_size)(x)\n        #x  = Dropout(0.1)(x)\n        \n        a_1 = ConvSelfAttention(kernel_size=3, n_head=6, output_dim=self.embed_size)(x)\n        a_2 = ConvSelfAttention(kernel_size=3, n_head=6, output_dim=self.embed_size, V_activation='tanh')(x)\n        a   = Lambda(lambda x: x[0] * x[1])([a_1, a_2])\n        M   = Add()([x, a]) # The Idea From Attention All you Need\n        M   = LayerNormalization()(M)\n        \n        #applied to each position separately and identically. \n        #representing a token of the input, \n        #and the same process is repeated for as many tokens as there are in the sentence.\n        F   = Conv1D(self.embed_size*2, kernel_size=1, activation='relu')(M)\n        F   = Conv1D(self.embed_size, kernel_size=1)(F)\n        F   = Add()([F, M])\n        F   = LayerNormalization()(F)\n        \n        x   = GlobalAvgPool1D()(F)\n        #x   = BatchNormalization()(x)\n        x   = Dropout(0.1)(x)\n        x   = Dense(1, activation='sigmoid', use_bias=True)(x)\n        \n        model = Model(Inp, x) \n\n        return model\n    \nmodel = main(max_features=max_features, max_len=max_len, pretrain_embed_matrix=embedding_matrix).build()\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b336977d4047b7f0b656504fef08961b1b3f3145"
      },
      "cell_type": "code",
      "source": "#https://arxiv.org/pdf/1608.03983.pdf\n#/1506.01186.pdf\n\nclass cyclr(Callback):\n\n    def __init__(self, max_lr, base_lr, step_size, gamma=None, mode='triangular'):\n\n        self.max_lr =  max_lr\n        self.base_lr = base_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n\n        self.clr_iteration = 0\n        self.trn_iteration = 0\n\n        if self.mode == 'triangular':\n            self.scale_fn = lambda x:1\n        elif self.mode == 'triangular2':\n            self.scale_fn = lambda x: 1/(2.**(x-1))\n        elif self.mode == 'exp_range':\n            self.scale_fn = lambda x: self.gamma**(x)\n\n        self.history = {}\n        self._reset()\n    \n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n\n    def clr(self):\n\n        cycle = np.floor(1 + self.clr_iteration/(2*self.step_size))\n        x = np.abs(self.clr_iteration/self.step_size-2*cycle)\n        if self.mode == 'exp_range':\n            lr = self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1-x))*self.scale_fn(self.clr_iteration)\n        else:\n            lr = self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1-x))*self.scale_fn(cycle)\n\n        return lr\n    \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n        \n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_batch_end(self, epoch, logs={}):\n        logs = logs or {}\n        self.trn_iteration +=1\n        self.clr_iteration +=1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iteration)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        K.set_value(self.model.optimizer.lr, self.clr())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0dcd9342880eafc6cfce65f4505a8eafb441daa9"
      },
      "cell_type": "code",
      "source": "cyc_lr = cyclr(max_lr=0.003, base_lr=0.001, step_size=300, gamma=0.99994, mode='exp_range')\n\nmodel.compile(optimizer=Adam(), loss=['binary_crossentropy'], metrics=[matthews_correlation, 'binary_accuracy'])\ncallbacks=[cyc_lr,\n           #EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001, verbose=1),\n           ModelCheckpoint('my_model.h5', save_best_only=True, save_weights_only=True,\n                          verbose=1, monitor='val_matthews_correlation', mode='max')]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1e5d740a76e7fc2715e3afbe21123a7b4675250"
      },
      "cell_type": "code",
      "source": "for i, split in enumerate(splits):\n    x_train_fold, y_train_fold, x_val_fold, y_val_fold = tr_X[split[0]],tr_y[split[0]],tr_X[split[1]],tr_y[split[1]]\n    \n    print('fold {}'.format(i))\n    print('Start Train')\n    model.fit_generator(input_creator(x_train_fold, y_train_fold, 512), steps_per_epoch= (x_train_fold.shape[0]//512)+1, \n                        epochs=5, validation_data=(x_val_fold, y_val_fold),shuffle=True, \n                        callbacks=callbacks, )\n    print('End Train')    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc0cccc6a6e17bbc5bf27bf8cfaa0251362d37cf"
      },
      "cell_type": "code",
      "source": "del model \nmodel2 = main(max_features=max_features, max_len=max_len, pretrain_embed_matrix=embedding_matrix).build()\nmodel2.load_weights('my_model.h5')\n\nbest_thresh = 0.1\nbest_score = 0.0\n\ntr_pred = model2.predict(tr_X, batch_size=800, verbose=1)\n\nfor thresh in [i * 0.01 for i in range(100)]:\n    thresh = np.round(thresh, 2)\n    score = metrics.f1_score(tr_y, (tr_pred < thresh).astype(int))\n    if score > best_score:\n        best_score = score\n        best_thresh = thresh\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, score))\nprint(best_thresh)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8305e1bf0c2ce06a6070f0640e9658443a7e0f79"
      },
      "cell_type": "code",
      "source": "for thresh in [i * 0.01 for i in range(100)]:\n    thresh = np.round(thresh, 2)\n    score = metrics.f1_score(tr_y, (tr_pred < thresh).astype(int))\n    if score > best_score:\n        best_score = score\n        best_thresh = thresh\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, score))\nprint(best_thresh)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5450f4f5e501913ac6500894be1fd592a02290a7"
      },
      "cell_type": "code",
      "source": "output = model2.predict(te_X, batch_size=800, verbose=1)\noutput = (output < best_thresh).astype(int)\noutput = output.flatten()\n\nsubmission = pd.DataFrame({'qid':test_df.qid.values,\n                          'prediction':output})\n\nprint(submission.head())\nsubmission.to_csv('submission.csv', index=False)\nprint(os.listdir('./'))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}