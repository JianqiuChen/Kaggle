{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06f87cf07f95ae0e45fa8c8fa0cba5ab272e3621"
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.utils import Sequence\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.optimizers import Adam\nfrom keras.callbacks import *\nfrom tqdm import tqdm\nfrom sklearn import metrics\nimport pickle",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "path = '../input/'\nmax_features = 50000\nmax_len = 100\nmax_word_len = 30\nembed_size = 300\nalphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\nw_index = 'word_index.pickle'\nchar_index = 'char_index.pickle'\n\ntrain_data = pd.read_csv(os.path.join(path, 'train.csv'))\ntest_data = pd.read_csv(os.path.join(path, 'test.csv'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95d295c7fad357042f8bca78875eb3a91d985987"
      },
      "cell_type": "code",
      "source": "##################################\n##Prepare data for train/test\n##################################\n\ntrain, val= train_test_split(train_data, test_size=0.2, random_state=2018)\n\ntr_X = train[\"question_text\"].fillna(\"_na_\").values\nv_X = val[\"question_text\"].fillna(\"_na_\").values\ntr_y = train['target'].values\nv_y = val['target'].values\nte_X = test_data[\"question_text\"].fillna(\"_na_\").values\n\nchar_dict = {}\nfor i, char in enumerate(alphabet):\n    char_dict[char] = i + 1\n    \ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(tr_X))\nword_index = tokenizer.index_word\n\nwith open(w_index, mode='wb') as f:\n    pickle.dump(word_index, f)\n\nwith open(char_index, mode='wb') as f:\n     pickle.dump(char_dict, f)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a6d7bd54980e52cf83c56f9c1a63727e8a6cbeb"
      },
      "cell_type": "code",
      "source": "##################################\n##create input for train/test\n##################################\n\nclass input_creater(Sequence):\n    \n    def __init__(self, x, y=None, batch_size=1024):\n        super(input_creater, self).__init__()\n        self.x = x\n        self.y = y\n        self.batch_size = batch_size\n        \n    def __getitem__(self, index):\n        start = self.batch_size * index\n        end = min(start + self.batch_size, len(self.x))\n        size  = end - start\n        inp2 = np.zeros((size, max_len, max_word_len))\n        batch_x = self.x[start:end]\n        \n        inp1 = tokenizer.texts_to_sequences(batch_x)\n        inp1 = pad_sequences(inp1, maxlen=max_len, padding='post')\n        \n        seqs = [text_to_word_sequence(s) for s in batch_x]\n        for i, s in enumerate(seqs): \n            m = []\n            for word in s: \n                a = []\n                for char in word:\n                    if char in char_dict.keys():\n                        a.append(char_dict[char])   \n                m.append(a)\n                if len(m) >= 100: break\n            m = pad_sequences(m, maxlen=max_word_len, padding='post')\n            for n in range(m.shape[0]):\n                inp2[i, n, :] = m[n,:]\n                \n        if self.y is not None:\n            batch_y = self.y[start:end]\n            return [inp1,inp2], batch_y\n        else:\n            return [inp1,inp2]\n\n    def __len__(self):\n        return (len(self.x) + self.batch_size - 1) // self.batch_size",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90e0d70a60b2cdc057346517407653d3a179250a"
      },
      "cell_type": "code",
      "source": "from tqdm import tqdm\n\ndef create_embed_matrix():\n    embedding_file_path = os.path.join(path, 'embeddings', 'glove.840B.300d', 'glove.840B.300d.txt')\n    \n    embed_dict = {}\n    with open(embedding_file_path) as file:\n        for o in tqdm(file):\n            split_o = o.split(\" \")\n            embed_dict[split_o[0]] = np.array(split_o[1:], dtype=np.float32)\n        \n    all_embeds = np.stack(embed_dict.values())\n    embed_mean,embed_std = all_embeds.mean(), all_embeds.std()\n    \n    embed_matrix = np.random.uniform(embed_mean, embed_std, size=(max_features, embed_size))\n    \n    for key, word in tqdm(word_index.items()):\n        if key >= max_features: continue\n        embed_vector = embed_dict.get(word)\n        if embed_vector is not None: \n            embed_matrix[key] = embed_vector\n    \n    return embed_matrix\n\nembed_matrix = create_embed_matrix()\nprint(embed_matrix.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f1309e1f888dfbf14e19f4b16b91722ab5a43b5"
      },
      "cell_type": "code",
      "source": "class Char_embed(object):\n\n    def __init__(self, filters,\n                 kernel, max_features,\n                 max_word_len,embed_size=30,\n                 highway=True):\n\n        self.filters = filters\n        self.kernel = kernel\n        self.max_features = max_features\n        self.max_word_len = max_word_len\n        self.embed_size = embed_size\n        self.highway = highway\n\n    def build(self):\n\n        inputs = Input(shape=(self.max_word_len,))\n\n        #Embedding\n        x = Embedding(self.max_features, self.embed_size, input_shape=(self.max_word_len,))(inputs) # shape: batch_size, max_len, emb_size\n\n        feature_maps = []\n\n        for i in range(len(self.kernel)):\n            conv = Conv1D(filters=self.filters[i], kernel_size=(self.kernel[i]),\n                          activation='tanh', name='conv_{}'.format(i))(x) # (batch_size, max_len-kernel_size + 1, num_filter)\n\n            feature_maps.append(conv)\n\n        max_pools=[]\n        #Max over time pooling layer\n\n        for i in range(len(feature_maps)):\n            max_pool = GlobalMaxPooling1D(name='Maxovertimepoolinglayer_{}'.format(i))(feature_maps[i])\n            max_pools.append(max_pool)\n\n        #High way\n        feature_vectors = Concatenate(axis=1)(max_pools)\n\n        transform_gate = Dense(sum(self.filters), activation='sigmoid',name='transform_gate', use_bias=True)(feature_vectors)\n        carry_gate = Lambda(lambda x: 1-x, name='carry_gate')(transform_gate)\n\n        z = Dense(sum(self.filters), activation='relu')(feature_vectors)\n        z = add([multiply([z, transform_gate]), multiply([carry_gate, feature_vectors])])\n\n        model = Model(inputs=inputs, outputs=z)\n\n        return model\n\nchar_embed = Char_embed(filters=[16, 16, 32, 32, 64, 64],\n                    kernel=[1, 2, 3, 4, 5, 6],\n                    max_features=len(list(char_dict.keys()))+1,\n                    max_word_len=max_word_len\n                   ).build()\nchar_embed.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28aa778d0671072ff7aae68ba7f10867e3cc377c"
      },
      "cell_type": "code",
      "source": "class main(object):\n\n    def __init__(self, filters, kernels, max_features, \n                 max_len, max_word_len, \n                 pretrain_embed_matrix=None, embed_size=300):\n        \n        self.filters = filters\n        self.kernels = kernels\n        self.num_unit = 64\n        self.output_dim = 16\n        self.max_features = max_features\n        self.max_len = max_len\n        self.max_word_len = max_word_len\n        self.pretrain_embed_matrix = pretrain_embed_matrix\n        self.embed_size = embed_size\n\n    def build(self, charRNN):\n\n        Inp1 = Input(shape=(self.max_len,))\n        Inp2 = Input(shape=(self.max_len,self.max_word_len))\n        \n        #x1= Embedding(self.max_features, self.embed_size, weights=[self.pretrain_embed_matrix])(Inp1)\n        x1 = Embedding(self.max_features, self.embed_size, weights=[self.pretrain_embed_matrix])(Inp1)\n        x2 = TimeDistributed(charRNN, input_shape=(self.max_len, self.max_word_len))(Inp2)\n        x = Concatenate()([x1, x2])\n        \n        #conv_0 = Conv1D(filters=self.filters, kernel_size=(self.kernels[0]),activation='tanh')(x) # (batch_size, max_len-kernel_size + 1 , num_filter)\n        #conv_1 = Conv1D(filters=self.filters, kernel_size=(self.kernels[1]),activation='tanh')(x) #  (batch_size, max_len-kernel_size + 1 , num_filter)\n        #conv_2 = Conv1D(filters=self.filters, kernel_size=(self.kernels[2]),activation='tanh')(x)\n        #conv_3 = Conv1D(filters=self.filters, kernel_size=(self.kernels[3]),activation='tanh')(x)\n\n        #maxpool_0 = MaxPool1D(pool_size=(self.max_len - self.kernels[0] + 1))(conv_0)\n        #maxpool_1 = MaxPool1D(pool_size=(self.max_len - self.kernels[1] + 1))(conv_1)\n        #maxpool_2 = MaxPool1D(pool_size=(self.max_len - self.kernels[2] + 1))(conv_2)\n        #maxpool_3 = MaxPool1D(pool_size=(self.max_len - self.kernels[3] + 1))(conv_3)\n\n        # Fully connected layer with dropout and softmax output\n        #x= Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])\n        #x = Flatten()(x)\n        #dropout on the penultimate layer with a constraint on l2-norms of the weight vectors(Hintonetal., 2012).\n        #x = Dropout(0.5)(x)\n        #x = Dense(1, activation='sigmoid', use_bias=True)(x)\n        \n        x = Bidirectional(LSTM(self.num_unit, return_sequences=True))(x)\n        x = GlobalMaxPool1D()(x)\n        x = Dense(self.output_dim)(x)\n        x = Dropout(0.1)(x)\n        x = Dense(1, activation='sigmoid', use_bias=True)(x)\n        \n        model = Model([Inp1, Inp2], x) \n\n        return model\n    \nmodel = main(filters=32, kernels=[1,5,7,10], \n             max_features=max_features, max_len=max_len, \n             max_word_len=max_word_len, pretrain_embed_matrix=embed_matrix).build(char_embed)\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0dcd9342880eafc6cfce65f4505a8eafb441daa9"
      },
      "cell_type": "code",
      "source": "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\ncallbacks=[EarlyStopping(monitor='val_loss', patience=1, min_delta=0.1, verbose=1),\n            ReduceLROnPlateau(monitor='val_loss', patience=1, min_delta=0.1, factor=0.25, min_lr=0.0001, verbose=1),\n            ModelCheckpoint('TagLM_CNN.h5', save_best_only=True, save_weights_only=True)]\nmodel.fit_generator(input_creater(tr_X, tr_y, batch_size=800), verbose=1, epochs=3,\n                    validation_data=input_creater(v_X, v_y, batch_size=800), callbacks=callbacks)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc0cccc6a6e17bbc5bf27bf8cfaa0251362d37cf"
      },
      "cell_type": "code",
      "source": "pred_v_y = model.predict_generator(input_creater(v_X, batch_size=700), verbose=1)\n\nbest_thresh = 0.1\nbest_score = 0.0\n\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    score = metrics.f1_score(v_y, (pred_v_y > thresh).astype(int))\n    if score > best_score:\n        best_score = score\n        best_thresh = thresh\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, score))\nprint(best_thresh)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5450f4f5e501913ac6500894be1fd592a02290a7"
      },
      "cell_type": "code",
      "source": "output = model.predict_generator(input_creater(te_X, batch_size=700), verbose=1)\noutput = (output > best_thresh).astype(int)\noutput = output.flatten()\n\nsubmission = pd.DataFrame({'qid':test_data.qid.values,\n                          'prediction':output})\n\nprint(submission.head())\nsubmission.to_csv('submission.csv', index=False)\nprint(os.listdir('./'))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}