{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['embeddings', 'sample_submission.csv', 'test.csv', 'train.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\n\npath = '../input'\nalphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\nmax_len = 1024\n\n#train Data \ncsv_path_train = os.path.join(path, 'train.csv')\ncsv_data_train = pd.read_csv(csv_path_train)\ntrain_data, val_data = train_test_split(csv_data_train, test_size=0.1, random_state=2018)\n\ntrain_X = train_data[\"question_text\"].fillna(\"_na_\").values\nval_X = val_data[\"question_text\"].fillna(\"_na_\").values\ntrain_y = train_data['target'].values\nval_y = val_data['target'].values\n\n#char_level: Trueなら，全文字はトークンとして扱われる\ntk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\ntk.fit_on_texts(list(train_X))\n\nchar_list = {}\nfor i, char in enumerate(alphabet):\n    char_list[char] = i + 1\n\ntk.word_index = char_list.copy()\ntk.word_index[tk.oov_token] = max(char_list.values()) + 1\n\ntrain_X = tk.texts_to_sequences(train_X)\nval_X = tk.texts_to_sequences(val_X)\ntrain_X = pad_sequences(train_X, maxlen=max_len)\nval_X = pad_sequences(val_X, maxlen=max_len)\n\ncsv_path_test = os.path.join(path, 'test.csv')\ncsv_data_test = pd.read_csv(csv_path_test)\ntest_X = csv_data_test[\"question_text\"].fillna(\"_na_\").values\ntest_X = tk.texts_to_sequences(test_X)\ntest_X = pad_sequences(test_X, maxlen=max_len)\n\ntest_qid = csv_data_test['qid']\n\ndel csv_data_train\ndel csv_data_test\n\nprint(train_X.shape, val_X.shape, test_X.shape, train_y.shape, val_y.shape, test_qid.shape)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "(1175509, 1024) (130613, 1024) (56370, 1024) (1175509,) (130613,) (56370,)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d094297eda1ab89990f704f254a4584ec337c31"
      },
      "cell_type": "code",
      "source": "#and then quantize each character using 1-of-m encoding\n#(or “one-hot” encoding)\n\nembedding_weights = np.identity(len(tk.word_index))\npad_weights = np.zeros(len(tk.word_index))\n\nembedding_weights = np.vstack((pad_weights, embedding_weights))\nprint(embedding_weights.shape)\n\nvocab_size = len(tk.word_index)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(70, 69)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6fe1765680f3b5a4db4a74725c0ea1d846080a6d",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom keras.layers import Input, Conv1D, Embedding, MaxPool1D, Dense, Dropout, Flatten, Concatenate\nfrom keras.optimizers import adam\nfrom keras.models import Model\nfrom keras.initializers import RandomNormal\n\nclass char_cnn(object):\n\n    def __init__(self, num_filter, num_output, max_features=70, max_len=1024, embed_size=69, pretrain_embeding_matrix=None):\n\n        self.max_features = max_features\n        self.max_len = max_len\n        self.embed_size = embed_size\n        self.num_filter = num_filter\n        self.num_output = num_output\n        self.pretrain_embed_matrix = pretrain_embeding_matrix\n        self.adam = adam(lr=0.01, decay=1e-6)\n        self.initializers = RandomNormal(mean=0.0, stddev=0.05)\n        \n    def get(self):\n        \n        inputs = Input(shape=(self.max_len,))\n        x = Embedding(self.max_features, self.embed_size,weights=[self.pretrain_embed_matrix])(inputs) # shape: batch_size, max_len, emb_size\n\n        net = Conv1D(filters=self.num_filter, kernel_size=(7),\n                     kernel_initializer=self.initializers, activation='tanh')(x) # (batch_size, max_len-kernel_size + 1 , num_filter)\n        net = MaxPool1D(pool_size=(3))(net)\n        net = Conv1D(filters=self.num_filter, kernel_size=(7),\n                     kernel_initializer=self.initializers, activation='tanh')(net) # (batch_size, max_len-kernel_size + 1 , num_filter)\n        net = MaxPool1D(pool_size=(3))(net)\n        net = Conv1D(filters=self.num_filter, kernel_size=(3),\n                     kernel_initializer=self.initializers, activation='tanh')(net) # (batch_size, max_len-kernel_size + 1 , num_filter)\n        net = Conv1D(filters=self.num_filter, kernel_size=(3),\n                     kernel_initializer=self.initializers, activation='tanh')(net) # (batch_size, max_len-kernel_size + 1 , num_filter)\n        net = Conv1D(filters=self.num_filter, kernel_size=(3),\n                     kernel_initializer=self.initializers, activation='tanh')(net) # (batch_size, max_len-kernel_size + 1 , num_filter)\n        net = Conv1D(filters=self.num_filter, kernel_size=(3),\n                     kernel_initializer=self.initializers, activation='tanh')(net) # (batch_size, max_len-kernel_size + 1 , num_filter)\n        net = MaxPool1D(pool_size=(3))(net)\n        net = Flatten()(net)\n        \n        #dropout on the penultimate layer with a constraint on l2-norms of the weight vectors(Hintonetal., 2012).\n        net = Dense(self.num_output, activation='tanh')(net)\n        net = Dropout(0.1)(net)\n        net = Dense(self.num_output, activation='tanh')(net)\n        net = Dropout(0.1)(net)\n        outputs = Dense(1, activation='sigmoid')(net)\n        model = Model(inputs=inputs, outputs=outputs)\n        model.compile(optimizer=self.adam, loss='binary_crossentropy', metrics=['accuracy'])\n\n        return model\n        \nmodel = char_cnn(num_filter=42, num_output=96, pretrain_embeding_matrix=embedding_weights).get()\nmodel.summary()",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         (None, 1024)              0         \n_________________________________________________________________\nembedding_4 (Embedding)      (None, 1024, 69)          4830      \n_________________________________________________________________\nconv1d_19 (Conv1D)           (None, 1018, 42)          20328     \n_________________________________________________________________\nmax_pooling1d_10 (MaxPooling (None, 339, 42)           0         \n_________________________________________________________________\nconv1d_20 (Conv1D)           (None, 333, 42)           12390     \n_________________________________________________________________\nmax_pooling1d_11 (MaxPooling (None, 111, 42)           0         \n_________________________________________________________________\nconv1d_21 (Conv1D)           (None, 109, 42)           5334      \n_________________________________________________________________\nconv1d_22 (Conv1D)           (None, 107, 42)           5334      \n_________________________________________________________________\nconv1d_23 (Conv1D)           (None, 105, 42)           5334      \n_________________________________________________________________\nconv1d_24 (Conv1D)           (None, 103, 42)           5334      \n_________________________________________________________________\nmax_pooling1d_12 (MaxPooling (None, 34, 42)            0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 1428)              0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 96)                137184    \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 96)                0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 96)                9312      \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 96)                0         \n_________________________________________________________________\ndense_12 (Dense)             (None, 1)                 97        \n=================================================================\nTotal params: 205,477\nTrainable params: 205,477\nNon-trainable params: 0\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5e0ebb99f7cdcc86d6462f841bad9b84444da08"
      },
      "cell_type": "code",
      "source": "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1175509 samples, validate on 130613 samples\nEpoch 1/2\n1175509/1175509 [==============================] - 351s 299us/step - loss: 0.2362 - acc: 0.9359 - val_loss: 0.2358 - val_acc: 0.9378\nEpoch 2/2\n1175509/1175509 [==============================] - 346s 295us/step - loss: 0.2337 - acc: 0.9382 - val_loss: 0.2366 - val_acc: 0.9378\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f1da0316d68>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e8f2d3c4f2e904c6ce681f60a2c805c8a377650a"
      },
      "cell_type": "code",
      "source": "from sklearn import metrics\n\npred_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))))",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "130613/130613 [==============================] - 12s 93us/step\nF1 score at threshold 0.1 is 0.0\nF1 score at threshold 0.11 is 0.0\nF1 score at threshold 0.12 is 0.0\nF1 score at threshold 0.13 is 0.0\nF1 score at threshold 0.14 is 0.0\nF1 score at threshold 0.15 is 0.0\nF1 score at threshold 0.16 is 0.0\nF1 score at threshold 0.17 is 0.0\nF1 score at threshold 0.18 is 0.0\nF1 score at threshold 0.19 is 0.0\nF1 score at threshold 0.2 is 0.0\nF1 score at threshold 0.21 is 0.0\nF1 score at threshold 0.22 is 0.0\nF1 score at threshold 0.23 is 0.0\nF1 score at threshold 0.24 is 0.0\nF1 score at threshold 0.25 is 0.0\nF1 score at threshold 0.26 is 0.0\nF1 score at threshold 0.27 is 0.0\nF1 score at threshold 0.28 is 0.0\nF1 score at threshold 0.29 is 0.0\nF1 score at threshold 0.3 is 0.0\nF1 score at threshold 0.31 is 0.0\nF1 score at threshold 0.32 is 0.0\nF1 score at threshold 0.33 is 0.0\nF1 score at threshold 0.34 is 0.0\nF1 score at threshold 0.35 is 0.0\nF1 score at threshold 0.36 is 0.0\nF1 score at threshold 0.37 is 0.0\nF1 score at threshold 0.38 is 0.0\nF1 score at threshold 0.39 is 0.0\nF1 score at threshold 0.4 is 0.0\nF1 score at threshold 0.41 is 0.0\nF1 score at threshold 0.42 is 0.0\nF1 score at threshold 0.43 is 0.0\nF1 score at threshold 0.44 is 0.0\nF1 score at threshold 0.45 is 0.0\nF1 score at threshold 0.46 is 0.0\nF1 score at threshold 0.47 is 0.0\nF1 score at threshold 0.48 is 0.0\nF1 score at threshold 0.49 is 0.0\nF1 score at threshold 0.5 is 0.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2a9730039180a466e81609637fe2fd2b0ddbc79"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}