{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['embeddings', 'sample_submission.csv', 'test.csv', 'train.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\npath = '../input'\nmax_features = 50000\nmax_len = 70\n\n#train Data \ncsv_path_train = os.path.join(path, 'train.csv')\ncsv_data_train = pd.read_csv(csv_path_train)\ntrain_data, val_data = train_test_split(csv_data_train, test_size=0.1, random_state=2018)\n\ntrain_X = train_data[\"question_text\"].fillna(\"_na_\").values\nval_X = val_data[\"question_text\"].fillna(\"_na_\").values\ntrain_y = train_data['target'].values\nval_y = val_data['target'].values\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\nword_index = tokenizer.index_word\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntrain_X = pad_sequences(train_X, maxlen=max_len)\nval_X = pad_sequences(val_X, maxlen=max_len)        \n\n#test_Data\ncsv_path_test = os.path.join(path, 'test.csv')\ncsv_data_test = pd.read_csv(csv_path_test)\n\ntest_X = csv_data_test[\"question_text\"].fillna(\"_na_\").values\ntest_X = tokenizer.texts_to_sequences(test_X)\ntest_X = pad_sequences(test_X, maxlen=max_len)\n\ntest_qid = csv_data_test['qid']\n\ndel csv_data_train\ndel csv_data_test\n\nprint(train_X.shape, val_X.shape, test_X.shape, train_y.shape, val_y.shape, test_qid.shape, len(word_index))",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "(1175509, 70) (130613, 70) (56370, 70) (1175509,) (130613,) (56370,) 209286\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "935c3cb2389c9dc410c3a2f582533e3e9ca7c1f7"
      },
      "cell_type": "code",
      "source": "from tqdm import tqdm\n\nprint(os.listdir(\"../input/embeddings/glove.840B.300d\"))\nembed_size = 300\ndef create_embed_matrix():\n    embedding_file_path = os.path.join(path, 'embeddings', 'glove.840B.300d', 'glove.840B.300d.txt')\n    \n    embed_dict = {}\n    with open(embedding_file_path) as file:\n        for o in tqdm(file):\n            split_o = o.split(\" \")\n            embed_dict[split_o[0]] = np.array(split_o[1:], dtype=np.float32)\n        \n    all_embeds = np.stack(embed_dict.values())\n    embed_mean,embed_std = all_embeds.mean(), all_embeds.std()\n    \n    embed_matrix = np.random.uniform(embed_mean, embed_std, size=(max_features, embed_size))\n    \n    for key, word in tqdm(word_index.items()):\n        if key >= max_features: continue\n        embed_vector = embed_dict.get(word)\n        if embed_vector is not None: \n            embed_matrix[key] = embed_vector\n    \n    return embed_matrix\n\nembed_matrix = create_embed_matrix()\nprint(embed_matrix.shape)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "2474it [00:00, 12435.29it/s]",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "['glove.840B.300d.txt']\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "2196017it [03:08, 11651.55it/s]\n100%|██████████| 209286/209286 [00:00<00:00, 911846.51it/s]\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "(50000, 300)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b887636554f7abb8041ebe97ed750bb6795192ab"
      },
      "cell_type": "code",
      "source": "from keras import backend as K\nfrom tensorflow.keras.backend import batch_dot\nfrom keras.layers import Dense\nfrom keras.activations import softmax\nfrom keras.engine.topology import Layer\n\nclass Position_Embeddding(Layer):\n\n    def __init__(self, size=None, **kwargs):\n\n       \"\"\"\n       \"positional encodings\" to the input embeddings at the\n       bottoms of the encoder and decoder stacks.\n\n       \"\"\"\n       self.size = size\n       super(Position_Embeddding, self).__init__(**kwargs)\n\n    def call(self, x):\n\n        if self.size == None:\n            self.size = int(x.shape[-1]) #embed_size\n        batch_size, seq_len = K.shape(x)[0], K.shape(x)[1]\n        position_j = 1. / K.pow(10000., (2*K.arange(self.size / 2, dtype='float32') / self.size))\n        position_j = K.expand_dims(position_j, 0)\n        position_i = K.cumsum(K.ones_like(x[:,:,0]), 1) -1\n        position_i = K.expand_dims(position_i, 2)\n        position_ij = K.dot(position_i, position_j)\n        position_ij = K.concatenate([K.sin(position_ij), K.cos(position_ij)], axis=2)\n        position_embeding = position_ij + x\n\n        return position_embeding #batch_size, max_len, embed_size\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n\nclass Attention(Layer):\n\n    def __init__(self, nb_head, size_per_head, **kwargs):\n        self.nb_head = nb_head\n        self.size_per_head = size_per_head\n        self.output_dim = nb_head*size_per_head\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.WQ = self.add_weight(name='WQ',\n                                  shape=(input_shape[0][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        self.WK = self.add_weight(name='WK',\n                                  shape=(input_shape[1][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        self.WV = self.add_weight(name='WV',\n                                  shape=(input_shape[2][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        super(Attention, self).build(input_shape)\n\n    def Mask(self, inputs, seq_len, mode='mul'):\n        if seq_len == None:\n            return inputs\n        else:\n            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n            mask = 1 - K.cumsum(mask, 1)\n            for _ in range(len(inputs.shape)-2):\n                mask = K.expand_dims(mask, 2)\n            if mode == 'mul':\n                return inputs * mask\n            if mode == 'add':\n                return inputs - (1 - mask) * 1e12\n\n    def call(self, x):\n        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n        if len(x) == 3:\n            Q_seq,K_seq,V_seq = x\n            Q_len,V_len = None,None\n        elif len(x) == 5:\n            Q_seq,K_seq,V_seq,Q_len,V_len = x\n        #对Q、K、V做线性变换\n        Q_seq = K.dot(Q_seq, self.WQ)\n        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n        K_seq = K.dot(K_seq, self.WK)\n        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n        V_seq = K.dot(V_seq, self.WV)\n        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n\n        A = batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n        A = K.permute_dimensions(A, (0,3,2,1))\n        A = self.Mask(A, V_len, 'add')\n        A = K.permute_dimensions(A, (0,3,2,1))\n        A = K.softmax(A)\n\n        O_seq = batch_dot(A, V_seq, axes=[3,2])\n        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n        O_seq = self.Mask(O_seq, Q_len, 'mul')\n        return O_seq\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], input_shape[0][1], self.output_dim)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "803a23a448df3e1a14d2afbaa972e995c9b4a2d2"
      },
      "cell_type": "code",
      "source": "import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom keras.layers import Input, Embedding, Dense, Flatten, GlobalAvgPool1D, Dropout\nfrom keras.optimizers import adam\nfrom keras.models import Model\n\nclass create_model(object):\n\n    def __init__(self, max_features=50000, max_len=100, embed_size=300, pretrain_embeding_matrix=None):\n\n        self.max_features = max_features\n        self.max_len = max_len\n        self.embed_size = embed_size\n        self.pretrain_embed_matrix = pretrain_embeding_matrix\n        self.adam = adam(lr=0.01)\n\n    def get(self):\n\n        inputs = Input(shape=(self.max_len,))\n\n        x = Embedding(self.max_features, self.embed_size, weights=[self.pretrain_embed_matrix])(inputs) # shape: batch_size, max_len, emb_size\n        x = Position_Embeddding()(x)\n        \n        O_seq = Attention(8, 4)([x, x, x])\n        O_seq = GlobalAvgPool1D()(O_seq)\n        O_seq = Dropout(0.1)(O_seq)\n        Outputs = Dense(1, activation='sigmoid')(O_seq)\n\n        model = Model(inputs=inputs, outputs=Outputs)\n        model.compile(optimizer=self.adam, loss='binary_crossentropy', metrics=['accuracy'])\n\n        return model\n\nmodel = create_model(max_len=70, pretrain_embeding_matrix=embed_matrix).get()\nmodel.summary()",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            (None, 70)           0                                            \n__________________________________________________________________________________________________\nembedding_3 (Embedding)         (None, 70, 300)      15000000    input_3[0][0]                    \n__________________________________________________________________________________________________\nposition__embeddding_2 (Positio (None, 70, 300)      0           embedding_3[0][0]                \n__________________________________________________________________________________________________\nattention_2 (Attention)         (None, 70, 32)       28800       position__embeddding_2[0][0]     \n                                                                 position__embeddding_2[0][0]     \n                                                                 position__embeddding_2[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling1d_2 (Glo (None, 32)           0           attention_2[0][0]                \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 32)           0           global_average_pooling1d_2[0][0] \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            33          dropout_2[0][0]                  \n==================================================================================================\nTotal params: 15,028,833\nTrainable params: 15,028,833\nNon-trainable params: 0\n__________________________________________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "609f50eadbb2887c8a45db518a333ccdabaa2b23"
      },
      "cell_type": "code",
      "source": "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1175509 samples, validate on 130613 samples\nEpoch 1/2\n1175509/1175509 [==============================] - 136s 116us/step - loss: 0.1297 - acc: 0.9504 - val_loss: 0.1187 - val_acc: 0.9541\nEpoch 2/2\n1175509/1175509 [==============================] - 134s 114us/step - loss: 0.1138 - acc: 0.9546 - val_loss: 0.1159 - val_acc: 0.9537\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fa993317e80>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8257b2b24445cb5b9b689148a5b32f6f0d4ba699"
      },
      "cell_type": "code",
      "source": "from sklearn import metrics\n\npred_glove_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nprint(pred_glove_val_y[pred_glove_val_y==1])\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_glove_val_y > thresh).astype(int))))",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "130613/130613 [==============================] - 4s 33us/step\n[]\nF1 score at threshold 0.1 is 0.5735788086228156\nF1 score at threshold 0.11 is 0.5827426271112278\nF1 score at threshold 0.12 is 0.5903776978417266\nF1 score at threshold 0.13 is 0.5975126669737448\nF1 score at threshold 0.14 is 0.6039837628622675\nF1 score at threshold 0.15 is 0.6076656394453004\nF1 score at threshold 0.16 is 0.6125258086717137\nF1 score at threshold 0.17 is 0.6171342685370742\nF1 score at threshold 0.18 is 0.6199969408045685\nF1 score at threshold 0.19 is 0.6218513527521509\nF1 score at threshold 0.2 is 0.6241373860822842\nF1 score at threshold 0.21 is 0.6256552904675297\nF1 score at threshold 0.22 is 0.6263515349089922\nF1 score at threshold 0.23 is 0.6268211920529801\nF1 score at threshold 0.24 is 0.6267420383948061\nF1 score at threshold 0.25 is 0.6263910969793324\nF1 score at threshold 0.26 is 0.6256392575992645\nF1 score at threshold 0.27 is 0.6255024174287879\nF1 score at threshold 0.28 is 0.623848878394333\nF1 score at threshold 0.29 is 0.6207845013154747\nF1 score at threshold 0.3 is 0.6195665334786294\nF1 score at threshold 0.31 is 0.617976839654433\nF1 score at threshold 0.32 is 0.6157757658439786\nF1 score at threshold 0.33 is 0.6138190954773869\nF1 score at threshold 0.34 is 0.6089499110094075\nF1 score at threshold 0.35 is 0.60570987654321\nF1 score at threshold 0.36 is 0.6026684022128214\nF1 score at threshold 0.37 is 0.5996572633799104\nF1 score at threshold 0.38 is 0.5963486140724947\nF1 score at threshold 0.39 is 0.5922028868204505\nF1 score at threshold 0.4 is 0.5882352941176471\nF1 score at threshold 0.41 is 0.5843099407141872\nF1 score at threshold 0.42 is 0.5809105486997143\nF1 score at threshold 0.43 is 0.5774489076814658\nF1 score at threshold 0.44 is 0.5728750713063321\nF1 score at threshold 0.45 is 0.5679030163082697\nF1 score at threshold 0.46 is 0.5633206886489641\nF1 score at threshold 0.47 is 0.5576965669988926\nF1 score at threshold 0.48 is 0.5526256816314334\nF1 score at threshold 0.49 is 0.5457015623820665\nF1 score at threshold 0.5 is 0.53820293398533\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d567da8f759e95b55b4e833b72b7c28699e2c202"
      },
      "cell_type": "code",
      "source": "output = model.predict([test_X], batch_size=1024, verbose=1)\noutput = (output > 0.35).astype(int)\noutput = output.flatten()\n\nsubmission = pd.DataFrame({'qid':test_qid,\n                          'prediction':output})\n\nsubmission.head()\nsubmission.to_csv('submission.csv', index=False)\nprint(os.listdir('./'))",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "56370/56370 [==============================] - 2s 34us/step\n['submission.csv', '__notebook_source__.ipynb', '.ipynb_checkpoints']\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}