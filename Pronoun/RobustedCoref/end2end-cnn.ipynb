{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c8d4646558010002f14aee3fc707e01dae3726d"
   },
   "source": [
    "This kernel implements use CNN models for coreference resolution. \n",
    "\n",
    "Features extraction and model used in this kernel follows Henry Y. Chen,  Ethan Zhou,  Jinho D. Choi work: http://aclweb.org/anthology/K17-1023\n",
    "and thanks Keyi Tang's kernel, I used some code from Keyi Tang's kernel, .  https://www.kaggle.com/keyit92/coref-by-mlp-cnn-coattention \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gendered-pronoun-resolution', 'gap-coreference']\n",
      "['gap-development.tsv', 'gap-test.tsv', 'gap-validation.tsv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from tqdm import tqdm\n",
    "from keras.layers import *\n",
    "from keras.models import Model, Sequential\n",
    "import keras.backend as K\n",
    "from keras import callbacks\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "print(os.listdir('../input'))\n",
    "print(os.listdir('../input/gap-coreference'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test-1</td>\n",
       "      <td>Upon their acceptance into the Kontinental Hoc...</td>\n",
       "      <td>His</td>\n",
       "      <td>383</td>\n",
       "      <td>Bob Suter</td>\n",
       "      <td>352</td>\n",
       "      <td>False</td>\n",
       "      <td>Dehner</td>\n",
       "      <td>366</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jeremy_Dehner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test-2</td>\n",
       "      <td>Between the years 1979-1981, River won four lo...</td>\n",
       "      <td>him</td>\n",
       "      <td>430</td>\n",
       "      <td>Alonso</td>\n",
       "      <td>353</td>\n",
       "      <td>True</td>\n",
       "      <td>Alfredo Di St*fano</td>\n",
       "      <td>390</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Norberto_Alonso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test-3</td>\n",
       "      <td>Though his emigration from the country has aff...</td>\n",
       "      <td>He</td>\n",
       "      <td>312</td>\n",
       "      <td>Ali Aladhadh</td>\n",
       "      <td>256</td>\n",
       "      <td>True</td>\n",
       "      <td>Saddam</td>\n",
       "      <td>295</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Aladhadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test-4</td>\n",
       "      <td>At the trial, Pisciotta said: ``Those who have...</td>\n",
       "      <td>his</td>\n",
       "      <td>526</td>\n",
       "      <td>Alliata</td>\n",
       "      <td>377</td>\n",
       "      <td>False</td>\n",
       "      <td>Pisciotta</td>\n",
       "      <td>536</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Gaspare_Pisciotta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test-5</td>\n",
       "      <td>It is about a pair of United States Navy shore...</td>\n",
       "      <td>his</td>\n",
       "      <td>406</td>\n",
       "      <td>Eddie</td>\n",
       "      <td>421</td>\n",
       "      <td>True</td>\n",
       "      <td>Rock Reilly</td>\n",
       "      <td>559</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Chasers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                       ...                                                                   URL\n",
       "0  test-1                       ...                            http://en.wikipedia.org/wiki/Jeremy_Dehner\n",
       "1  test-2                       ...                          http://en.wikipedia.org/wiki/Norberto_Alonso\n",
       "2  test-3                       ...                                 http://en.wikipedia.org/wiki/Aladhadh\n",
       "3  test-4                       ...                        http://en.wikipedia.org/wiki/Gaspare_Pisciotta\n",
       "4  test-5                       ...                                  http://en.wikipedia.org/wiki/Chasers\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df  = pd.read_table('../input/gap-coreference/gap-development.tsv')\n",
    "train_df = pd.read_table('../input/gap-coreference/gap-test.tsv')\n",
    "val_df   = pd.read_table('../input/gap-coreference/gap-validation.tsv')\n",
    "nlp      = spacy.load('en_core_web_lg')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "be26809f835a9c4a51d2e8316e4d1bf9ad4a6948"
   },
   "outputs": [],
   "source": [
    "def bs(lens, target):\n",
    "    low, high = 0, len(lens) - 1\n",
    "\n",
    "    while low < high:\n",
    "        mid = low + int((high - low) / 2)\n",
    "\n",
    "        if target > lens[mid]:\n",
    "            low = mid + 1\n",
    "        elif target < lens[mid]:\n",
    "            high = mid\n",
    "        else:\n",
    "            return mid + 1\n",
    "\n",
    "    return low\n",
    "\n",
    "class Mention_Features():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def create(self, charoffset, text):\n",
    "\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        lens = [token.idx for token in doc]  # The Charactor offset the token within the parent\n",
    "        mention_offset = bs(lens, charoffset) - 1  # The target in which index of tokens\n",
    "        mention = doc[mention_offset]  # mention\n",
    "\n",
    "        dependency_parent = mention.head  # The syntactic parent, or \"governor\", of this token.\n",
    "        nbor = mention.nbor()  # The following word of nbor\n",
    "\n",
    "        sent_lens = [len(sent) for sent in doc.sents]  # the sentence length\n",
    "        acc_lens = sent_lens\n",
    "        pre_lens = 0\n",
    "        for i in range(0, len(sent_lens)):\n",
    "            pre_lens += acc_lens[i]\n",
    "            acc_lens[i] = pre_lens\n",
    "        sent_index = bs(acc_lens, mention_offset)  # to Find out the charoffset which sentence\n",
    "        current_sent = list(doc.sents)[sent_index]\n",
    "        current_sent = [token for token in current_sent]\n",
    "\n",
    "        preceding3 = self.n_preceding_words(3, doc, mention_offset)\n",
    "        following3 = self.n_following_words(3, doc, mention_offset)\n",
    "\n",
    "        proceed_sents = [] # 3 proceeding sentence\n",
    "        for i in range(sent_index - 3, sent_index):\n",
    "            if i < 0: continue\n",
    "            proceeding = [token for token in list(doc.sents)[i]]\n",
    "            proceed_sents.extend(proceeding)\n",
    "\n",
    "        if sent_index + 1 < len(list(doc.sents)): #1 succeeding sentence\n",
    "            succeeding = list(doc.sents)[sent_index + 1]\n",
    "            succeed_sent = [token for token in succeeding]\n",
    "        else:\n",
    "            succeed_sent = []\n",
    "\n",
    "        return mention, dependency_parent, nbor, preceding3, following3, proceed_sents, current_sent, succeed_sent\n",
    "\n",
    "    def n_preceding_words(self, n, tokens, offset):\n",
    "\n",
    "        start = offset - n\n",
    "        precedings = [None] * max(0, 0 - start)\n",
    "        start = max(0, start)\n",
    "        precedings += tokens[start: offset]\n",
    "\n",
    "        return precedings\n",
    "\n",
    "    def n_following_words(self, n, tokens, offset):\n",
    "\n",
    "        end = offset + n\n",
    "        followings = [None] * max(0, end - len(tokens))\n",
    "        end = min(end, len(tokens))\n",
    "        followings += tokens[offset: end]\n",
    "        \n",
    "        return followings\n",
    "\n",
    "class Distance_Features():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def create(self, char_offsetA, char_offsetB, text):\n",
    "\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        lens = [token.idx for token in doc]\n",
    "        mention_offsetA = bs(lens, char_offsetA) - 1\n",
    "        mention_offsetB = bs(lens, char_offsetB) - 1\n",
    "\n",
    "        mention_dist = mention_offsetA - mention_offsetB\n",
    "        mentionA = doc[mention_offsetA].head\n",
    "        mentionB = doc[mention_offsetB].head\n",
    "             \n",
    "        sent_lens = [len(sent) for sent in doc.sents] #the sentence length\n",
    "        acc_lens = sent_lens\n",
    "        pre_lens = 0\n",
    "        for i in range(0, len(sent_lens)):\n",
    "            pre_lens += acc_lens[i]\n",
    "            acc_lens[i] = pre_lens\n",
    "\n",
    "        sentA_index = bs(acc_lens, mention_offsetA)\n",
    "        sentB_index = bs(acc_lens, mention_offsetB)\n",
    "\n",
    "        sent_dist = sentA_index - sentB_index\n",
    "\n",
    "        return [mention_dist, sent_dist]\n",
    "\n",
    "def extract_embedding_features(df, text_column, offset_column,  embed_dim=300):\n",
    "    text_offset_list = df[[text_column, offset_column]].values.tolist()\n",
    "    extractor = Mention_Features()\n",
    "\n",
    "    feature_map1 = np.zeros(shape=(len(text_offset_list), 3, embed_dim))\n",
    "    feature_map2 = np.zeros(shape=(len(text_offset_list), 6, embed_dim))\n",
    "    feature_map3 = np.zeros(shape=(len(text_offset_list), 3, embed_dim))\n",
    "\n",
    "    for text_offset_index in range(len(text_offset_list)):\n",
    "        text_offset = text_offset_list[text_offset_index]\n",
    "        mention, dependency_parent, nbor, preceding3, following3, proceed_sents, current_sent, succeed_sent = extractor.create( text_offset[1], text_offset[0])\n",
    "\n",
    "        # Feature Map1\n",
    "        feature_map1[text_offset_index, 0, :] = dependency_parent.vector\n",
    "        feature_map1[text_offset_index, 1, :] = mention.vector\n",
    "        feature_map1[text_offset_index, 2, :] = nbor.vector\n",
    "\n",
    "        # Feature Map2\n",
    "        feature_map2[text_offset_index, 0:3, :] = np.asarray(\n",
    "            [token.vector if token is not None else np.zeros((embed_dim,)) for token in preceding3])\n",
    "        feature_map2[text_offset_index, 3:6, :] = np.asarray(\n",
    "            [token.vector if token is not None else np.zeros((embed_dim,)) for token in following3])\n",
    "\n",
    "        # Feature Map3\n",
    "        feature_map3[text_offset_index, 0, :] = np.mean(np.asarray([token.vector for token in proceed_sents]),\n",
    "                                                        axis=0) if len(proceed_sents) > 0 else np.zeros(embed_dim)\n",
    "        feature_map3[text_offset_index, 1, :] =  np.mean(np.asarray([token.vector for token in current_sent]),\n",
    "                                                         axis=0) if len(current_sent) > 0 else np.zeros(embed_dim)\n",
    "        feature_map3[text_offset_index, 2, :] =  np.mean(np.asarray([token.vector for token in succeed_sent]),\n",
    "                                                         axis=0) if len(succeed_sent) > 0 else np.zeros(embed_dim)\n",
    "\n",
    "    return feature_map1, feature_map2, feature_map3\n",
    "\n",
    "def extract_dist_features(df, text_column, pronoun_offset_column, name_offset_column):\n",
    "    text_offset_list = df[[text_column, pronoun_offset_column, name_offset_column]].values.tolist()\n",
    "    extractor = Distance_Features()\n",
    "    dist_feas = []\n",
    "\n",
    "    for text_offset_index in range(len(text_offset_list)):\n",
    "        text_offset = text_offset_list[text_offset_index]\n",
    "        dist_fea = extractor.create(text_offset[1], text_offset[2], text_offset[0])\n",
    "        dist_feas.append(dist_fea)\n",
    "\n",
    "    return np.asarray(dist_feas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "d8a1c28bb6ff05d61e53348238de4ba71328bd4d"
   },
   "outputs": [],
   "source": [
    "p_emb_1, p_emb_2, p_emb_3 = extract_embedding_features(train_df, 'Text', 'Pronoun-offset')\n",
    "p_emb_dev_1, p_emb_dev_2, p_emb_dev_3 = extract_embedding_features(val_df, 'Text', 'Pronoun-offset')\n",
    "a_emb_1, a_emb_2, a_emb_3 = extract_embedding_features(train_df, 'Text', 'A-offset')\n",
    "a_emb_dev_1, a_emb_dev_2, a_emb_dev_3 = extract_embedding_features(val_df, 'Text', 'A-offset')\n",
    "b_emb_1, b_emb_2, b_emb_3  = extract_embedding_features(train_df, 'Text', 'B-offset')\n",
    "b_emb_dev_1, b_emb_dev_2, b_emb_dev_3 = extract_embedding_features(val_df, 'Text', 'B-offset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "0ebe10c1be1187201ee45f3988e6e6c963ca7c11"
   },
   "outputs": [],
   "source": [
    "pa_pos_tra = extract_dist_features(train_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pa_pos_dev = extract_dist_features(val_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pb_pos_tra = extract_dist_features(train_df, 'Text', 'Pronoun-offset', 'B-offset')\n",
    "pb_pos_dev = extract_dist_features(val_df, 'Text', 'Pronoun-offset', 'B-offset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f01722809593f1a2ee5e86e0f086a09a83660036"
   },
   "outputs": [],
   "source": [
    "class Mention_Embedding(object):\n",
    "\n",
    "    def __init__(self, filters=120, embed_size=300):\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def build(self):\n",
    "        \n",
    "        P_Fea1 = Input(shape=(3, self.embed_size))  # Embedding of Parents, Mention, and Suceeding Word: String Features\n",
    "        P_Fea2 = Input(shape=(6, self.embed_size))  # Embeddings of 3 proceedings words, 3 succedings words of m\n",
    "        P_Fea3 = Input(shape=(3, self.embed_size))\n",
    "        Antecedent_Fea1 = Input(shape=(3, self.embed_size))\n",
    "        Antecedent_Fea2 = Input(shape=(6, self.embed_size))\n",
    "        Antecedent_Fea3 = Input(shape=(3, self.embed_size))\n",
    "        Dist_Fea = Input(shape=(2,))\n",
    "        \n",
    "        Dist_Embed = Dense(self.filters, use_bias=True)(Dist_Fea)\n",
    "\n",
    "        Mention_Represent1 = self.mention_embed(P_Fea1, P_Fea2, P_Fea3, 'Mention')\n",
    "        Mention_Represent2 = self.mention_embed(Antecedent_Fea1, Antecedent_Fea2, Antecedent_Fea3, 'Antecedent')\n",
    "\n",
    "        x = self.mentionpair_embed(Mention_Represent1, Mention_Represent2)\n",
    "        x = Concatenate(name='Mention_Pair_Embedding')([x, Dist_Embed])\n",
    "\n",
    "        model = Model([P_Fea1, P_Fea2, P_Fea3, Antecedent_Fea1, Antecedent_Fea2, Antecedent_Fea3, Dist_Fea], x)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def mention_embed(self, inp1, inp2, inp3, target):\n",
    "        \n",
    "        Conv1_fea1 = self.Conv1k(inp1, [1, 2, 3]) # n_gram\n",
    "        Conv1_fea2 = self.Conv1k(inp2, [1, 2, 3]) # n_gram\n",
    "        Conv1_fea3 = self.Conv1k(inp3, [1, 2, 3])\n",
    "\n",
    "        self.Expand_dim = Lambda(lambda x: K.expand_dims(x, axis=1))\n",
    "        Conv1_fea1 = self.Expand_dim(Conv1_fea1)\n",
    "        Conv1_fea2 = self.Expand_dim(Conv1_fea2)\n",
    "        Conv1_fea3 = self.Expand_dim(Conv1_fea3)\n",
    "        Conv2_Input = Concatenate(axis=1)([Conv1_fea1, Conv1_fea2, Conv1_fea3])\n",
    "\n",
    "        x = Conv2D(self.filters, kernel_size=(3, 3), activation='tanh')(Conv2_Input)\n",
    "        x = MaxPool2D(pool_size=(1, 1))(x)\n",
    "        x = Lambda(lambda x: K.squeeze(x, axis=1), name=\"{}_Embed\".format(target))(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def Conv1k(self, x, kernels):\n",
    "        \n",
    "        assert len(kernels) != 0\n",
    "        convs = []\n",
    "        shape = x.get_shape().as_list()\n",
    "        for kernel in kernels:\n",
    "            conv = Conv1D(self.filters, kernel, activation='tanh')(x)\n",
    "            pool = MaxPool1D(pool_size=int(shape[1] - kernel + 1))(conv)\n",
    "            pool = Dropout(0.8)(pool)\n",
    "            convs.append(pool)\n",
    "\n",
    "        convs = Concatenate(axis=1)(convs)\n",
    "\n",
    "        return convs\n",
    "\n",
    "    def mentionpair_embed(self, M1, M2):\n",
    "        \n",
    "        x = Concatenate(axis=1)([M1, M2])\n",
    "        x = Conv1D(self.filters, kernel_size=2, activation='tanh')(x)\n",
    "        x = MaxPool1D(pool_size=1)(x)\n",
    "        x = Dropout(0.8)(x)\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "291bc68ea24eccebd74ad86d8686c164b4a129a0"
   },
   "outputs": [],
   "source": [
    "class Coreference_Classifier(object):\n",
    "\n",
    "    def __init__(self, Mention_Pair, Mention_Embedding, filters=120, embed_size=300):\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.embed_size = embed_size\n",
    "        self.Mention_Pair = Mention_Pair\n",
    "        self.Mention_Embed = Mention_Embedding\n",
    "        \n",
    "    def build(self):\n",
    "\n",
    "        M1 = Input(shape=(3, self.embed_size))  # Embedding of Parents, Mention, and Suceeding Word: String Features\n",
    "        M2 = Input(shape=(6, self.embed_size))  # Embeddings of 3 proceedings words, 3 succedings words of m\n",
    "        M3 = Input(shape=(3, self.embed_size))  # Average Embedding of 3 proceeding sentence, 1 succeding sentence, and current sentence\n",
    "        A1 = Input(shape=(3, self.embed_size))\n",
    "        A2 = Input(shape=(6, self.embed_size))\n",
    "        A3 = Input(shape=(3, self.embed_size))\n",
    "        B1 = Input(shape=(3, self.embed_size))\n",
    "        B2 = Input(shape=(6, self.embed_size))\n",
    "        B3 = Input(shape=(3, self.embed_size))\n",
    "        Dist_M_A = Input(shape=(2,)) #Mention and Antecedent A\n",
    "        Dist_M_B = Input(shape=(2,)) #Mention and Antecedent B\n",
    "        \n",
    "        # Define layer \n",
    "        self.Expand_dim = Lambda(lambda x: K.expand_dims(x, axis=1))\n",
    "        \n",
    "        Mention_Pair1 = self.Mention_Pair([M1, M2, M3, A1, A2, A3, Dist_M_A])\n",
    "        Mention_Pair2 = self.Mention_Pair([M1, M2, M3, B1, B2, B3, Dist_M_B])\n",
    "        Mention_embedding= self.Mention_Embed([M1, M2, M3])\n",
    "        Mention_Embedding = Flatten()(Mention_embedding)\n",
    "        \n",
    "        output1 = Concatenate()([Mention_Pair1, Mention_Pair2, Mention_Embedding]) \n",
    "        output1 = BatchNormalization()(output1)\n",
    "        output1 = Dense(self.filters, use_bias=True, activation='relu')(output1)\n",
    "        output1 = Dense(self.filters, use_bias=True, activation='relu')(output1)\n",
    "        output1 = Dense(3, use_bias=True, activation='softmax', name='cluster_output')(output1)\n",
    "        \n",
    "        pair1 = self.cluster_classifier(Mention_Pair1, \"pair1\")\n",
    "        pair2 = self.cluster_classifier(Mention_Pair2, \"pair2\")\n",
    "        output2 = Add(name='singleton_output')([pair1, pair2]) # if output2 great and equal 1, there is a ancedent to represent pronoun\n",
    "                                                               # if ouptut2 less than 1, there \n",
    "        \n",
    "        model = Model([M1, M2, M3, A1, A2, A3, B1, B2, B3, Dist_M_A, Dist_M_B], [output1, output2])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def cluster_classifier(self, x, _name):\n",
    "        \n",
    "        x = Dense(self.filters, use_bias=True, activation='relu')(x)\n",
    "        x = Dense(1, use_bias=True, activation='sigmoid', name='{}_output'.format(_name))(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "0d6eea0df8301d04828645439329d5aedc33b9e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 6, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 3, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 3, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 6, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 3, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 3, 120)       36120       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2, 120)       72120       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1, 120)       108120      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 6, 120)       36120       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 5, 120)       72120       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 4, 120)       108120      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 3, 120)       36120       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 2, 120)       72120       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1, 120)       108120      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 3, 120)       36120       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 2, 120)       72120       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 1, 120)       108120      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 6, 120)       36120       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 5, 120)       72120       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 4, 120)       108120      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 3, 120)       36120       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 2, 120)       72120       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1, 120)       108120      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 120)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 120)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 120)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 120)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 120)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 120)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1, 120)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1, 120)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 1, 120)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 1, 120)       0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 1, 120)       0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 1, 120)       0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1, 120)       0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 1, 120)       0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 1, 120)       0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 1, 120)       0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 1, 120)       0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 1, 120)       0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 120)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1, 120)       0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1, 120)       0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1, 120)       0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1, 120)       0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1, 120)       0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1, 120)       0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1, 120)       0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1, 120)       0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1, 120)       0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1, 120)       0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 1, 120)       0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 1, 120)       0           max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 1, 120)       0           max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1, 120)       0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 1, 120)       0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 1, 120)       0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 1, 120)       0           max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 120)       0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3, 120)       0           dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 3, 120)       0           dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 3, 120)       0           dropout_10[0][0]                 \n",
      "                                                                 dropout_11[0][0]                 \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 3, 120)       0           dropout_13[0][0]                 \n",
      "                                                                 dropout_14[0][0]                 \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 3, 120)       0           dropout_16[0][0]                 \n",
      "                                                                 dropout_17[0][0]                 \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 3, 120)    0           concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 3, 120)    0           concatenate_5[0][0]              \n",
      "                                                                 concatenate_6[0][0]              \n",
      "                                                                 concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 3, 3, 120)    0           lambda_1[0][0]                   \n",
      "                                                                 lambda_1[1][0]                   \n",
      "                                                                 lambda_1[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 3, 3, 120)    0           lambda_2[0][0]                   \n",
      "                                                                 lambda_2[1][0]                   \n",
      "                                                                 lambda_2[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 1, 1, 120)    129720      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 1, 120)    129720      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 120)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 120)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Mention_Embed (Lambda)          (None, 1, 120)       0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Antecedent_Embed (Lambda)       (None, 1, 120)       0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 2, 120)       0           Mention_Embed[0][0]              \n",
      "                                                                 Antecedent_Embed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1, 120)       28920       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1, 120)       0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 1, 120)       0           max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 120)          0           dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 120)          360         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mention_Pair_Embedding (Concate (None, 240)          0           flatten_1[0][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,586,880\n",
      "Trainable params: 1,586,880\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Embedding_model = Mention_Embedding().build()\n",
    "Embedding_model.summary()\n",
    "layer_name = 'Mention_Embed'\n",
    "Mention_Pair = Model(Embedding_model.inputs, Embedding_model.output)\n",
    "Mention_Embedding = Model([Embedding_model.inputs[0], Embedding_model.inputs[1], Embedding_model.inputs[2]],\n",
    "Embedding_model.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c7c336869cd9fd74f4ac9dcbd9bee6377e71a6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 3, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 6, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 3, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 3, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 6, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 3, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 3, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 6, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 3, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 1, 120)       778800      input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 240)          1586880     input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "                                                                 input_11[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "                                                                 input_13[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 120)          0           model_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 600)          0           model_2[1][0]                    \n",
      "                                                                 model_2[2][0]                    \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 600)          2400        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 120)          72120       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 120)          28920       model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 120)          28920       model_2[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 120)          14520       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pair1_output (Dense)            (None, 1)            121         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pair2_output (Dense)            (None, 1)            121         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cluster_output (Dense)          (None, 3)            363         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "singleton_output (Add)          (None, 1)            0           pair1_output[0][0]               \n",
      "                                                                 pair2_output[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,734,365\n",
      "Trainable params: 1,733,165\n",
      "Non-trainable params: 1,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Coreference_Classifier(Mention_Pair, Mention_Embedding).build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "2b2852664ce1b03c3b52a16bcfee189430d134b5"
   },
   "outputs": [],
   "source": [
    "X_train = [p_emb_1, p_emb_2, p_emb_3, a_emb_1, a_emb_2, a_emb_3, b_emb_1, b_emb_2, b_emb_3, pa_pos_tra, pb_pos_tra]\n",
    "X_dev = [p_emb_dev_1, p_emb_dev_2, p_emb_dev_3, a_emb_dev_1, a_emb_dev_2, a_emb_dev_3, b_emb_dev_1, b_emb_dev_2, b_emb_dev_3, pa_pos_dev, pb_pos_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "8786130fda02d83f333122c41554eefa34c82e75"
   },
   "outputs": [],
   "source": [
    "def _row_to_y(row):\n",
    "    if row.loc['A-coref']:\n",
    "        return 0\n",
    "    if row.loc['B-coref']:\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "y_tra = train_df.apply(_row_to_y, axis=1)\n",
    "y_dev = val_df.apply(_row_to_y, axis=1)\n",
    "y_test = test_df.apply(_row_to_y, axis=1)\n",
    "\n",
    "def _row_to_y_AB(row):\n",
    "    if row.loc['B-coref'] or row.loc['A-coref']:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "y_tra_AB = train_df.apply(_row_to_y_AB, axis=1)\n",
    "y_dev_AB = val_df.apply(_row_to_y_AB, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "cfeeec821607d322005a9e3e83176296240836b5"
   },
   "outputs": [],
   "source": [
    "def custom_mse(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "    return  K.mean(K.square(y_pred - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "08b002cb242ae1b5552922c42c5763786faab175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 454 samples\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 1.4147 - cluster_output_loss: 1.0706 - singleton_output_loss: 0.1147 - cluster_output_sparse_categorical_accuracy: 0.4820 - val_loss: 1.4153 - val_cluster_output_loss: 1.0056 - val_singleton_output_loss: 0.1366 - val_cluster_output_sparse_categorical_accuracy: 0.5066\n",
      "\n",
      "Epoch 00001: val_cluster_output_loss improved from inf to 1.00558, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.3208 - cluster_output_loss: 0.9809 - singleton_output_loss: 0.1133 - cluster_output_sparse_categorical_accuracy: 0.4975 - val_loss: 1.3669 - val_cluster_output_loss: 0.9572 - val_singleton_output_loss: 0.1366 - val_cluster_output_sparse_categorical_accuracy: 0.5286\n",
      "\n",
      "Epoch 00002: val_cluster_output_loss improved from 1.00558 to 0.95723, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.2900 - cluster_output_loss: 0.9496 - singleton_output_loss: 0.1135 - cluster_output_sparse_categorical_accuracy: 0.5320 - val_loss: 1.3734 - val_cluster_output_loss: 0.9637 - val_singleton_output_loss: 0.1366 - val_cluster_output_sparse_categorical_accuracy: 0.5749\n",
      "\n",
      "Epoch 00003: val_cluster_output_loss did not improve from 0.95723\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.2453 - cluster_output_loss: 0.9051 - singleton_output_loss: 0.1134 - cluster_output_sparse_categorical_accuracy: 0.5795 - val_loss: 1.2519 - val_cluster_output_loss: 0.8422 - val_singleton_output_loss: 0.1366 - val_cluster_output_sparse_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00004: val_cluster_output_loss improved from 0.95723 to 0.84216, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.1642 - cluster_output_loss: 0.8237 - singleton_output_loss: 0.1135 - cluster_output_sparse_categorical_accuracy: 0.6175 - val_loss: 1.2636 - val_cluster_output_loss: 0.8539 - val_singleton_output_loss: 0.1366 - val_cluster_output_sparse_categorical_accuracy: 0.5925\n",
      "\n",
      "Epoch 00005: val_cluster_output_loss did not improve from 0.84216\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.1235 - cluster_output_loss: 0.7839 - singleton_output_loss: 0.1132 - cluster_output_sparse_categorical_accuracy: 0.6385 - val_loss: 1.1803 - val_cluster_output_loss: 0.7707 - val_singleton_output_loss: 0.1366 - val_cluster_output_sparse_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00006: val_cluster_output_loss improved from 0.84216 to 0.77066, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.0909 - cluster_output_loss: 0.7481 - singleton_output_loss: 0.1143 - cluster_output_sparse_categorical_accuracy: 0.6590 - val_loss: 1.1424 - val_cluster_output_loss: 0.7415 - val_singleton_output_loss: 0.1336 - val_cluster_output_sparse_categorical_accuracy: 0.6828\n",
      "\n",
      "Epoch 00007: val_cluster_output_loss improved from 0.77066 to 0.74155, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.0539 - cluster_output_loss: 0.7163 - singleton_output_loss: 0.1125 - cluster_output_sparse_categorical_accuracy: 0.6845 - val_loss: 1.1568 - val_cluster_output_loss: 0.7480 - val_singleton_output_loss: 0.1363 - val_cluster_output_sparse_categorical_accuracy: 0.6608\n",
      "\n",
      "Epoch 00008: val_cluster_output_loss did not improve from 0.74155\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.0102 - cluster_output_loss: 0.6730 - singleton_output_loss: 0.1124 - cluster_output_sparse_categorical_accuracy: 0.7115 - val_loss: 1.1077 - val_cluster_output_loss: 0.6980 - val_singleton_output_loss: 0.1366 - val_cluster_output_sparse_categorical_accuracy: 0.6850\n",
      "\n",
      "Epoch 00009: val_cluster_output_loss improved from 0.74155 to 0.69797, saving model to best_model.hdf5\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.9628 - cluster_output_loss: 0.6232 - singleton_output_loss: 0.1132 - cluster_output_sparse_categorical_accuracy: 0.7305 - val_loss: 1.0957 - val_cluster_output_loss: 0.6999 - val_singleton_output_loss: 0.1319 - val_cluster_output_sparse_categorical_accuracy: 0.6850\n",
      "\n",
      "Epoch 00010: val_cluster_output_loss did not improve from 0.69797\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.9387 - cluster_output_loss: 0.6009 - singleton_output_loss: 0.1126 - cluster_output_sparse_categorical_accuracy: 0.7415 - val_loss: 1.1343 - val_cluster_output_loss: 0.7331 - val_singleton_output_loss: 0.1337 - val_cluster_output_sparse_categorical_accuracy: 0.6828\n",
      "\n",
      "Epoch 00011: val_cluster_output_loss did not improve from 0.69797\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.9011 - cluster_output_loss: 0.5677 - singleton_output_loss: 0.1111 - cluster_output_sparse_categorical_accuracy: 0.7645 - val_loss: 1.1064 - val_cluster_output_loss: 0.7205 - val_singleton_output_loss: 0.1287 - val_cluster_output_sparse_categorical_accuracy: 0.7115\n",
      "\n",
      "Epoch 00012: val_cluster_output_loss did not improve from 0.69797\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss={'cluster_output':'sparse_categorical_crossentropy', 'singleton_output':custom_mse},\n",
    "              loss_weights={'cluster_output': 1.0, 'singleton_output': 3.0},\n",
    "              metrics={'cluster_output':\"sparse_categorical_accuracy\"})\n",
    "file_path = \"best_model.hdf5\"\n",
    "check_point = callbacks.ModelCheckpoint(file_path, monitor = \"val_cluster_output_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "early_stop = callbacks.EarlyStopping(monitor = \"val_cluster_output_loss\", mode = \"min\", patience=3)\n",
    "history = model.fit(X_train,{'cluster_output': y_tra, 'singleton_output':y_tra_AB} , \n",
    "                    batch_size=20, epochs=20, \n",
    "                    validation_data=(X_dev, {'cluster_output': y_dev, 'singleton_output':y_dev_AB}), \n",
    "                    shuffle=True, callbacks = [check_point, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "0abd676a3090c51df7c7f7c12d82f35754825e3e"
   },
   "outputs": [],
   "source": [
    "p_emb_test1, p_embed_test2, p_embed_test3 = extract_embedding_features(test_df, 'Text', 'Pronoun-offset')\n",
    "a_emb_test1, a_embed_test2, a_embed_test3 = extract_embedding_features(test_df, 'Text', 'A-offset')\n",
    "b_emb_test1, b_embed_test2, b_embed_test3 = extract_embedding_features(test_df, 'Text', 'B-offset')\n",
    "pa_pos_test = extract_dist_features(test_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pb_pos_test = extract_dist_features(test_df, 'Text', 'Pronoun-offset', 'B-offset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f77ca0b5b7bb42fca3747418cf3cdcc8c95fc1ab"
   },
   "outputs": [],
   "source": [
    "X_test = [p_emb_test1, p_embed_test2, p_embed_test3, a_emb_test1, a_embed_test2, a_embed_test3, b_emb_test1, b_embed_test2, b_embed_test3, pa_pos_test, pb_pos_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "bc650484c6a50e950d5d0d9da08a294c027cb2c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 642us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.256550</td>\n",
       "      <td>0.543487</td>\n",
       "      <td>0.199963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.984703</td>\n",
       "      <td>0.010564</td>\n",
       "      <td>0.004733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.245831</td>\n",
       "      <td>0.560125</td>\n",
       "      <td>0.194044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.161173</td>\n",
       "      <td>0.407429</td>\n",
       "      <td>0.431398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.481281</td>\n",
       "      <td>0.255429</td>\n",
       "      <td>0.263290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>development-6</td>\n",
       "      <td>0.975726</td>\n",
       "      <td>0.020907</td>\n",
       "      <td>0.003367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>development-7</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>0.107767</td>\n",
       "      <td>0.057760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>development-8</td>\n",
       "      <td>0.271912</td>\n",
       "      <td>0.606543</td>\n",
       "      <td>0.121544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>development-9</td>\n",
       "      <td>0.134429</td>\n",
       "      <td>0.776752</td>\n",
       "      <td>0.088819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>development-10</td>\n",
       "      <td>0.261940</td>\n",
       "      <td>0.560262</td>\n",
       "      <td>0.177798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>development-11</td>\n",
       "      <td>0.268505</td>\n",
       "      <td>0.560603</td>\n",
       "      <td>0.170892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>development-12</td>\n",
       "      <td>0.995073</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.001689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>development-13</td>\n",
       "      <td>0.896116</td>\n",
       "      <td>0.102330</td>\n",
       "      <td>0.001554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>development-14</td>\n",
       "      <td>0.496811</td>\n",
       "      <td>0.478473</td>\n",
       "      <td>0.024716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>development-15</td>\n",
       "      <td>0.519788</td>\n",
       "      <td>0.371936</td>\n",
       "      <td>0.108276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>development-16</td>\n",
       "      <td>0.535958</td>\n",
       "      <td>0.257522</td>\n",
       "      <td>0.206520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>development-17</td>\n",
       "      <td>0.508104</td>\n",
       "      <td>0.479845</td>\n",
       "      <td>0.012052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>development-18</td>\n",
       "      <td>0.381628</td>\n",
       "      <td>0.252905</td>\n",
       "      <td>0.365467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>development-19</td>\n",
       "      <td>0.475613</td>\n",
       "      <td>0.405193</td>\n",
       "      <td>0.119194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>development-20</td>\n",
       "      <td>0.143903</td>\n",
       "      <td>0.657032</td>\n",
       "      <td>0.199065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID         A         B   NEITHER\n",
       "0    development-1  0.256550  0.543487  0.199963\n",
       "1    development-2  0.984703  0.010564  0.004733\n",
       "2    development-3  0.245831  0.560125  0.194044\n",
       "3    development-4  0.161173  0.407429  0.431398\n",
       "4    development-5  0.481281  0.255429  0.263290\n",
       "5    development-6  0.975726  0.020907  0.003367\n",
       "6    development-7  0.834473  0.107767  0.057760\n",
       "7    development-8  0.271912  0.606543  0.121544\n",
       "8    development-9  0.134429  0.776752  0.088819\n",
       "9   development-10  0.261940  0.560262  0.177798\n",
       "10  development-11  0.268505  0.560603  0.170892\n",
       "11  development-12  0.995073  0.003238  0.001689\n",
       "12  development-13  0.896116  0.102330  0.001554\n",
       "13  development-14  0.496811  0.478473  0.024716\n",
       "14  development-15  0.519788  0.371936  0.108276\n",
       "15  development-16  0.535958  0.257522  0.206520\n",
       "16  development-17  0.508104  0.479845  0.012052\n",
       "17  development-18  0.381628  0.252905  0.365467\n",
       "18  development-19  0.475613  0.405193  0.119194\n",
       "19  development-20  0.143903  0.657032  0.199065"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./best_model.hdf5')\n",
    "layer_name = \"cluster_output\"\n",
    "predict_model = Model(model.inputs, model.get_layer(layer_name).output)\n",
    "\n",
    "y_preds = predict_model.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "\n",
    "sub_df_path = os.path.join('../input/gendered-pronoun-resolution/', 'sample_submission_stage_1.csv')\n",
    "sub_df = pd.read_csv(sub_df_path)\n",
    "sub_df.loc[:, 'A'] = pd.Series(y_preds[:, 0])\n",
    "sub_df.loc[:, 'B'] = pd.Series(y_preds[:, 1])\n",
    "sub_df.loc[:, 'NEITHER'] = pd.Series(y_preds[:, 2])\n",
    "\n",
    "sub_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "fd20bdb415789dae90a3754c2974a9dc9a7df824"
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
