{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The basic idea is from my kernel (https://www.kaggle.com/chanhu/bert-score-layer-lb-0-475).\n",
    "In this kernel, I had changed several points below.\n",
    "* keras -> pytorch(this is my second kernel wrote in pytorch)\n",
    "* use pretrain Bert, EndpointSpanExtractor, and weight decay.\n",
    "(similar to Lee's work https://www.kaggle.com/ceshine/pytorch-bert-endpointspanextractor-kfold) \n",
    "* use kfold to get a robust score.(according to the comment from Matei Ionita, and huiqin. Thanks!)\n",
    "\n",
    "P.S: the best I can get is 0.486. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best_model_1.hdf5', 'best_model_2.hdf5', '__output__.json', 'contextual_embeddings_gap_validation.json', 'contextual_embeddings_gap_train.json', 'train_dist_df.csv', 'tokenization.py', 'uncased_L-12_H-768_A-12', 'best_model_4.hdf5', 'contextual_embeddings_gap_test.json', '__results___files', '__notebook__.ipynb', 'test_dist_df.csv', 'uncased_L-12_H-768_A-12.zip', 'submission.csv', '__results__.html', 'modeling.py', 'custom.css', 'extract_features.py', '__pycache__', 'val_dist_df.csv', 'best_model_5.hdf5', 'best_model_3.hdf5']\n",
      "['gap-development.tsv', 'gap-test.tsv', 'gap-validation.tsv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "print(os.listdir('../input/bert-score-layer-lb-0-475'))\n",
    "print(os.listdir('../input/gap-coreference'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "3535d049dab0510188e7cb3d7319b8ea245b9505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\r\n",
      "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\r\n",
      "\r\n",
      "## Package Plan ##\r\n",
      "\r\n",
      "  environment location: /opt/conda\r\n",
      "\r\n",
      "  removed specs:\r\n",
      "    - greenlet\r\n",
      "\r\n",
      "\r\n",
      "The following packages will be REMOVED:\r\n",
      "\r\n",
      "  gevent-1.3.0-py36h14c3975_0\r\n",
      "  greenlet-0.4.13-py36h14c3975_0\r\n",
      "\r\n",
      "\r\n",
      "Preparing transaction: | \b\bdone\r\n",
      "Verifying transaction: - \b\bdone\r\n",
      "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\r\n",
      "Collecting pytorch-pretrained-bert\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 6.4MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2019.3.12)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.16.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.21.0)\r\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.0.1.post2)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.9.126)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.31.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.6)\r\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.22)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\r\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.126 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.12.126)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.126->boto3->pytorch-pretrained-bert) (2.6.0)\r\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.126->boto3->pytorch-pretrained-bert) (0.14)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.126->boto3->pytorch-pretrained-bert) (1.12.0)\r\n",
      "Installing collected packages: pytorch-pretrained-bert\r\n",
      "Successfully installed pytorch-pretrained-bert-0.6.1\r\n",
      "Collecting allennlp\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/c8/10342a6068a8d156a5947e03c95525d559e71ad62de0f2585ab922e14533/allennlp-0.8.3-py3-none-any.whl (5.6MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 5.6MB 6.2MB/s \r\n",
      "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/dc/3abd3971869a741d7acdba166d71d4f9366b6b53028dfd56f95de356af0f/jsonnet-0.12.1.tar.gz (240kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 245kB 29.0MB/s \r\n",
      "\u001b[?25hCollecting word2number>=1.1 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\r\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.5.1)\r\n",
      "Requirement already satisfied: requests>=2.18 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.21.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.16.2)\r\n",
      "Collecting gevent>=1.3.6 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/ca/5b5962361ed832847b6b2f9a2d0452c8c2f29a93baef850bb8ad067c7bf9/gevent-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 5.5MB 6.2MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /opt/conda/lib/python3.6/site-packages (from allennlp) (4.31.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.20.3)\r\n",
      "Collecting msgpack<0.6.0,>=0.5.6 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/4e/dcf124fd97e5f5611123d6ad9f40ffd6eb979d1efdc1049e28a795672fcd/msgpack-0.5.6-cp36-cp36m-manylinux1_x86_64.whl (315kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 25.4MB/s \r\n",
      "\u001b[?25hCollecting conllu==0.11 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\r\n",
      "Collecting flask-cors>=3.0.7 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/65/cb/683f71ff8daa3aea0a5cbb276074de39f9ab66d3fbb8ad5efb5bb83e90d2/Flask_Cors-3.0.7-py2.py3-none-any.whl\r\n",
      "Collecting sqlparse>=0.2.4 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/53/900f7d2a54557c6a37886585a91336520e5539e3ae2423ff1102daf4f3a7/sqlparse-0.3.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: flask>=1.0.2 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.0.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2018.4)\r\n",
      "Collecting overrides (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\r\n",
      "Requirement already satisfied: numpydoc>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.8.0)\r\n",
      "Collecting parsimonious>=0.8.0 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 15.5MB/s \r\n",
      "\u001b[?25hCollecting moto>=1.3.4 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/40/cec89fa5c13108eb1c8de435633f8b7639e0e43fcbcdc8ac52633efeeabe/moto-1.3.7-py2.py3-none-any.whl (552kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 22.6MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.2,>=2.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.1.3)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.2.4)\r\n",
      "Requirement already satisfied: tensorboardX>=1.2 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.6)\r\n",
      "Collecting awscli>=1.11.91 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/a6/1aacae318329a0315121933261a290766c623c4d9585c1d0632fc609ba25/awscli-1.16.138-py2.py3-none-any.whl (1.5MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 16.5MB/s \r\n",
      "\u001b[?25hCollecting editdistance (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/67/2b1fe72bdd13ee9ec32b97959d7dfbfcd7c0548081d69aaf8493c1e695f9/editdistance-0.5.3-cp36-cp36m-manylinux1_x86_64.whl (178kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 184kB 38.0MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.0.3)\r\n",
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.0.23)\r\n",
      "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.6.1)\r\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.6/site-packages (from allennlp) (4.4.3)\r\n",
      "Collecting responses>=0.7 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.9.126)\r\n",
      "Collecting flaky (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/02/42/cca66659a786567c8af98587d66d75e7d2b6e65662f8daab75db708ac35b/flaky-3.5.3-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.1.0)\r\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.0.1.post2)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.9.0)\r\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (1.5.3)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (1.12.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (39.1.0)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (18.1.0)\r\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (4.1.0)\r\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (0.6.0)\r\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (1.22)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (2.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (2019.3.9)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (3.0.4)\r\n",
      "Collecting greenlet>=0.4.14; platform_python_implementation == \"CPython\" (from gevent>=1.3.6->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl (41kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 22.1MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp) (0.24)\r\n",
      "Requirement already satisfied: Werkzeug>=0.14 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp) (0.14.1)\r\n",
      "Requirement already satisfied: Jinja2>=2.10 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp) (2.10)\r\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp) (7.0)\r\n",
      "Requirement already satisfied: sphinx>=1.2.3 in /opt/conda/lib/python3.6/site-packages (from numpydoc>=0.8.0->allennlp) (1.7.4)\r\n",
      "Collecting pyaml (from moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/e1/1523fb1dab744e2c6b1f02446f2139a78726c18c062a8ddd53875abb20f8/pyaml-18.11.0-py2.py3-none-any.whl\r\n",
      "Collecting aws-xray-sdk<0.96,>=0.93 (from moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a5/da7887285564f9e0ae5cd25a453cca36e2cd43d8ccc9effde260b4d80904/aws_xray_sdk-0.95-py2.py3-none-any.whl (52kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 26.3MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: botocore>=1.12.13 in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (1.12.126)\r\n",
      "Collecting docker>=2.5.1 (from moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/68/c3afca1a5aa8d2997ec3b8ee822a4d752cf85907b321f07ea86888545152/docker-3.7.2-py2.py3-none-any.whl (134kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 37.8MB/s \r\n",
      "\u001b[?25hCollecting jsondiff==1.1.1 (from moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/5f/13e28a2f9abeda2ffb3f44f2f809b01b52bc02cdb63816e05b8c9cbbdfc5/jsondiff-1.1.1.tar.gz\r\n",
      "Collecting python-jose<3.0.0 (from moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/5c/5fa238c0c5b0656994b52721dd8b1d7bf52ebd8786518dde794f44de86b6/python_jose-2.0.2-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (2.6.0)\r\n",
      "Requirement already satisfied: boto>=2.36.0 in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (2.48.0)\r\n",
      "Collecting xmltodict (from moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\r\n",
      "Collecting cryptography>=2.3.0 (from moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/12/b0409a94dad366d98a8eee2a77678c7a73aafd8c0e4b835abea634ea3896/cryptography-2.6.1-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 2.3MB 13.5MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: mock in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (2.0.0)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (0.2.1)\r\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (0.9.6)\r\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (2.0.1)\r\n",
      "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (2.6.0)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (2.0.2)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (0.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (1.0.0)\r\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (0.2.4)\r\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (7.0.4)\r\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\r\n",
      "Requirement already satisfied: PyYAML<=3.13,>=3.10 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (3.12)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (0.2.0)\r\n",
      "Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 25.0MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: colorama<=0.3.9,>=0.2.5 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (0.3.9)\r\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (0.14)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (2.2.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.3.12)\r\n",
      "Requirement already satisfied: html5lib in /opt/conda/lib/python3.6/site-packages (from ftfy->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy->allennlp) (0.1.7)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->allennlp) (0.9.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=2.10->flask>=1.0.2->allennlp) (1.0)\r\n",
      "Requirement already satisfied: Pygments>=2.0 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (2.2.0)\r\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.2.1)\r\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (2.5.3)\r\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (0.7.10)\r\n",
      "Requirement already satisfied: imagesize in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.0.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (17.1)\r\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: jsonpickle in /opt/conda/lib/python3.6/site-packages (from aws-xray-sdk<0.96,>=0.93->moto>=1.3.4->allennlp) (0.9.6)\r\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.6/site-packages (from aws-xray-sdk<0.96,>=0.93->moto>=1.3.4->allennlp) (1.10.11)\r\n",
      "Collecting docker-pycreds>=0.4.0 (from docker>=2.5.1->moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from docker>=2.5.1->moto>=1.3.4->allennlp) (0.56.0)\r\n",
      "Collecting pycryptodome<4.0.0,>=3.3.1 (from python-jose<3.0.0->moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/b4/7d007370568d98822c833e0d0e804417620865710dc8a5830bbed58328d1/pycryptodome-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (9.7MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 9.7MB 4.2MB/s \r\n",
      "\u001b[?25hCollecting ecdsa<1.0 (from python-jose<3.0.0->moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f4/73669d51825516ce8c43b816c0a6b64cd6eb71d08b99820c00792cb42222/ecdsa-0.13-py2.py3-none-any.whl (86kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 32.6MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: future<1.0 in /opt/conda/lib/python3.6/site-packages (from python-jose<3.0.0->moto>=1.3.4->allennlp) (0.17.1)\r\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp) (0.24.0)\r\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp) (1.11.5)\r\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.6/site-packages (from mock->moto>=1.3.4->allennlp) (5.1.3)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\r\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.6/site-packages (from html5lib->ftfy->allennlp) (0.5.1)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3.0->moto>=1.3.4->allennlp) (2.18)\r\n",
      "Building wheels for collected packages: jsonnet, word2number, overrides, parsimonious, jsondiff\r\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/f0/47/51/a178b15274ed0db775a1ae9c799ce31e511609c3ab75a7dec5\r\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\r\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\r\n",
      "  Building wheel for parsimonious (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\r\n",
      "  Building wheel for jsondiff (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/68/08/07/69d839606fb7fdc778fa86476abc0a864693d45969a0c1936c\r\n",
      "Successfully built jsonnet word2number overrides parsimonious jsondiff\r\n",
      "\u001b[31mawscli 1.16.138 has requirement botocore==1.12.128, but you'll have botocore 1.12.126 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: jsonnet, word2number, greenlet, gevent, msgpack, conllu, flask-cors, sqlparse, overrides, parsimonious, pyaml, responses, aws-xray-sdk, docker-pycreds, docker, jsondiff, pycryptodome, ecdsa, python-jose, xmltodict, cryptography, moto, rsa, awscli, editdistance, flaky, allennlp\r\n",
      "  Found existing installation: msgpack 0.6.1\r\n",
      "    Uninstalling msgpack-0.6.1:\r\n",
      "      Successfully uninstalled msgpack-0.6.1\r\n",
      "  Found existing installation: Flask-Cors 3.0.4\r\n",
      "    Uninstalling Flask-Cors-3.0.4:\r\n",
      "      Successfully uninstalled Flask-Cors-3.0.4\r\n",
      "  Found existing installation: cryptography 2.2.2\r\n",
      "    Uninstalling cryptography-2.2.2:\r\n",
      "      Successfully uninstalled cryptography-2.2.2\r\n",
      "  Found existing installation: rsa 4.0\r\n",
      "    Uninstalling rsa-4.0:\r\n",
      "      Successfully uninstalled rsa-4.0\r\n",
      "Successfully installed allennlp-0.8.3 aws-xray-sdk-0.95 awscli-1.16.138 conllu-0.11 cryptography-2.6.1 docker-3.7.2 docker-pycreds-0.4.0 ecdsa-0.13 editdistance-0.5.3 flaky-3.5.3 flask-cors-3.0.7 gevent-1.4.0 greenlet-0.4.15 jsondiff-1.1.1 jsonnet-0.12.1 moto-1.3.7 msgpack-0.5.6 overrides-1.9 parsimonious-0.8.1 pyaml-18.11.0 pycryptodome-3.8.0 python-jose-2.0.2 responses-0.10.6 rsa-3.4.2 sqlparse-0.3.0 word2number-1.1 xmltodict-0.12.0\r\n"
     ]
    }
   ],
   "source": [
    "!conda remove -y greenlet\n",
    "!pip install pytorch-pretrained-bert\n",
    "!pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 490376.16B/s]\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.span_extractors import EndpointSpanExtractor \n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentencizer = nlp.create_pipe('sentencizer')\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "803a2bcc22037e64732556cac31e2ce326c3696e"
   },
   "outputs": [],
   "source": [
    "def candidate_length(candidate):\n",
    "    count = 0\n",
    "    for i in range(len(candidate)):\n",
    "        if candidate[i] !=  \" \": count += 1\n",
    "    return count\n",
    "\n",
    "def count_char(text, offset):\n",
    "    count = 0\n",
    "    for pos in range(offset):\n",
    "        if text[pos] != \" \": count +=1\n",
    "    return count\n",
    "\n",
    "def count_token_length_special(token):\n",
    "    count = 0\n",
    "    special_token = [\"#\", \" \"]\n",
    "    for i in range(len(token)):\n",
    "        if token[i] not in special_token: \n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def find_word_index(tokenized_text, char_start, target):\n",
    "    tar_len = candidate_length(target)\n",
    "    char_count = 0\n",
    "    word_index = []\n",
    "    special_token = [\"[CLS]\", \"[SEP]\"]\n",
    "    for i in range(len(tokenized_text)):\n",
    "        token = tokenized_text[i]\n",
    "        if char_count in range(char_start, char_start+tar_len):\n",
    "            if token in special_token: # for the case like \"[SEP]. she\"\n",
    "                continue\n",
    "            word_index.append(i)\n",
    "        if token not in special_token:\n",
    "            token_length = count_token_length_special(token)\n",
    "            char_count += token_length\n",
    "    \n",
    "    if len(word_index) == 1:\n",
    "        return [word_index[0], word_index[0]]\n",
    "    else:\n",
    "        return [word_index[0], word_index[-1]]\n",
    "\n",
    "def create_tokenizer_input(sents):\n",
    "    tokenizer_input = str()\n",
    "    for i, sent in enumerate(sents):\n",
    "        if i == 0:\n",
    "            tokenizer_input += \"[CLS] \"+sent.text+\" [SEP] \"\n",
    "        elif i == len(sents) - 1:\n",
    "            tokenizer_input += sent.text+\" [SEP]\"\n",
    "        else:\n",
    "            tokenizer_input += sent.text+\" [SEP] \"\n",
    "            \n",
    "    return  tokenizer_input\n",
    "\n",
    "def create_inputs(dataframe):\n",
    "    \n",
    "    idxs = dataframe.index\n",
    "    columns = ['indexed_token', 'offset']\n",
    "    features_df = pd.DataFrame(index=idxs, columns=columns)\n",
    "    max_len = 0\n",
    "    for i in tqdm(range(len(dataframe))):\n",
    "        text           = dataframe.loc[i, 'Text']\n",
    "        Pronoun_offset = dataframe.loc[i, 'Pronoun-offset']\n",
    "        A_offset       = dataframe.loc[i, \"A-offset\"]\n",
    "        B_offset       = dataframe.loc[i, \"B-offset\"]\n",
    "        Pronoun        = dataframe.loc[i, \"Pronoun\"]\n",
    "        A              = dataframe.loc[i, \"A\"]\n",
    "        B              = dataframe.loc[i, \"B\"]\n",
    "        doc            = nlp(text)\n",
    "        \n",
    "        sents = []\n",
    "        for sent in doc.sents: sents.append(sent)\n",
    "        token_input = create_tokenizer_input(sents)\n",
    "        token_input = token_input.replace(\"#\", \"*\")\n",
    "        tokenized_text = tokenizer.tokenize(token_input)\n",
    "        if len(tokenized_text) > max_len: max_len = len(tokenized_text)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        \n",
    "        A_char_start, B_char_start = count_char(text, A_offset), count_char(text, B_offset)\n",
    "        Pronoun_char_start         = count_char(text, Pronoun_offset)\n",
    "        \n",
    "        word_indexes = []\n",
    "        for char_start, target in zip([A_char_start, B_char_start, Pronoun_char_start], [A, B, Pronoun]):\n",
    "            word_indexes.append(find_word_index(tokenized_text, char_start, target))\n",
    "        features_df.iloc[i] = [indexed_tokens, word_indexes]\n",
    "    print('max length of sentence:', max_len)\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:09<00:00, 204.62it/s]\n",
      "  1%|          | 22/2000 [00:00<00:09, 211.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of sentence: 357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:08<00:00, 224.29it/s]\n",
      " 17%|█▋        | 76/454 [00:00<00:01, 251.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of sentence: 353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:01<00:00, 249.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of sentence: 237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_table('../input/gap-coreference/gap-test.tsv')\n",
    "test_df  = pd.read_table('../input/gap-coreference/gap-development.tsv')\n",
    "val_df   = pd.read_table('../input/gap-coreference/gap-validation.tsv')\n",
    "new_train_df = create_inputs(train_df)\n",
    "new_test_df  = create_inputs(test_df)\n",
    "new_val_df   = create_inputs(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "470f21a85ce86d3c8853776a20817d43bd4f6025"
   },
   "outputs": [],
   "source": [
    "def get_label(dataframe):\n",
    "    labels = []\n",
    "    for i in range(len(dataframe)):\n",
    "        if dataframe.loc[i, 'A-coref']:\n",
    "            labels.append(0)\n",
    "        elif dataframe.loc[i, 'B-coref']:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(2)\n",
    "            \n",
    "    return labels\n",
    "\n",
    "new_train_df['label'] = get_label(train_df)\n",
    "new_val_df['label']   = get_label(val_df)\n",
    "new_df = pd.concat([new_train_df, new_val_df])\n",
    "new_df = new_df.reset_index(drop=True)\n",
    "new_df.to_csv('train.csv', index=False)\n",
    "new_test_df['label'] = get_label(test_df)\n",
    "new_test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_df\n",
    "del new_val_df\n",
    "del new_test_df\n",
    "del new_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "31405d984490fccf860fb68c5a9aceafc0d57cb8"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from ast import literal_eval\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        index_token = self.df.loc[idx, 'indexed_token']\n",
    "        index_token = literal_eval(index_token) # Change string to list\n",
    "        index_token = pad_sequences([index_token], maxlen=360, padding='post')[0] #pad \n",
    "        \n",
    "        offset = self.df.loc[idx, 'offset']\n",
    "        offset = literal_eval(offset)\n",
    "        offset = np.asarray(offset, dtype='int32')\n",
    "        label  = int(self.df.loc[idx, 'label'])\n",
    "        \n",
    "        distP_A = self.df.loc[idx, 'D_PA']\n",
    "        distP_B = self.df.loc[idx, 'D_PB']\n",
    "        \n",
    "        if self.transform:\n",
    "            index_token = self.transform(index_token)\n",
    "            offset = self.transform(offset)\n",
    "            label = self.transform(label)\n",
    "        \n",
    "        return (index_token, offset, distP_A, distP_B), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class score(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super(score, self).__init__()\n",
    "        self.score = torch.nn.Sequential(\n",
    "                     torch.nn.Linear(embed_dim, hidden_dim),\n",
    "                     torch.nn.LayerNorm(hidden_dim),\n",
    "                     torch.nn.ReLU(inplace=True),\n",
    "                     torch.nn.Dropout(0.6),\n",
    "                     torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "                     torch.nn.LayerNorm(hidden_dim),\n",
    "                     torch.nn.ReLU(inplace=True),\n",
    "                     torch.nn.Dropout(0.6),\n",
    "                     torch.nn.Linear(hidden_dim, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.score(x)\n",
    "    \n",
    "class mentionpair_score(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(mentionpair_score, self).__init__()\n",
    "        self.score = score(input_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, g1, g2, dist_embed): #sent_repres\n",
    "        \n",
    "        element_wise = g1 * g2\n",
    "        pair_score   = self.score(torch.cat((g1, g2, element_wise, dist_embed), dim=-1)) #sent_repres\n",
    "        \n",
    "        return pair_score\n",
    "\n",
    "class score_model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(score_model, self).__init__()\n",
    "        self.buckets        = [1, 2, 3, 4, 5, 8, 16, 32, 64] \n",
    "        self.bert           = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.embedding      = torch.nn.Embedding(len(self.buckets)+1, 20)\n",
    "        self.span_extractor = EndpointSpanExtractor(768, \"x,y,x*y\")\n",
    "        self.pair_score     = mentionpair_score(2304*3+20, 100)\n",
    "        \n",
    "    def forward(self, sent, offsets, distP_A, distP_B):\n",
    "        \n",
    "        bert_output, _   = self.bert(sent, output_all_encoded_layers=False) # (batch_size, max_len, 768)\n",
    "        #Distance Embeddings\n",
    "        distPA_embed     = self.embedding(distP_A)\n",
    "        distPB_embed     = self.embedding(distP_B)\n",
    "        \n",
    "        #Span Representation\n",
    "        span_repres     = self.span_extractor(bert_output, offsets) #(batch, 3, 2304)\n",
    "        span_repres     = torch.unbind(span_repres, dim=1) #[A: (bath, 2304), B: (bath, 2304), Pronoun:  (bath, 2304)]\n",
    "        span_norm = []\n",
    "        for i in range(len(span_repres)): \n",
    "            span_norm.append(F.normalize(span_repres[i], p=2, dim=1)) #avoid overfitting\n",
    "    \n",
    "        ap_score = self.pair_score(span_norm[2], span_norm[0], distPA_embed)\n",
    "        bp_score = self.pair_score(span_norm[2], span_norm[1], distPB_embed)\n",
    "        nan_score = torch.zeros_like(ap_score)\n",
    "        output = torch.cat((ap_score, bp_score, nan_score), dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Code from https://www.kaggle.com/ceshine/pytorch-bert-endpointspanextractor-kfold\n",
    "\n",
    "def children(m):\n",
    "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "def set_trainable_attr(m, b):\n",
    "    m.trainable = b\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = b\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, torch.nn.Module):\n",
    "        f(m)\n",
    "    if len(c) > 0:\n",
    "        for l in c:\n",
    "            apply_leaf(l, f)\n",
    "\n",
    "            \n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dist = pd.read_csv('../input/bert-score-layer-lb-0-475/train_dist_df.csv')\n",
    "val_dist   = pd.read_csv('../input/bert-score-layer-lb-0-475/val_dist_df.csv')\n",
    "test_dist  = pd.read_csv('../input/bert-score-layer-lb-0-475/test_dist_df.csv')\n",
    "\n",
    "train_dist = pd.concat([train_dist, val_dist])\n",
    "train_dist = train_dist.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "n_split = 5\n",
    "\n",
    "train = pd.read_csv('../working/train.csv')\n",
    "test  = pd.read_csv('../working/test.csv')\n",
    "\n",
    "train = pd.concat([train, train_dist], axis=1)\n",
    "test  = pd.concat([test, test_dist], axis=1)\n",
    "train.head()\n",
    "Kfold = StratifiedKFold(n_splits=n_split, random_state=2019).split(train, train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407873900/407873900 [00:12<00:00, 32149456.41B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fold: 1\n",
      "Epoch 1/30 \t loss=0.9957 \t val_loss=0.8718 \t time=38.64s\n",
      "Epoch 2/30 \t loss=0.8997 \t val_loss=0.7223 \t time=38.19s\n",
      "Epoch 3/30 \t loss=0.7969 \t val_loss=0.6650 \t time=38.10s\n",
      "Epoch 4/30 \t loss=0.7409 \t val_loss=0.6261 \t time=38.13s\n",
      "Epoch 5/30 \t loss=0.7115 \t val_loss=0.5980 \t time=38.14s\n",
      "Epoch 6/30 \t loss=0.6610 \t val_loss=0.5978 \t time=38.20s\n",
      "Epoch 7/30 \t loss=0.6422 \t val_loss=0.5793 \t time=38.13s\n",
      "Epoch 8/30 \t loss=0.6094 \t val_loss=0.5777 \t time=37.90s\n",
      "Epoch 9/30 \t loss=0.6032 \t val_loss=0.5995 \t time=37.77s\n",
      "Epoch 10/30 \t loss=0.5796 \t val_loss=0.5868 \t time=37.88s\n",
      "Epoch 11/30 \t loss=0.5471 \t val_loss=0.5934 \t time=37.67s\n",
      "Epoch 12/30 \t loss=0.5509 \t val_loss=0.5749 \t time=37.77s\n",
      "Epoch 13/30 \t loss=0.5402 \t val_loss=0.5789 \t time=37.78s\n",
      "Epoch 14/30 \t loss=0.5296 \t val_loss=0.5988 \t time=37.79s\n",
      "Epoch 15/30 \t loss=0.4896 \t val_loss=0.5864 \t time=37.68s\n",
      "Epoch 16/30 \t loss=0.4955 \t val_loss=0.5936 \t time=37.77s\n",
      "Epoch 17/30 \t loss=0.4576 \t val_loss=0.6287 \t time=37.70s\n",
      "Epoch 18/30 \t loss=0.4826 \t val_loss=0.5871 \t time=37.81s\n",
      "Epoch 19/30 \t loss=0.4599 \t val_loss=0.5889 \t time=37.77s\n",
      "Epoch 20/30 \t loss=0.4427 \t val_loss=0.5787 \t time=37.75s\n",
      "Epoch 21/30 \t loss=0.4386 \t val_loss=0.5906 \t time=37.77s\n",
      "Epoch 22/30 \t loss=0.4118 \t val_loss=0.6314 \t time=37.69s\n",
      "Epoch 23/30 \t loss=0.4107 \t val_loss=0.6112 \t time=37.76s\n",
      "Epoch 24/30 \t loss=0.3774 \t val_loss=0.6300 \t time=37.70s\n",
      "Epoch 25/30 \t loss=0.4060 \t val_loss=0.6097 \t time=37.79s\n",
      "Epoch 26/30 \t loss=0.3682 \t val_loss=0.6195 \t time=37.71s\n",
      "Epoch 27/30 \t loss=0.3851 \t val_loss=0.6123 \t time=38.09s\n",
      "Epoch 28/30 \t loss=0.3554 \t val_loss=0.6269 \t time=37.80s\n",
      "Epoch 29/30 \t loss=0.3548 \t val_loss=0.6412 \t time=38.05s\n",
      "Epoch 30/30 \t loss=0.3378 \t val_loss=0.6993 \t time=37.72s\n",
      "n_fold: 2\n",
      "Epoch 1/30 \t loss=0.9958 \t val_loss=0.8561 \t time=37.96s\n",
      "Epoch 2/30 \t loss=0.8740 \t val_loss=0.7140 \t time=38.18s\n",
      "Epoch 3/30 \t loss=0.7676 \t val_loss=0.6442 \t time=38.13s\n",
      "Epoch 4/30 \t loss=0.7301 \t val_loss=0.6172 \t time=38.34s\n",
      "Epoch 5/30 \t loss=0.6889 \t val_loss=0.6053 \t time=38.24s\n",
      "Epoch 6/30 \t loss=0.6488 \t val_loss=0.5621 \t time=38.34s\n",
      "Epoch 7/30 \t loss=0.6167 \t val_loss=0.5733 \t time=38.21s\n",
      "Epoch 8/30 \t loss=0.6351 \t val_loss=0.5872 \t time=38.25s\n",
      "Epoch 9/30 \t loss=0.6046 \t val_loss=0.5509 \t time=38.18s\n",
      "Epoch 10/30 \t loss=0.5747 \t val_loss=0.5399 \t time=38.28s\n",
      "Epoch 11/30 \t loss=0.5563 \t val_loss=0.5323 \t time=38.28s\n",
      "Epoch 12/30 \t loss=0.5315 \t val_loss=0.5418 \t time=38.32s\n",
      "Epoch 13/30 \t loss=0.5384 \t val_loss=0.5502 \t time=38.22s\n",
      "Epoch 14/30 \t loss=0.5227 \t val_loss=0.5513 \t time=38.25s\n",
      "Epoch 15/30 \t loss=0.4953 \t val_loss=0.5576 \t time=38.25s\n",
      "Epoch 16/30 \t loss=0.5007 \t val_loss=0.5308 \t time=38.19s\n",
      "Epoch 17/30 \t loss=0.4889 \t val_loss=0.5321 \t time=38.31s\n",
      "Epoch 18/30 \t loss=0.4651 \t val_loss=0.5400 \t time=38.19s\n",
      "Epoch 19/30 \t loss=0.4518 \t val_loss=0.5405 \t time=38.25s\n",
      "Epoch 20/30 \t loss=0.4466 \t val_loss=0.5308 \t time=38.28s\n",
      "Epoch 21/30 \t loss=0.4225 \t val_loss=0.5439 \t time=38.25s\n",
      "Epoch 22/30 \t loss=0.4082 \t val_loss=0.5530 \t time=38.24s\n",
      "Epoch 23/30 \t loss=0.3872 \t val_loss=0.5366 \t time=38.25s\n",
      "Epoch 24/30 \t loss=0.4151 \t val_loss=0.5620 \t time=38.16s\n",
      "Epoch 25/30 \t loss=0.3789 \t val_loss=0.5479 \t time=37.87s\n",
      "Epoch 26/30 \t loss=0.3809 \t val_loss=0.5333 \t time=37.78s\n",
      "Epoch 27/30 \t loss=0.3436 \t val_loss=0.5800 \t time=37.85s\n",
      "Epoch 28/30 \t loss=0.3520 \t val_loss=0.6101 \t time=37.87s\n",
      "Epoch 29/30 \t loss=0.3632 \t val_loss=0.5798 \t time=37.81s\n",
      "Epoch 30/30 \t loss=0.3301 \t val_loss=0.5867 \t time=37.81s\n",
      "n_fold: 3\n",
      "Epoch 1/30 \t loss=0.9967 \t val_loss=0.8394 \t time=37.75s\n",
      "Epoch 2/30 \t loss=0.8693 \t val_loss=0.6692 \t time=37.83s\n",
      "Epoch 3/30 \t loss=0.8052 \t val_loss=0.6257 \t time=37.81s\n",
      "Epoch 4/30 \t loss=0.7172 \t val_loss=0.5919 \t time=37.80s\n",
      "Epoch 5/30 \t loss=0.7016 \t val_loss=0.5897 \t time=38.00s\n",
      "Epoch 6/30 \t loss=0.6556 \t val_loss=0.5443 \t time=37.79s\n",
      "Epoch 7/30 \t loss=0.6392 \t val_loss=0.5553 \t time=37.94s\n",
      "Epoch 8/30 \t loss=0.6166 \t val_loss=0.5655 \t time=37.77s\n",
      "Epoch 9/30 \t loss=0.5840 \t val_loss=0.5550 \t time=37.86s\n",
      "Epoch 10/30 \t loss=0.5664 \t val_loss=0.5693 \t time=37.77s\n",
      "Epoch 11/30 \t loss=0.5440 \t val_loss=0.5533 \t time=37.90s\n",
      "Epoch 12/30 \t loss=0.5411 \t val_loss=0.5576 \t time=37.77s\n",
      "Epoch 13/30 \t loss=0.5664 \t val_loss=0.5391 \t time=37.94s\n",
      "Epoch 14/30 \t loss=0.5206 \t val_loss=0.5635 \t time=37.84s\n",
      "Epoch 15/30 \t loss=0.4841 \t val_loss=0.5566 \t time=37.90s\n",
      "Epoch 16/30 \t loss=0.4830 \t val_loss=0.5337 \t time=37.78s\n",
      "Epoch 17/30 \t loss=0.4569 \t val_loss=0.5829 \t time=37.88s\n",
      "Epoch 18/30 \t loss=0.4629 \t val_loss=0.5644 \t time=37.75s\n",
      "Epoch 19/30 \t loss=0.4534 \t val_loss=0.5633 \t time=37.84s\n",
      "Epoch 20/30 \t loss=0.4332 \t val_loss=0.5785 \t time=37.73s\n",
      "Epoch 21/30 \t loss=0.4384 \t val_loss=0.5704 \t time=37.82s\n",
      "Epoch 22/30 \t loss=0.4065 \t val_loss=0.5313 \t time=37.79s\n",
      "Epoch 23/30 \t loss=0.4226 \t val_loss=0.5694 \t time=37.90s\n",
      "Epoch 24/30 \t loss=0.3924 \t val_loss=0.5643 \t time=37.85s\n",
      "Epoch 25/30 \t loss=0.3669 \t val_loss=0.5563 \t time=37.75s\n",
      "Epoch 26/30 \t loss=0.3662 \t val_loss=0.5856 \t time=37.85s\n",
      "Epoch 27/30 \t loss=0.3486 \t val_loss=0.6035 \t time=37.80s\n",
      "Epoch 28/30 \t loss=0.3399 \t val_loss=0.6157 \t time=37.85s\n",
      "Epoch 29/30 \t loss=0.3209 \t val_loss=0.6102 \t time=37.76s\n",
      "Epoch 30/30 \t loss=0.3338 \t val_loss=0.5772 \t time=37.96s\n",
      "n_fold: 4\n",
      "Epoch 1/30 \t loss=1.0140 \t val_loss=0.8911 \t time=37.87s\n",
      "Epoch 2/30 \t loss=0.8726 \t val_loss=0.7261 \t time=37.80s\n",
      "Epoch 3/30 \t loss=0.7856 \t val_loss=0.6439 \t time=37.94s\n",
      "Epoch 4/30 \t loss=0.7185 \t val_loss=0.6285 \t time=37.81s\n",
      "Epoch 5/30 \t loss=0.6694 \t val_loss=0.5949 \t time=37.90s\n",
      "Epoch 6/30 \t loss=0.6772 \t val_loss=0.5841 \t time=37.80s\n",
      "Epoch 7/30 \t loss=0.6407 \t val_loss=0.5986 \t time=38.01s\n",
      "Epoch 8/30 \t loss=0.5918 \t val_loss=0.5825 \t time=37.83s\n",
      "Epoch 9/30 \t loss=0.5964 \t val_loss=0.5937 \t time=37.88s\n",
      "Epoch 10/30 \t loss=0.5644 \t val_loss=0.5443 \t time=37.78s\n",
      "Epoch 11/30 \t loss=0.5584 \t val_loss=0.5554 \t time=37.82s\n",
      "Epoch 12/30 \t loss=0.5610 \t val_loss=0.5542 \t time=37.85s\n",
      "Epoch 13/30 \t loss=0.5119 \t val_loss=0.5451 \t time=37.76s\n",
      "Epoch 14/30 \t loss=0.4993 \t val_loss=0.5740 \t time=37.86s\n",
      "Epoch 15/30 \t loss=0.5058 \t val_loss=0.5328 \t time=37.88s\n",
      "Epoch 16/30 \t loss=0.4910 \t val_loss=0.5368 \t time=37.92s\n",
      "Epoch 17/30 \t loss=0.4881 \t val_loss=0.5369 \t time=37.77s\n",
      "Epoch 18/30 \t loss=0.4544 \t val_loss=0.5611 \t time=37.88s\n",
      "Epoch 19/30 \t loss=0.4729 \t val_loss=0.5401 \t time=37.77s\n",
      "Epoch 20/30 \t loss=0.4428 \t val_loss=0.5176 \t time=37.87s\n",
      "Epoch 21/30 \t loss=0.4245 \t val_loss=0.5508 \t time=37.81s\n",
      "Epoch 22/30 \t loss=0.4200 \t val_loss=0.5118 \t time=37.84s\n",
      "Epoch 23/30 \t loss=0.4010 \t val_loss=0.5611 \t time=37.91s\n",
      "Epoch 24/30 \t loss=0.3912 \t val_loss=0.5843 \t time=37.89s\n",
      "Epoch 25/30 \t loss=0.4038 \t val_loss=0.5811 \t time=37.75s\n",
      "Epoch 26/30 \t loss=0.3718 \t val_loss=0.5456 \t time=37.82s\n",
      "Epoch 27/30 \t loss=0.3573 \t val_loss=0.5183 \t time=37.76s\n",
      "Epoch 28/30 \t loss=0.3446 \t val_loss=0.5532 \t time=37.81s\n",
      "Epoch 29/30 \t loss=0.3426 \t val_loss=0.5359 \t time=37.82s\n",
      "Epoch 30/30 \t loss=0.3404 \t val_loss=0.5300 \t time=37.75s\n",
      "n_fold: 5\n",
      "Epoch 1/30 \t loss=0.9974 \t val_loss=0.8568 \t time=37.83s\n",
      "Epoch 2/30 \t loss=0.8635 \t val_loss=0.7102 \t time=37.90s\n",
      "Epoch 3/30 \t loss=0.7632 \t val_loss=0.6547 \t time=37.86s\n",
      "Epoch 4/30 \t loss=0.7128 \t val_loss=0.6331 \t time=37.90s\n",
      "Epoch 5/30 \t loss=0.6817 \t val_loss=0.6094 \t time=37.86s\n",
      "Epoch 6/30 \t loss=0.6440 \t val_loss=0.5981 \t time=37.95s\n",
      "Epoch 7/30 \t loss=0.6213 \t val_loss=0.5845 \t time=37.90s\n",
      "Epoch 8/30 \t loss=0.6097 \t val_loss=0.6065 \t time=38.00s\n",
      "Epoch 9/30 \t loss=0.5763 \t val_loss=0.5636 \t time=37.82s\n",
      "Epoch 10/30 \t loss=0.5558 \t val_loss=0.5520 \t time=37.89s\n",
      "Epoch 11/30 \t loss=0.5541 \t val_loss=0.5583 \t time=37.86s\n",
      "Epoch 12/30 \t loss=0.5515 \t val_loss=0.5671 \t time=37.86s\n",
      "Epoch 13/30 \t loss=0.5330 \t val_loss=0.5578 \t time=37.77s\n",
      "Epoch 14/30 \t loss=0.5093 \t val_loss=0.5516 \t time=37.84s\n",
      "Epoch 15/30 \t loss=0.5034 \t val_loss=0.5606 \t time=37.83s\n",
      "Epoch 16/30 \t loss=0.4788 \t val_loss=0.5494 \t time=37.87s\n",
      "Epoch 17/30 \t loss=0.4713 \t val_loss=0.5774 \t time=37.84s\n",
      "Epoch 18/30 \t loss=0.4647 \t val_loss=0.5478 \t time=37.79s\n",
      "Epoch 19/30 \t loss=0.4667 \t val_loss=0.5414 \t time=37.83s\n",
      "Epoch 20/30 \t loss=0.4280 \t val_loss=0.5416 \t time=37.82s\n",
      "Epoch 21/30 \t loss=0.4272 \t val_loss=0.5344 \t time=37.84s\n",
      "Epoch 22/30 \t loss=0.4292 \t val_loss=0.5267 \t time=37.82s\n",
      "Epoch 23/30 \t loss=0.4081 \t val_loss=0.5539 \t time=37.92s\n",
      "Epoch 24/30 \t loss=0.4010 \t val_loss=0.5320 \t time=37.83s\n",
      "Epoch 25/30 \t loss=0.3628 \t val_loss=0.5395 \t time=37.86s\n",
      "Epoch 26/30 \t loss=0.3777 \t val_loss=0.5022 \t time=37.74s\n",
      "Epoch 27/30 \t loss=0.3734 \t val_loss=0.5461 \t time=37.94s\n",
      "Epoch 28/30 \t loss=0.3596 \t val_loss=0.5302 \t time=37.80s\n",
      "Epoch 29/30 \t loss=0.3247 \t val_loss=0.5641 \t time=37.84s\n",
      "Epoch 30/30 \t loss=0.3321 \t val_loss=0.5407 \t time=37.74s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from torch.optim.lr_scheduler import \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    y = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    return y\n",
    "\n",
    "output = np.zeros((len(test_df), 3))\n",
    "testset = MyDataset(test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=20)\n",
    "n_epochs = 30\n",
    "\n",
    "for n_fold, (train_index, val_index) in enumerate(Kfold):\n",
    "    min_val_loss = 100.0\n",
    "    count = 0\n",
    "    PATH = \"./best_model.hdf5\"\n",
    "    \n",
    "    train_df = train.loc[train_index]\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df   = train.loc[val_index]\n",
    "    val_df   = val_df.reset_index(drop=True)\n",
    "    \n",
    "    trainset = MyDataset(train_df)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
    "    valset = MyDataset(val_df)\n",
    "    val_loader = torch.utils.data.DataLoader(valset, batch_size=20, shuffle=True)\n",
    "    \n",
    "    model = score_model()\n",
    "    set_trainable(model.bert, False)\n",
    "    set_trainable(model.embedding, True)\n",
    "    set_trainable(model.pair_score, True)\n",
    "    model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    \n",
    "    print('n_fold:', n_fold+1)\n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model.train() \n",
    "        avg_loss = 0.\n",
    "        for idx, (inputs, label) in enumerate(train_loader):\n",
    "            index_token, offset, distP_A, distP_B = inputs\n",
    "            index_token = index_token.type(torch.LongTensor).cuda()\n",
    "            offset      = offset.type(torch.LongTensor).cuda()\n",
    "            label       = label.type(torch.LongTensor).cuda()\n",
    "            distP_A     = distP_A.type(torch.LongTensor).cuda()\n",
    "            distP_B     = distP_B.type(torch.LongTensor).cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output_train = model(index_token, offset, distP_A, distP_B)\n",
    "            loss = criterion(output_train, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        avg_val_loss = 0.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (inputs, label) in enumerate(val_loader):\n",
    "                index_token, offset, distP_A, distP_B = inputs\n",
    "                index_token = index_token.type(torch.LongTensor).cuda()\n",
    "                offset      = offset.type(torch.LongTensor).cuda()\n",
    "                label       = label.type(torch.LongTensor).cuda()\n",
    "                distP_A     = distP_A.type(torch.LongTensor).cuda()\n",
    "                distP_B     = distP_B.type(torch.LongTensor).cuda()\n",
    "                \n",
    "                output_test =  model(index_token, offset, distP_A, distP_B)\n",
    "                avg_val_loss += criterion(output_test, label).item() / len(val_loader)\n",
    "                \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "                i + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "        \n",
    "        # Simple Early Stop\n",
    "        if min_val_loss > avg_val_loss:\n",
    "            min_val_loss = avg_val_loss \n",
    "            torch.save(model.state_dict(), PATH)\n",
    "        \n",
    "    \n",
    "    del model\n",
    "    \n",
    "    model = score_model()\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, label) in enumerate(test_loader):\n",
    "            index_token, offset, distP_A, distP_B = inputs\n",
    "            index_token = index_token.type(torch.LongTensor).cuda()\n",
    "            offset      = offset.type(torch.LongTensor).cuda()\n",
    "            label       = label.type(torch.LongTensor).cuda()\n",
    "            distP_A     = distP_A.type(torch.LongTensor).cuda()\n",
    "            distP_B     = distP_B.type(torch.LongTensor).cuda()\n",
    "                \n",
    "            y_pred = model(index_token, offset, distP_A, distP_B)\n",
    "            y_pred = softmax(y_pred.cpu().numpy())\n",
    "            start = idx * 20\n",
    "            end = start + 20\n",
    "            output[start:end, :] += y_pred                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.600046</td>\n",
       "      <td>0.357497</td>\n",
       "      <td>0.042457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.990822</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.008768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.031095</td>\n",
       "      <td>0.931218</td>\n",
       "      <td>0.037688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.045824</td>\n",
       "      <td>0.155976</td>\n",
       "      <td>0.798199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.989948</td>\n",
       "      <td>0.005657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>development-6</td>\n",
       "      <td>0.866779</td>\n",
       "      <td>0.127916</td>\n",
       "      <td>0.005305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>development-7</td>\n",
       "      <td>0.935494</td>\n",
       "      <td>0.020891</td>\n",
       "      <td>0.043615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>development-8</td>\n",
       "      <td>0.329409</td>\n",
       "      <td>0.626504</td>\n",
       "      <td>0.044087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>development-9</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.994971</td>\n",
       "      <td>0.004847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>development-10</td>\n",
       "      <td>0.721386</td>\n",
       "      <td>0.229466</td>\n",
       "      <td>0.049147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>development-11</td>\n",
       "      <td>0.073338</td>\n",
       "      <td>0.772435</td>\n",
       "      <td>0.154228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>development-12</td>\n",
       "      <td>0.960239</td>\n",
       "      <td>0.006095</td>\n",
       "      <td>0.033666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>development-13</td>\n",
       "      <td>0.801289</td>\n",
       "      <td>0.181394</td>\n",
       "      <td>0.017317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>development-14</td>\n",
       "      <td>0.728302</td>\n",
       "      <td>0.190078</td>\n",
       "      <td>0.081619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>development-15</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.519404</td>\n",
       "      <td>0.038419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>development-16</td>\n",
       "      <td>0.044174</td>\n",
       "      <td>0.751891</td>\n",
       "      <td>0.203935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>development-17</td>\n",
       "      <td>0.511383</td>\n",
       "      <td>0.358984</td>\n",
       "      <td>0.129633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>development-18</td>\n",
       "      <td>0.284188</td>\n",
       "      <td>0.226163</td>\n",
       "      <td>0.489649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>development-19</td>\n",
       "      <td>0.346912</td>\n",
       "      <td>0.552387</td>\n",
       "      <td>0.100700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>development-20</td>\n",
       "      <td>0.048989</td>\n",
       "      <td>0.777971</td>\n",
       "      <td>0.173040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID         A         B   NEITHER\n",
       "0    development-1  0.600046  0.357497  0.042457\n",
       "1    development-2  0.990822  0.000411  0.008768\n",
       "2    development-3  0.031095  0.931218  0.037688\n",
       "3    development-4  0.045824  0.155976  0.798199\n",
       "4    development-5  0.004394  0.989948  0.005657\n",
       "5    development-6  0.866779  0.127916  0.005305\n",
       "6    development-7  0.935494  0.020891  0.043615\n",
       "7    development-8  0.329409  0.626504  0.044087\n",
       "8    development-9  0.000182  0.994971  0.004847\n",
       "9   development-10  0.721386  0.229466  0.049147\n",
       "10  development-11  0.073338  0.772435  0.154228\n",
       "11  development-12  0.960239  0.006095  0.033666\n",
       "12  development-13  0.801289  0.181394  0.017317\n",
       "13  development-14  0.728302  0.190078  0.081619\n",
       "14  development-15  0.442177  0.519404  0.038419\n",
       "15  development-16  0.044174  0.751891  0.203935\n",
       "16  development-17  0.511383  0.358984  0.129633\n",
       "17  development-18  0.284188  0.226163  0.489649\n",
       "18  development-19  0.346912  0.552387  0.100700\n",
       "19  development-20  0.048989  0.777971  0.173040"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "output /= 5\n",
    "sub_df_path = os.path.join('../input/gendered-pronoun-resolution/', 'sample_submission_stage_1.csv')\n",
    "sub_df = pd.read_csv(sub_df_path)\n",
    "sub_df.loc[:, 'A'] = pd.Series(output[:, 0])\n",
    "sub_df.loc[:, 'B'] = pd.Series(output[:, 1])\n",
    "sub_df.loc[:, 'NEITHER'] = pd.Series(output[:, 2])\n",
    "\n",
    "sub_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4793930958369195"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = pd.read_csv('../working/test.csv')['label']\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "y_one_hot = np.zeros((2000, 3))\n",
    "for i in range(len(y_test)):\n",
    "    y_one_hot[i, y_test[i]] = 1\n",
    "log_loss(y_one_hot, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8015\n"
     ]
    }
   ],
   "source": [
    "_output = np.argmax(output, axis=1)\n",
    "print('acc:', np.asarray(np.where(_output == y_test)).shape[1]/ 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
